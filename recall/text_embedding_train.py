import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import os
from tqdm import tqdm
from datetime import datetime, timedelta
from peft import LoraConfig, get_peft_model, TaskType
from modelscope import AutoModelForCausalLM, AutoTokenizer
from accelerate import Accelerator
from recall.model_utils import forward_backbone, ensure_last_hidden_state

class RecallDataset(Dataset):
    """è®­ç»ƒæ•°æ®é›†ï¼šæ„é€ "åŸå§‹æ–‡æœ¬<recall>"æ ¼å¼"""
    
    def __init__(self, texts, target_embeddings, tokenizer, base_model, max_length=None):
        self.texts = texts
        self.target_embeddings = target_embeddings
        self.tokenizer = tokenizer
        self.base_model = base_model
        self.max_length = max_length
        self.recall_token = '<recall>'
        
        # è·å–æ¨¡å‹çš„æ•°æ®ç±»å‹ï¼ˆä¸é¢„å…ˆç§»åŠ¨åˆ°è®¾å¤‡ï¼‰
        first_param = next(base_model.parameters())
        self.model_dtype = first_param.dtype
        self.model_device = first_param.device
        print(f"ğŸ”§ RecallDatasetæ£€æµ‹åˆ°æ¨¡å‹æ•°æ®ç±»å‹: {self.model_dtype}, è®¾å¤‡: {self.model_device}")

        # æ³¨æ„ï¼šä¸åœ¨__init__ä¸­é¢„å…ˆç§»åŠ¨æ‰€æœ‰embeddingsåˆ°GPUï¼Œé¿å…æ˜¾å­˜ç´¯ç§¯
        # ç¡®ä¿target_embeddingsåœ¨CPUä¸Šï¼Œé¿å…æ˜¾å­˜æ³¢åŠ¨
        if isinstance(self.target_embeddings, torch.Tensor) and self.target_embeddings.is_cuda:
            print(f"âš ï¸ target_embeddingsåœ¨GPUä¸Šï¼Œç§»åŠ¨åˆ°CPUä»¥é¿å…æ˜¾å­˜æ³¢åŠ¨...")
            self.target_embeddings = self.target_embeddings.cpu()
        print(f"ğŸ“Š target_embeddingsä¿æŒåœ¨CPUä¸Šï¼Œè®­ç»ƒæ—¶æŒ‰éœ€ç§»åŠ¨: {self.target_embeddings.shape}")
        
        # æ£€æŸ¥tokenæ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœä¼ å…¥çš„æ˜¯processorï¼Œä½¿ç”¨processor.tokenizerï¼‰
        self.actual_tokenizer = self.tokenizer.tokenizer if hasattr(self.tokenizer, 'tokenizer') else self.tokenizer
        self.recall_token_id = self.actual_tokenizer.convert_tokens_to_ids(self.recall_token)
        if self.recall_token_id == self.actual_tokenizer.unk_token_id:
            raise ValueError(f"âŒ {self.recall_token} tokenä¸å­˜åœ¨ï¼è¯·å…ˆæ·»åŠ æ­¤ç‰¹æ®Štoken")
        
        print(f"âœ… æ‰¾åˆ°ç‰¹æ®Štoken: {self.recall_token} (ID: {self.recall_token_id})")
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        target_embedding = self.target_embeddings[idx]
        
        # æ„é€ è¾“å…¥ï¼š"åŸå§‹æ–‡æœ¬<recall>"
        input_text = f"{text}{self.recall_token}"
        
        # ç¼–ç  - ä¸è¿›è¡Œpaddingï¼Œä¿ç•™åŸå§‹é•¿åº¦
        # æ³¨æ„ï¼šåªæœ‰åœ¨è®¾ç½®äº†max_lengthæ—¶æ‰ä½¿ç”¨ï¼Œå¦åˆ™è®©tokenizerä½¿ç”¨åŸå§‹é•¿åº¦
        encode_kwargs = {
            'return_tensors': 'pt'
        }
        if self.max_length is not None:
            # å½“è®¾ç½®äº†max_lengthæ—¶ï¼Œå¿…é¡»æ˜¾å¼è®¾ç½®truncation=Trueä»¥é¿å…è­¦å‘Š
            encode_kwargs['max_length'] = self.max_length
            encode_kwargs['truncation'] = True
        else:
            # å¦‚æœæ²¡æœ‰è®¾ç½®max_lengthï¼Œä¸è¿›è¡Œæˆªæ–­
            encode_kwargs['truncation'] = False

        encoding = self.actual_tokenizer(input_text, **encode_kwargs)
        
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)
        
        # æ‰¾åˆ°<recall>çš„ä½ç½®
        recall_positions = (input_ids == self.recall_token_id).nonzero(as_tuple=True)[0]
        if len(recall_positions) > 0:
            recall_position = recall_positions[-1]  # ä½¿ç”¨æœ€åä¸€ä¸ªä½ç½®
        else:
            # å¦‚æœè¢«æˆªæ–­äº†ï¼ŒæŠ¥é”™
            raise ValueError(f"æ–‡æœ¬è¿‡é•¿ï¼Œ{self.recall_token} tokenè¢«æˆªæ–­")
        
        # æ³¨æ„ï¼štarget_embeddingä¿æŒåœ¨CPUä¸Šï¼Œåªè½¬æ¢æ•°æ®ç±»å‹ï¼ˆä¸ç§»åŠ¨è®¾å¤‡ï¼‰
        # è®¾å¤‡åˆ†é…ç”±Acceleratoråœ¨collate_fnä¸­ç»Ÿä¸€å¤„ç†ï¼Œé¿å…åœ¨__getitem__ä¸­ç§»åŠ¨å¯¼è‡´æ˜¾å­˜æ³¢åŠ¨
        # ä½¿ç”¨clone().detach()é¿å…åˆ›å»ºæ–°çš„è®¡ç®—å›¾ï¼Œå‡å°‘æ˜¾å­˜æ³¢åŠ¨
        if target_embedding.is_cuda:
            # å¦‚æœåŸæœ¬åœ¨GPUä¸Šï¼Œå…ˆç§»åˆ°CPUï¼ˆé¿å…æ˜¾å­˜æ³¢åŠ¨ï¼‰
            target_embedding = target_embedding.cpu()
        if target_embedding.dtype != self.model_dtype:
            # åªè½¬æ¢æ•°æ®ç±»å‹ï¼Œä¸ç§»åŠ¨è®¾å¤‡ï¼ˆä¿æŒåœ¨CPUä¸Šï¼‰
            target_embedding = target_embedding.to(dtype=self.model_dtype)
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'recall_position': recall_position,
            'target_embedding': target_embedding,
            'seq_len': len(input_ids)  # ä¿å­˜åŸå§‹åºåˆ—é•¿åº¦ï¼Œç”¨äºåŠ¨æ€padding
        }

class RecallMemoryTrainer:
    """<recall> tokenè®­ç»ƒå™¨ - æ”¯æŒå¤šGPUè®¾å¤‡é€‰æ‹©"""
    
    def _get_tokenizer(self):
        """è·å–çœŸæ­£çš„tokenizerï¼ˆå¦‚æœä¼ å…¥çš„æ˜¯processorï¼Œåˆ™è¿”å›processor.tokenizerï¼‰"""
        if hasattr(self.tokenizer, 'tokenizer'):
            # å¦‚æœä¼ å…¥çš„æ˜¯processorï¼Œè¿”å›å…¶å†…éƒ¨çš„tokenizer
            return self.tokenizer.tokenizer
        else:
            # å¦‚æœä¼ å…¥çš„æ˜¯tokenizerï¼Œç›´æ¥è¿”å›
            return self.tokenizer
    
    def __init__(self, model_name, device=None, lora_r=8, lora_alpha=32, lora_dropout=0.1, original_device=None, preloaded_model=None, preloaded_tokenizer=None, gradient_accumulation_steps=1, max_memory=None, epoch_end_hook=None, max_length=8000, lora_target_modules=None):
        """
        Args:
            model_name: æ¨¡å‹è·¯å¾„æˆ–åç§°
            device: è®­ç»ƒè®¾å¤‡ï¼Œæ”¯æŒï¼š
                   - None: ä½¿ç”¨é»˜è®¤è®¾å¤‡
                   - "auto": è‡ªåŠ¨åˆ†é…å¤šGPU
                   - ['cuda:0', 'cuda:1', ...]: GPUåˆ—è¡¨
                   - "cuda:0": æŒ‡å®šå•GPU
                   - "cpu": CPUè®¾å¤‡
            lora_r: LoRA rankï¼ˆé»˜è®¤8ï¼‰
            lora_alpha: LoRA alphaï¼ˆé»˜è®¤32ï¼‰
            lora_dropout: LoRA dropoutï¼ˆé»˜è®¤0.1ï¼‰
        """

        # æ³¨æ„ï¼šCUDA_VISIBLE_DEVICES å·²ç»åœ¨ app.py ä¸­æ­£ç¡®è®¾ç½®ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤è®¾ç½®
        # åªä¿å­˜åŸå§‹ç¯å¢ƒå˜é‡ç”¨äºcleanupæ—¶æ¢å¤
        self._original_cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')

        self.model_name = model_name
        self.specified_device = device
        self.original_device = original_device or device  # ä¿å­˜åŸå§‹è®¾å¤‡ä¿¡æ¯ç”¨äºæ˜¾ç¤º
        self.recall_token = '<recall>'
        self.max_length = max_length  # æœ€å¤§åºåˆ—é•¿åº¦
        # æ˜¾ç¤ºæ­£ç¡®çš„è®¾å¤‡ä¿¡æ¯
        display_device = self.original_device or device
        if isinstance(display_device, str) and display_device.startswith('cuda:'):
            print(f"   ä½¿ç”¨GPUè®¾å¤‡: {display_device}")
        elif display_device == "auto":
            print("   è‡ªåŠ¨é€‰æ‹©è®¾å¤‡")
        else:
            print(f"   ä½¿ç”¨è®¾å¤‡: {display_device}")
        # é…ç½®æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.epoch_end_hook = epoch_end_hook

        # æ ¹æ®è®¾å¤‡é…ç½®å†³å®šæ˜¯å¦å¯ç”¨DDP
        use_ddp = False
        cuda_visible_devices = None

        if isinstance(device, list) and len(device) > 1:
            use_ddp = True
            print(f"   å¤šGPUæ¨¡å¼: å¯ç”¨DDPï¼ŒGPUæ•°é‡: {len(device)}")
        elif device == "auto":
            if torch.cuda.is_available() and torch.cuda.device_count() > 1:
                use_ddp = True
                print(f"   å¤šGPUæ¨¡å¼: è‡ªåŠ¨æ£€æµ‹å¤šGPUï¼Œå¯ç”¨DDP")
        # åˆå§‹åŒ–Acceleratorï¼Œæ”¯æŒå¤šGPUå’Œæ¢¯åº¦ç´¯ç§¯
        accelerator_kwargs = {
            'mixed_precision': 'bf16',
            'gradient_accumulation_steps': self.gradient_accumulation_steps,
        }

        self.accelerator = Accelerator(**accelerator_kwargs)

        self.accelerate_enabled = True
        self.ddp_enabled = use_ddp
        self.local_rank = None

        # æ ¹æ®è®¾å¤‡é…ç½®è®¾ç½®ç›¸å…³å˜é‡
        if device is None:
            self.use_auto_device = False
            self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.multi_gpu_list = None
        elif isinstance(device, list):
            # å¤šGPUé…ç½®
            if len(device) > 0:
                self.use_auto_device = False
                self.primary_device = torch.device(device[0])
                self.multi_gpu_list = device
                print(f"   ä½¿ç”¨å¤šGPUåˆ—è¡¨: {device}ï¼Œä¸»è®¾å¤‡: {device[0]}")
        elif isinstance(device, str) and device.startswith('cuda:'):
            # å•GPUé…ç½®
            self.use_auto_device = False
            self.primary_device = torch.device(device)
            self.multi_gpu_list = None
        elif device == "auto":
            self.use_auto_device = True
            self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.multi_gpu_list = None
        else:
            # CPUæˆ–å…¶ä»–
            self.use_auto_device = False
            self.primary_device = torch.device('cpu')
            self.multi_gpu_list = None
        
        # LoRAé…ç½®å‚æ•°
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        # LoRAç›®æ ‡æ¨¡å—ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
        self.lora_target_modules = lora_target_modules
        self.max_memory = max_memory
        self._model_prepared = False

        # è®¾å¤‡å¤„ç†é€»è¾‘ - ä¸get_text_embedding.pyä¿æŒä¸€è‡´
        if device is None:
            self.use_auto_device = False
            self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.multi_gpu_list = None
        elif isinstance(device, list):
            # å¤„ç†GPUåˆ—è¡¨
            if len(device) > 0:
                self.use_auto_device = False
                self.primary_device = torch.device(device[0])
                self.multi_gpu_list = device
                print(f"   ä½¿ç”¨å¤šGPUåˆ—è¡¨: {device}ï¼Œä¸»è®¾å¤‡: {device[0]}")
            else:
                self.use_auto_device = True
                self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.multi_gpu_list = None
        elif isinstance(device, str):
            if device == "auto":
                self.use_auto_device = True
                self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.multi_gpu_list = None
            else:
                self.use_auto_device = False
                self.primary_device = torch.device(device)
                self.multi_gpu_list = None
        else:
            self.use_auto_device = False
            self.primary_device = device
            self.multi_gpu_list = None
        
        print(f"ğŸ¤– åˆå§‹åŒ–è®­ç»ƒå™¨...")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   è®¾å¤‡é…ç½®: {device}")

        # è‹¥ç”± torchrun å¯åŠ¨ï¼Œè‡ªåŠ¨å¯ç”¨DDPå¹¶å›ºå®šåˆ°å•å¡
        if 'LOCAL_RANK' in os.environ and not self.accelerator.state.initialized:  # Accelerate å·²åˆå§‹åŒ–åˆ™è·³è¿‡
            self.local_rank = int(os.environ['LOCAL_RANK'])
            os.environ.setdefault('RANK', os.environ.get('RANK', '0'))
            os.environ.setdefault('WORLD_SIZE', os.environ.get('WORLD_SIZE', '1'))
            torch.cuda.set_device(self.local_rank)
            if not (dist.is_available() and dist.is_initialized()):
                dist.init_process_group(backend='nccl', timeout=timedelta(minutes=60))
            self.ddp_enabled = True
            self.use_auto_device = False
            self.multi_gpu_list = None
            self.primary_device = torch.device(f'cuda:{self.local_rank}')
            self.specified_device = f'cuda:{self.local_rank}'
            if self.is_main_process():
                print(f"ğŸ§© DDPå·²å¯ç”¨ï¼ŒLOCAL_RANK={self.local_rank}")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥å‡å°‘æ˜¾å­˜ç¢ç‰‡åŒ–ï¼ˆå³ä½¿ä½¿ç”¨é¢„åŠ è½½æ¨¡å‹ä¹Ÿéœ€è¦ï¼‰
        import os as _os
        _os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')
        
        # å¤„ç†é¢„åŠ è½½æ¨¡å‹æˆ–åŠ è½½æ–°æ¨¡å‹
        if preloaded_model is not None and preloaded_tokenizer is not None:
            # ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡å‹
            print("   ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡å‹å’Œtokenizer")
            
            # åœ¨åˆ›å»ºLoRAå‰ï¼Œæ¸…ç†æ˜¾å­˜å¹¶ç¡®ä¿æ¨¡å‹å¤„äºå¹²å‡€çŠ¶æ€
            preloaded_model.eval()
            with torch.no_grad():
                import gc
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
            
            self.model = preloaded_model
            self.tokenizer = preloaded_tokenizer
            # æ£€æŸ¥ç‰¹æ®Štokenï¼ˆå³ä½¿æ˜¯é¢„åŠ è½½çš„æ¨¡å‹ä¹Ÿéœ€è¦è®¾ç½®token IDï¼‰
            self._check_special_token()
            self._skip_model_loading = True
        else:
            # æ­£å¸¸åŠ è½½æ¨¡å‹
            self._load_model()
            # æ£€æŸ¥ç‰¹æ®Štoken
            self._check_special_token()
            self._skip_model_loading = False

        # è·å–å®é™…è®¾å¤‡ä¿¡æ¯
        first_param = next(self.model.parameters())
        self.actual_device = first_param.device
        print(f"   å®é™…æ¨¡å‹è®¾å¤‡: {self.actual_device}")

        # å¦‚æœä¸æ˜¯é¢„åŠ è½½æ¨¡å‹ï¼Œæ‰è¿›è¡Œåç»­åˆå§‹åŒ–
        if not getattr(self, '_skip_model_loading', False):
            # ä¿å­˜åŸå§‹embedding
            self._save_original_embedding()

            # è®¾ç½®LoRA
            self._setup_lora()
        else:
            # å¯¹äºé¢„åŠ è½½æ¨¡å‹ï¼Œéœ€è¦é‡æ–°è®¾ç½®ä¸€äº›å±æ€§
            # è®°å½•åŸå§‹embedding
            self._save_original_embedding()

            # è®¾ç½®LoRA
            self._setup_lora()
        
        # æ˜¾ç¤ºå‚æ•°ç»Ÿè®¡
        self._print_trainable_parameters()

    def is_main_process(self):
        if hasattr(self, 'accelerator'):
            return self.accelerator.is_main_process
        return (not self.ddp_enabled) or (dist.get_rank() == 0)
    
    def _prepare_model_once(self):
        """ç¡®ä¿Acceleratorä»…å¯¹æ¨¡å‹æ‰§è¡Œä¸€æ¬¡prepareï¼Œé¿å…é‡å¤åŒ…è£…å¯¼è‡´æ˜¾å­˜è†¨èƒ€"""
        if not self._model_prepared:
            if self.is_main_process():
                print("ğŸ”„ é¦–æ¬¡è°ƒç”¨Accelerator.prepareï¼Œå‡†å¤‡æ¨¡å‹...")
            self.model = self.accelerator.prepare(self.model)
            self._model_prepared = True

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ - æ”¯æŒå¤šGPUé…ç½®"""
        # é™ç¢ç‰‡ï¼šå°½é‡ä½¿ç”¨å¯æ‰©å±•åˆ†é…æ®µ
        import os as _os
        _os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')

        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨local_files_only
        import os
        model_path = self.model_name
        if not os.path.isabs(model_path):
            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(script_dir)
            model_path = os.path.abspath(os.path.join(project_root, model_path))
        
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œä½¿ç”¨local_files_onlyé¿å…modelscopeå°è¯•ä»ç½‘ç»œä¸‹è½½
        is_local_path = os.path.exists(model_path) and os.path.isdir(model_path)
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path if is_local_path else self.model_name, 
            trust_remote_code=True,
            local_files_only=is_local_path
        )
        
        try:
            # æ ¹æ®è®¾å¤‡é…ç½®é€‰æ‹©device_map
            if self.use_auto_device:
                device_map = "auto"
                print("   ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…")
            elif hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                # å¤šGPUé…ç½®
                device_map = "auto"
                print(f"   ä½¿ç”¨å¤šGPUè‡ªåŠ¨åˆ†é…: {self.multi_gpu_list}")

                # è®¾ç½®ç¯å¢ƒå˜é‡é™åˆ¶å¯è§GPU
                import os
                if 'CUDA_VISIBLE_DEVICES' not in os.environ:
                    gpu_indices = [gpu.split(':')[1] for gpu in self.multi_gpu_list if gpu.startswith('cuda:')]
                    if gpu_indices:
                        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(gpu_indices)
                        print(f"   è®¾ç½®CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")

                # å¦‚æœæœ‰max_memoryé…ç½®ï¼Œè®¾ç½®å®ƒæ¥æ§åˆ¶å¤šGPUåˆ†å¸ƒ
                if hasattr(self, 'max_memory') and self.max_memory:
                    device_map = self.max_memory
                    print(f"   ä½¿ç”¨max_memoryæ§åˆ¶GPUåˆ†å¸ƒ: {device_map}")
                else:
                    print(f"   ä½¿ç”¨è‡ªåŠ¨GPUåˆ†å¸ƒ (æœªè®¾ç½®max_memory)")
            elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                # å•GPUæŒ‡å®š - å¦‚æœæœåŠ¡å™¨å·²è®¾ç½®CUDA_VISIBLE_DEVICESï¼Œä½¿ç”¨cuda:0
                import os
                if 'CUDA_VISIBLE_DEVICES' in os.environ:
                    # æœåŠ¡å™¨å·²è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨cuda:0
                    device_map = {"": 0}
                    print(f"   æœåŠ¡å™¨å·²è®¾ç½®CUDA_VISIBLE_DEVICESï¼Œä½¿ç”¨cuda:0 (åŸå§‹è®¾å¤‡: {self.specified_device})")
                else:
                    # æœåŠ¡å™¨æœªè®¾ç½®ï¼Œä½¿ç”¨åŸå§‹è®¾å¤‡ç´¢å¼•
                    device_index = int(self.specified_device.split(':')[1])
                    device_map = {"": device_index}
                    print(f"   ä½¿ç”¨æŒ‡å®šå•GPU: {self.specified_device} (device_map: {device_map})")
            elif self.specified_device == "cpu":
                device_map = {"": "cpu"}
                print(f"   ä½¿ç”¨CPUè®¾å¤‡")
            else:
                # é»˜è®¤æƒ…å†µ
                if hasattr(self, 'primary_device') and self.primary_device.type == 'cuda':
                    device_map = {"": self.primary_device.index}
                else:
                    device_map = "auto"
                print(f"   ä½¿ç”¨é»˜è®¤è®¾å¤‡æ˜ å°„: {device_map}")
            
            print(f"   å®é™…ä½¿ç”¨è®¾å¤‡æ˜ å°„: {device_map}")
            
            # æ£€æŸ¥æ¨¡å‹ç±»å‹ï¼šå¦‚æœæ˜¯Qwen3-VLï¼Œéœ€è¦ä½¿ç”¨Qwen3VLForConditionalGeneration
            # æ£€æŸ¥config.jsonæ–‡ä»¶æ¥ç¡®å®šæ¨¡å‹ç±»å‹
            import json
            config_file = os.path.join(model_path if is_local_path else self.model_name, "config.json")
            is_qwen3vl = False
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        model_config = json.load(f)
                        model_type = model_config.get("model_type", "").lower()
                        if "qwen3_vl" in model_type or "qwen3-vl" in model_type:
                            is_qwen3vl = True
                            print(f"   æ£€æµ‹åˆ°Qwen3-VLæ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨Qwen3VLForConditionalGenerationåŠ è½½")
                except:
                    pass
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼
            if is_qwen3vl:
                # ä½¿ç”¨Qwen3VLForConditionalGenerationåŠ è½½Qwen3-VLæ¨¡å‹
                from transformers import Qwen3VLForConditionalGeneration
                self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                    model_path if is_local_path else self.model_name,
                    torch_dtype="auto",
                    device_map=device_map,
                    trust_remote_code=True,
                    local_files_only=is_local_path
                )
            else:
                # ä½¿ç”¨AutoModelForCausalLMåŠ è½½æ™®é€šæ–‡æœ¬æ¨¡å‹
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path if is_local_path else self.model_name,
                    torch_dtype="auto",
                    device_map=device_map,
                    trust_remote_code=True,
                    local_files_only=is_local_path
                )

            # é™å†…å­˜ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ + å…³é—­use_cache
            try:
                if hasattr(self.model, 'gradient_checkpointing_enable'):
                    self.model.gradient_checkpointing_enable()
                    print("   âœ… å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰ä»¥å‡å°‘æ˜¾å­˜å ç”¨")
                if hasattr(self.model, 'config'):
                    setattr(self.model.config, 'use_cache', False)
                    print("   âœ… å·²å…³é—­use_cacheä»¥å‡å°‘æ˜¾å­˜å ç”¨")
            except Exception as e:
                print(f"   âš ï¸ å¯ç”¨æ˜¾å­˜ä¼˜åŒ–åŠŸèƒ½å¤±è´¥: {e}")
            
            # è·å–å®é™…è®¾å¤‡ä¿¡æ¯
            first_param = next(self.model.parameters())
            model_dtype = first_param.dtype
            model_device = first_param.device
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   å®é™…è®¾å¤‡: {model_device}")
            print(f"   æ•°æ®ç±»å‹: {model_dtype}")
            
            # æ˜¾ç¤ºè®¾å¤‡æ˜ å°„ä¿¡æ¯
            if hasattr(self.model, 'hf_device_map'):
                print(f"   è®¾å¤‡æ˜ å°„è¯¦æƒ…: {self.model.hf_device_map}")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å›é€€ç­–ç•¥
            print("ğŸ”„ å°è¯•å›é€€åˆ°å•GPUæ¨¡å¼...")
            
            try:
                # ç¡®å®šå›é€€è®¾å¤‡
                if hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                    fallback_device = self.multi_gpu_list[0]
                elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                    fallback_device = self.specified_device
                else:
                    fallback_device = 'cuda:0'
                
                # æå–è®¾å¤‡ç´¢å¼•
                if fallback_device.startswith('cuda:'):
                    device_index = int(fallback_device.split(':')[1])
                    device_map = {"": device_index}
                else:
                    device_map = {"": "cpu"}
                
                print(f"   å›é€€è®¾å¤‡æ˜ å°„: {device_map}")
                
                # æ£€æŸ¥æ¨¡å‹ç±»å‹ï¼šå¦‚æœæ˜¯Qwen3-VLï¼Œéœ€è¦ä½¿ç”¨Qwen3VLForConditionalGeneration
                import json
                config_file = os.path.join(model_path if is_local_path else self.model_name, "config.json")
                is_qwen3vl = False
                if os.path.exists(config_file):
                    try:
                        with open(config_file, 'r', encoding='utf-8') as f:
                            model_config = json.load(f)
                            model_type = model_config.get("model_type", "").lower()
                            if "qwen3_vl" in model_type or "qwen3-vl" in model_type:
                                is_qwen3vl = True
                                print(f"   æ£€æµ‹åˆ°Qwen3-VLæ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨Qwen3VLForConditionalGenerationåŠ è½½")
                    except:
                        pass
                
                # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼
                if is_qwen3vl:
                    # ä½¿ç”¨Qwen3VLForConditionalGenerationåŠ è½½Qwen3-VLæ¨¡å‹
                    from transformers import Qwen3VLForConditionalGeneration
                    self.model = Qwen3VLForConditionalGeneration.from_pretrained(
                        model_path if is_local_path else self.model_name,
                        torch_dtype="auto",
                        device_map=device_map,
                        trust_remote_code=True,
                        local_files_only=is_local_path
                    )
                else:
                    # ä½¿ç”¨AutoModelForCausalLMåŠ è½½æ™®é€šæ–‡æœ¬æ¨¡å‹
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_path if is_local_path else self.model_name,
                        torch_dtype="auto",
                        device_map=device_map,
                        trust_remote_code=True,
                        local_files_only=is_local_path
                    )
                
                first_param = next(self.model.parameters())
                print(f"âœ… ä½¿ç”¨å›é€€è®¾å¤‡åŠ è½½æˆåŠŸ: {first_param.device}")
                
            except Exception as fallback_error:
                print(f"âŒ å›é€€åŠ è½½ä¹Ÿå¤±è´¥: {fallback_error}")
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: åŸé”™è¯¯={e}, å›é€€é”™è¯¯={fallback_error}")
    
    def _check_and_add_special_token(self):
        """æ£€æŸ¥å¹¶æ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        tokenizer = self._get_tokenizer()
        self.recall_token_id = tokenizer.convert_tokens_to_ids(self.recall_token)
        
        if self.recall_token_id == tokenizer.unk_token_id:
            # tokenä¸å­˜åœ¨ï¼Œéœ€è¦æ·»åŠ 
            print(f"âš ï¸ {self.recall_token} tokenä¸å­˜åœ¨ï¼Œæ­£åœ¨æ·»åŠ ...")
            original_vocab_size = len(tokenizer)
            
            # æ·»åŠ ç‰¹æ®Štoken
            tokenizer.add_tokens(self.recall_token)
            
            new_vocab_size = len(tokenizer)
            print(f"   è¯è¡¨å¤§å°: {original_vocab_size} -> {new_vocab_size} (+{new_vocab_size - original_vocab_size})")
            
            # è°ƒæ•´æ¨¡å‹embeddingå±‚
            print("   è°ƒæ•´æ¨¡å‹embeddingå±‚...")
            self.model.resize_token_embeddings(len(tokenizer))
            
            # è·å–æ–°æ·»åŠ çš„token ID
            self.recall_token_id = tokenizer.convert_tokens_to_ids(self.recall_token)
            
            # åˆå§‹åŒ–æ–°tokençš„æƒé‡ï¼ˆä½¿ç”¨"æ€»ç»“"å’Œ"å›å¿†"çš„åµŒå…¥å‘é‡ä¹‹å’Œï¼‰
            print("   åˆå§‹åŒ–æ–°tokenæƒé‡...")
            try:
                embedding_layer = self.model.get_input_embeddings()

                # <recall> token: ä½¿ç”¨"æ€»ç»“"å’Œ"å›å¿†"çš„åµŒå…¥å‘é‡ä¹‹å’Œ
                ref_words = ["æ€»ç»“", "å›å¿†"]
                ref_embeddings = []
                used_references = []

                for word in ref_words:
                    ref_id = tokenizer.convert_tokens_to_ids(word)
                    if ref_id != tokenizer.unk_token_id:
                        ref_embedding = embedding_layer.weight[ref_id].clone().detach()
                        ref_embeddings.append(ref_embedding)
                        used_references.append(word)
                        print(f"   âœ… ä½¿ç”¨å‚è€ƒtoken: '{word}' (ID: {ref_id})")
                    else:
                        print(f"   âš ï¸ å‚è€ƒtoken '{word}' ä¸å­˜åœ¨ï¼Œè·³è¿‡")

                if len(ref_embeddings) > 0:
                    # è®¡ç®—å‚è€ƒtokenåµŒå…¥å‘é‡çš„å’Œï¼Œç„¶åç›´æ¥å½’ä¸€åŒ–
                    new_embedding = ref_embeddings[0]
                    for ref_emb in ref_embeddings[1:]:
                        new_embedding = new_embedding + ref_emb
                    
                    # ç›´æ¥å½’ä¸€åŒ–ï¼šç¼©æ”¾åˆ°ç¬¬ä¸€ä¸ªå‚è€ƒtokençš„èŒƒæ•°ï¼Œç„¶åæ·»åŠ å°çš„æ­£äº¤æ‰°åŠ¨ä»¥åŒºåˆ†
                    if len(ref_embeddings) > 1:
                        target_norm = ref_embeddings[0].norm()
                        current_norm = new_embedding.norm()
                        if current_norm > 0:
                            new_embedding = new_embedding / current_norm * target_norm
                        
                        # æ·»åŠ å°çš„æ­£äº¤æ‰°åŠ¨ï¼Œé¿å…ä¸å‚è€ƒtokenè¿‡äºç›¸ä¼¼
                        ref1_normalized = ref_embeddings[0] / ref_embeddings[0].norm()
                        new_normalized = new_embedding / new_embedding.norm()
                        proj = torch.dot(new_normalized, ref1_normalized) * ref1_normalized
                        orthogonal = new_normalized - proj
                        if orthogonal.norm() > 1e-6:
                            orthogonal = orthogonal / orthogonal.norm()
                            perturbation_scale = 0.1
                            new_embedding = new_embedding + orthogonal * perturbation_scale * target_norm
                            new_embedding = new_embedding / new_embedding.norm() * target_norm
                    
                    embedding_layer.weight.data[self.recall_token_id] = new_embedding
                    ref_str = " + ".join(used_references)
                    print(f"   âœ… {self.recall_token} (ID: {self.recall_token_id}) åˆå§‹åŒ–å®Œæˆï¼ˆå‚è€ƒ: {ref_str}ï¼‰")
                else:
                    print(f"   âš ï¸ æ‰€æœ‰å‚è€ƒtokenéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

            except Exception as e:
                print(f"   âš ï¸ åˆå§‹åŒ–tokenæƒé‡æ—¶å‡ºé”™: {e}")
            
            print(f"âœ… ç‰¹æ®Štokenæ·»åŠ å®Œæˆ: {self.recall_token} (ID: {self.recall_token_id})")
        else:
            # tokenå·²å­˜åœ¨
            print(f"âœ… ç‰¹æ®Štokenæ£€æŸ¥é€šè¿‡: {self.recall_token} (ID: {self.recall_token_id})")
    
    def _check_special_token(self):
        """æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦å­˜åœ¨ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨_check_and_add_special_tokenä»£æ›¿ï¼‰"""
        # è¿™ä¸ªæ–¹æ³•ä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹æ€§ï¼Œä½†å®é™…è°ƒç”¨çš„æ˜¯_check_and_add_special_token
        self._check_and_add_special_token()
    
    def _save_original_embedding(self):
        """ä¿å­˜åŸå§‹embeddingå‚æ•°ï¼ˆç”¨äºè®­ç»ƒåå¯¹æ¯”ï¼‰"""
        # ä½¿ç”¨get_input_embeddings()æ–¹æ³•è·å–embeddingå±‚ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰
        embedding_layer = self.model.get_input_embeddings()
        self.original_recall_embedding = embedding_layer.weight[self.recall_token_id].clone().detach()
        print(f"ğŸ“ å·²ä¿å­˜åŸå§‹embeddingå‚æ•°")
        print(f"   åŸå§‹embeddingèŒƒå›´: [{self.original_recall_embedding.min().item():.6f}, {self.original_recall_embedding.max().item():.6f}]")
    
    def _setup_lora(self):
        """è®¾ç½®LoRAé…ç½®"""
        print("âš¡ é…ç½®LoRA...")
        print(f"   LoRAå‚æ•°: r={self.lora_r}, alpha={self.lora_alpha}, dropout={self.lora_dropout}")
        
        # ç¡®å®štarget_modules
        if self.lora_target_modules is None:
            # é»˜è®¤é…ç½®ï¼šæ‰€æœ‰æ¨¡å—
            target_modules = [
                "q_proj", "v_proj", "k_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]
        else:
            # ä½¿ç”¨ä¼ å…¥çš„é…ç½®
            target_modules = self.lora_target_modules
        
        print(f"   LoRAç›®æ ‡æ¨¡å—: {target_modules}")
        print(f"   æ¨¡å—æ•°é‡: {len(target_modules)} (é»˜è®¤7ä¸ªï¼Œå½“å‰{len(target_modules)}ä¸ª)")
        if len(target_modules) < 7:
            reduction = (1 - len(target_modules) / 7) * 100
            print(f"   âš¡ LoRAå‚æ•°å‡å°‘çº¦ {reduction:.1f}%ï¼Œæ˜¾å­˜å ç”¨ç›¸åº”å‡å°‘")
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            lora_dropout=self.lora_dropout,
            target_modules=target_modules
        )
        
        self.model = get_peft_model(self.model, lora_config)
        print(f"âœ… LoRAé…ç½®å®Œæˆ")
        
        # åˆ›å»ºLoRAåæ¸…ç†æ˜¾å­˜ï¼ˆLoRAåŒ…è£…å¯èƒ½äº§ç”Ÿä¸´æ—¶çŠ¶æ€ï¼‰
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
        
        # å†æ¬¡æ£€æŸ¥æ•°æ®ç±»å‹å’Œè®¾å¤‡
        first_param = next(self.model.parameters())
        model_dtype = first_param.dtype
        model_device = first_param.device
        print(f"ğŸ”§ LoRAåæ¨¡å‹æ•°æ®ç±»å‹: {model_dtype}, è®¾å¤‡: {model_device}")
        
        # é‡è¦ï¼šåœ¨LoRAè®¾ç½®åæ‰§è¡Œè§£å†»æ“ä½œ
        self._freeze_embeddings_except_special_tokens()

    def _freeze_embeddings_except_special_tokens(self):
        """å†»ç»“é™¤äº†ç‰¹æ®Štokenä»¥å¤–çš„æ‰€æœ‰embeddingå‚æ•° - ä¿®å¤ç‰ˆ"""
        print("ğŸ§Š å†»ç»“é™¤ç‰¹æ®Štokenå¤–çš„æ‰€æœ‰embeddingå‚æ•°...")
        
        # è·å–æ­£ç¡®çš„embeddingå±‚ - ä½¿ç”¨get_input_embeddings()æ–¹æ³•ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰
        # å¯¹äºQwen3-VLæ¨¡å‹ï¼Œè¿™ä¼šè‡ªåŠ¨æ‰¾åˆ°æ­£ç¡®çš„embeddingå±‚
        embedding_layer = self.model.get_input_embeddings()
        
        # é¦–å…ˆå†»ç»“æ‰€æœ‰embeddingå‚æ•°
        embedding_layer.weight.requires_grad_(False)
        
        # ç„¶ååªè§£å†»ç‰¹æ®Štokençš„embeddingå‚æ•°
        embedding_layer.weight[self.recall_token_id].requires_grad_(True)
        
        # å¼ºåˆ¶è®¾ç½®requires_gradæ ‡å¿—
        embedding_layer.weight.requires_grad = True
        
        # éªŒè¯è®¾ç½®æˆåŠŸ
        total_embedding_params = embedding_layer.weight.numel()
        trainable_params = embedding_layer.weight[self.recall_token_id].numel()
        
        # éªŒè¯ç¡®å®å¯è®­ç»ƒ
        is_trainable = embedding_layer.weight[self.recall_token_id].requires_grad
        
        print(f"âœ… embeddingå±‚è®¾ç½®å®Œæˆ:")
        print(f"   æ€»embeddingå‚æ•°: {total_embedding_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({self.recall_token} token only)")
        print(f"   å†»ç»“å‚æ•°: {total_embedding_params - trainable_params:,}")
        print(f"   ç‰¹æ®Štoken embeddingæ˜¯å¦å¯è®­ç»ƒ: {is_trainable}")
        
        # è°ƒè¯•ä¿¡æ¯
        if not is_trainable:
            print("âš ï¸ è­¦å‘Š: ç‰¹æ®Štoken embeddingæ— æ³•è®¾ç½®ä¸ºå¯è®­ç»ƒï¼")
            print("å°è¯•ä½¿ç”¨ä»¥ä¸‹å¤‡ç”¨æ–¹æ³•...")
            
            # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥ä¿®æ”¹å‚æ•°çš„requires_gradå±æ€§
            param_pointer = embedding_layer.weight[self.recall_token_id]
            param_pointer.requires_grad = True
            print(f"   å†æ¬¡æ£€æŸ¥: {param_pointer.requires_grad}")
    
    def _print_trainable_parameters(self):
        """æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°ç»Ÿè®¡ - ä¿®å¤ç‰ˆ"""
        print("ğŸ“Š å‚æ•°ç»Ÿè®¡ (ä»…ç‰¹æ®Štoken embeddingå¯è®­ç»ƒ):")
        
        # è·å–æ­£ç¡®çš„embeddingå±‚è·¯å¾„ - ä½¿ç”¨get_input_embeddings()æ–¹æ³•
        try:
            embedding_layer = self.model.get_input_embeddings()
            special_token_embedding = embedding_layer.weight[self.recall_token_id]
        except:
            try:
                # å°è¯•å¦ä¸€ç§å¯èƒ½çš„è·¯å¾„
                embedding_layer = self.model.get_input_embeddings()
                special_token_embedding = embedding_layer.weight[self.recall_token_id]
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è·å–embeddingå±‚: {e}")
                embedding_layer = None
        
        # ç»Ÿè®¡å‚æ•°
        lora_params = 0
        embedding_params = 0
        other_params = 0
        
        # æ£€æŸ¥embeddingå±‚æ˜¯å¦å¯è®­ç»ƒ
        if embedding_layer is not None:
            is_trainable = special_token_embedding.requires_grad
            if is_trainable:
                embedding_params = special_token_embedding.numel()
        
        # ç»Ÿè®¡æ‰€æœ‰å‚æ•°
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                if 'lora' in name.lower():
                    lora_params += param.numel()
                elif 'embed' in name.lower() and 'embed_tokens.weight' in name:
                    # å·²åœ¨å‰é¢è®¡ç®—ï¼Œä¸é‡å¤è®¡ç®—
                    pass
                else:
                    other_params += param.numel()
        
        total_trainable = lora_params + embedding_params + other_params
        total_params = sum(p.numel() for p in self.model.parameters())
        
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {total_trainable:,}")
        print(f"     - LoRAå‚æ•°: {lora_params:,}")
        print(f"     - ç‰¹æ®ŠToken Embeddingå‚æ•°: {embedding_params:,}")
        print(f"     - å…¶ä»–å‚æ•°: {other_params:,}")
        print(f"   å¯è®­ç»ƒæ¯”ä¾‹: {100 * total_trainable / total_params:.4f}%")
        
        # æ£€æŸ¥ç‰¹æ®Štokençš„embeddingçŠ¶æ€
        if embedding_layer is not None:
            print(f"\nğŸ¯ {self.recall_token} tokençŠ¶æ€:")
            print(f"   Token ID: {self.recall_token_id}")
            print(f"   Requires grad: {special_token_embedding.requires_grad}")
            print(f"   å½“å‰å€¼èŒƒå›´: [{special_token_embedding.min().item():.6f}, {special_token_embedding.max().item():.6f}]")
    
    def load_data(self, pt_file_path):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        print(f"ğŸ“– åŠ è½½æ•°æ®: {pt_file_path}")
        
        if not os.path.exists(pt_file_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {pt_file_path}")
        
        data = torch.load(pt_file_path, map_location='cpu')
        texts = data['texts']
        embeddings = data['embeddings']

        # ä¿å­˜æ•°æ®å…ƒä¿¡æ¯ï¼ˆç”¨äºæ•°æ®é›†åˆ’åˆ†ï¼‰
        self.data_info = {
            'memory_count': data.get('memory_count', len(texts)),
            'sft_count': data.get('sft_count', 0)
        }
        
        print(f"   æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"   åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
        print(f"   åŸå§‹embeddingæ•°æ®ç±»å‹: {embeddings.dtype}")
        print(f"   æ•°æ®ç»„æˆ: {self.data_info['memory_count']} æ¡è®°å¿†æ¡ç›® + {self.data_info['sft_count']} æ¡SFTå‘é‡")
        
        return texts, embeddings
    
    def create_dataloader(self, texts, embeddings, batch_size=2, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä½¿ç”¨åŠ¨æ€paddingä»¥èŠ‚çœæ˜¾å­˜"""

        def collate_fn(batch):
            """åŠ¨æ€paddingï¼šæ ¹æ®batchå†…æœ€é•¿åºåˆ—è¿›è¡Œpadding"""
            if not batch:
                return {}

            # æ‰¾å‡ºbatchå†…æœ€é•¿åºåˆ—çš„é•¿åº¦
            max_len = max(item['seq_len'] for item in batch)

            # å¯¹æ‰€æœ‰åºåˆ—è¿›è¡Œpaddingåˆ°max_len
            padded_input_ids = []
            padded_attention_masks = []
            recall_positions = []
            target_embeddings = []

            for item in batch:
                input_ids = item['input_ids']
                attention_mask = item['attention_mask']
                recall_pos = item['recall_position']
                target_emb = item['target_embedding']

                # padding åˆ° max_len
                pad_len = max_len - len(input_ids)
                if pad_len > 0:
                    # ä½¿ç”¨tokenizerçš„pad_token_idè¿›è¡Œpadding
                    tokenizer = self._get_tokenizer()
                    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
                    padded_input_ids.append(torch.cat([input_ids, torch.full((pad_len,), pad_token_id, dtype=input_ids.dtype)]))
                    padded_attention_masks.append(torch.cat([attention_mask, torch.zeros(pad_len, dtype=attention_mask.dtype)]))
                else:
                    padded_input_ids.append(input_ids)
                    padded_attention_masks.append(attention_mask)

                recall_positions.append(recall_pos)
                target_embeddings.append(target_emb)

            # å †å æˆbatch
            # ç¡®ä¿æ‰€æœ‰tensoråœ¨CPUä¸Šï¼Œè®¾å¤‡åˆ†é…ç”±Acceleratorç»Ÿä¸€å¤„ç†
            # æ³¨æ„ï¼šrecall_positionsä¸­çš„å…ƒç´ æ˜¯tensorï¼ˆæ ‡é‡tensorï¼‰ï¼Œæ‰€ä»¥ç›´æ¥stackå³å¯
            batch_dict = {
                'input_ids': torch.stack(padded_input_ids),
                'attention_mask': torch.stack(padded_attention_masks),
                'recall_position': torch.stack(recall_positions),  # recall_positionsæ˜¯æ ‡é‡tensoråˆ—è¡¨ï¼Œç›´æ¥stack
                'target_embedding': torch.stack(target_embeddings)
            }
            
            # ç¡®ä¿æ‰€æœ‰tensoråœ¨CPUä¸Šï¼ˆAcceleratorä¼šè‡ªåŠ¨ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡ï¼‰
            for key, value in batch_dict.items():
                if isinstance(value, torch.Tensor) and value.is_cuda:
                    batch_dict[key] = value.cpu()
            
            return batch_dict

        dataset = RecallDataset(texts, embeddings, self.tokenizer, self.model, max_length=self.max_length)
        # ä½¿ç”¨æ ‡å‡†çš„æ•°æ®åŠ è½½ä¼˜åŒ–ï¼špin_memoryå’Œnum_workers
        # pin_memory=True: å°†æ•°æ®å›ºå®šåœ¨CPUå†…å­˜ä¸­ï¼ŒåŠ é€ŸGPUä¼ è¾“
        # num_workers=0: åœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½æ•°æ®ï¼Œé¿å…å¤šè¿›ç¨‹å¯¼è‡´çš„æ˜¾å­˜é—®é¢˜
        return DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=shuffle, 
            collate_fn=collate_fn,
            pin_memory=True,  # å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
            num_workers=0,    # åœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½ï¼Œé¿å…å¤šè¿›ç¨‹æ˜¾å­˜é—®é¢˜
            persistent_workers=False  # ä¸æŒä¹…åŒ–workerï¼ŒèŠ‚çœå†…å­˜
        )

    def compute_loss(self, last_hidden_states, recall_positions, target_embeddings):
        """è®¡ç®—æŸå¤±ï¼šrecall tokenåµŒå…¥ä¸ç›®æ ‡åµŒå…¥çš„MSEæŸå¤±
        
        å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡ç´¢å¼•ç›´æ¥æå–éœ€è¦çš„tokenä½ç½®ï¼Œæœ€å°åŒ–è®¡ç®—å›¾
        """
        # å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡ç´¢å¼•ä¸€æ¬¡æ€§æå–æ‰€æœ‰éœ€è¦çš„tokenä½ç½®
        # è¿™æ¯”å¾ªç¯æå–æ›´é«˜æ•ˆï¼Œä¸”è®¡ç®—å›¾æ›´å°
        batch_size = last_hidden_states.size(0)
        batch_indices = torch.arange(batch_size, device=last_hidden_states.device)
        
        # æ‰¹é‡æå–ï¼šç›´æ¥ç´¢å¼•ï¼Œåªä¿ç•™è¿™äº›ä½ç½®çš„è®¡ç®—å›¾
        recall_embeddings = last_hidden_states[batch_indices, recall_positions, :]  # [batch_size, hidden_dim]

        # ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        target_embeddings = target_embeddings.to(recall_embeddings.dtype)

        # è®¡ç®—MSEæŸå¤±
        loss = nn.MSELoss()(recall_embeddings, target_embeddings)
        return loss
    
    def train_epoch(self, dataloader, optimizer, epoch_idx=0):
        """è®­ç»ƒä¸€ä¸ªepoch - ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§"""
        self.model.train()
        total_loss = 0
        
        # è·å–æ¨¡å‹å½“å‰è®¾å¤‡
        model_device = next(self.model.parameters()).device
        
        progress_bar = tqdm(dataloader, desc="è®­ç»ƒ", disable=not self.is_main_process())
        
        for batch in progress_bar:
            # ç¡®ä¿æ‰€æœ‰æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            input_ids = batch['input_ids'].to(model_device)
            attention_mask = batch['attention_mask'].to(model_device)
            recall_positions = batch['recall_position']
            target_embeddings = batch['target_embedding'].to(model_device)
            
            # å‰å‘ä¼ æ’­ï¼ˆç›´æ¥èµ°backboneä»¥è·å–last_hidden_stateï¼‰
            backbone_outputs = forward_backbone(
                self.model,
                input_ids=input_ids,
                attention_mask=attention_mask,
                use_cache=False,
                output_hidden_states=False,
                return_dict=True,
            )
            last_hidden_states = ensure_last_hidden_state(backbone_outputs)
            
            # ä½¿ç”¨ä¼˜åŒ–çš„compute_lossæ–¹æ³•
            loss = self.compute_loss(last_hidden_states, recall_positions, target_embeddings)
            
            # ç«‹å³æ¸…ç†backbone outputsï¼Œé‡Šæ”¾æ˜¾å­˜
            del backbone_outputs
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            # ä½¿ç”¨ Accelerator è¿›è¡Œåä¼ 
            self.accelerator.backward(loss)
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            if self.is_main_process():
                progress_bar.set_postfix({'loss': f'{loss.item():.6f}'})
        
        return total_loss / len(dataloader)
    
    def evaluate(self, dataloader):
        """è¯„ä¼°æ¨¡å‹ - ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§"""
        self.model.eval()
        total_loss = 0
        total_cosine_sim = 0
        
        # è·å–æ¨¡å‹å½“å‰è®¾å¤‡
        model_device = next(self.model.parameters()).device
        
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch['input_ids'].to(model_device)
                attention_mask = batch['attention_mask'].to(model_device)
                recall_positions = batch['recall_position']
                target_embeddings = batch['target_embedding'].to(model_device)
                
                backbone_outputs = forward_backbone(
                    self.model,
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    use_cache=False,
                    output_hidden_states=False,
                    return_dict=True,
                )
                last_hidden_states = ensure_last_hidden_state(backbone_outputs)
                
                # ä½¿ç”¨ä¼˜åŒ–çš„compute_lossæ–¹æ³•
                loss = self.compute_loss(last_hidden_states, recall_positions, target_embeddings)
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆéœ€è¦é‡æ–°æå–ï¼Œä½†éªŒè¯é˜¶æ®µæ˜¾å­˜å‹åŠ›è¾ƒå°ï¼‰
                batch_size = last_hidden_states.size(0)
                batch_indices = torch.arange(batch_size, device=last_hidden_states.device)
                recall_embeddings = last_hidden_states[batch_indices, recall_positions, :]
                target_embeddings = target_embeddings.to(recall_embeddings.dtype)
                cosine_sim = nn.CosineSimilarity(dim=-1)(recall_embeddings, target_embeddings).mean()
                
                # ç«‹å³æ¸…ç†outputsï¼Œé‡Šæ”¾æ˜¾å­˜
                del backbone_outputs
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                total_loss += loss.item()
                total_cosine_sim += cosine_sim.item()
        
        avg_loss = total_loss / len(dataloader)
        avg_cosine_sim = total_cosine_sim / len(dataloader)
        
        return avg_loss, avg_cosine_sim
    
    def compare_embeddings(self):
        """æ¯”è¾ƒè®­ç»ƒå‰åembeddingçš„å˜åŒ–"""
        print("\nğŸ” åˆ†æembeddingå˜åŒ–...")
        
        # ä»åˆå¹¶åçš„æ¨¡å‹è·å–embedding
        current_embedding = self.merged_model.get_input_embeddings().weight[self.recall_token_id]
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        if self.original_recall_embedding.dtype != current_embedding.dtype:
            original_embedding = self.original_recall_embedding.to(current_embedding.dtype)
        else:
            original_embedding = self.original_recall_embedding
        
        # è®¡ç®—å˜åŒ–
        change = torch.abs(current_embedding - original_embedding).mean().item()
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cosine_sim = nn.CosineSimilarity(dim=0)(
            current_embedding, 
            original_embedding
        ).item()
        
        print(f"   {self.recall_token} embeddingå¹³å‡å˜åŒ–: {change:.6f}")
        print(f"   è®­ç»ƒå‰åä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim:.6f}")
        print(f"   è®­ç»ƒå‰èŒƒå›´: [{original_embedding.min().item():.4f}, {original_embedding.max().item():.4f}]")
        print(f"   è®­ç»ƒåèŒƒå›´: [{current_embedding.min().item():.4f}, {current_embedding.max().item():.4f}]")
        
        return {
            'change': change,
            'cosine_similarity': cosine_sim,
            'before_range': (original_embedding.min().item(), original_embedding.max().item()),
            'after_range': (current_embedding.min().item(), current_embedding.max().item())
        }
    
    def merge_and_save_model(self, save_path):
        """åˆå¹¶LoRAæƒé‡å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹"""
        if not self.is_main_process():
            return None
        print("ğŸ”„ åˆå¹¶LoRAæƒé‡...")

        # åˆå¹¶æƒé‡
        base_model = self.accelerator.unwrap_model(self.model) if hasattr(self, 'accelerator') else self.model
        merged_model = base_model.merge_and_unload()

        if os.path.isdir(save_path):
            print(f"ğŸ§¹ æ¸…ç†å·²æœ‰çš„æ¨¡å‹è¾“å‡ºç›®å½•: {save_path}")
            import shutil
            shutil.rmtree(save_path)

        os.makedirs(save_path, exist_ok=True)

        # ç”±äºPEFTçŠ¶æ€å¯èƒ½æ··ä¹±ï¼Œå°è¯•ä¸åŒçš„ä¿å­˜æ–¹å¼
        try:
            # é¦–å…ˆå°è¯•æ­£å¸¸ä¿å­˜
            merged_model.save_pretrained(save_path)
        except Exception as e:
            print(f"âš ï¸ æ­£å¸¸ä¿å­˜å¤±è´¥: {e}ï¼Œå°è¯•å¤‡ç”¨ä¿å­˜æ–¹æ³•...")
            # å¦‚æœæ­£å¸¸ä¿å­˜å¤±è´¥ï¼Œå°è¯•ç¦ç”¨adapterç›¸å…³çš„ä¿å­˜
            try:
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ¨¡å‹ï¼Œç§»é™¤æ‰€æœ‰PEFTç›¸å…³å±æ€§
                import copy
                temp_model = copy.deepcopy(merged_model)
                # ç§»é™¤å¯èƒ½å¯¼è‡´é—®é¢˜çš„PEFTå±æ€§
                peft_attrs = ['peft_config', 'active_adapters', 'adapter_config']
                for attr in peft_attrs:
                    if hasattr(temp_model, attr):
                        delattr(temp_model, attr)

                temp_model.save_pretrained(save_path)
                print("âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ä¿å­˜æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ å¤‡ç”¨ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")
                raise e  # æŠ›å‡ºåŸå§‹é”™è¯¯

        tokenizer = self._get_tokenizer()

        # ç¡®ä¿ç‰¹æ®Štokenåœ¨è¯æ±‡è¡¨ä¸­ï¼ˆè°ƒè¯•ç”¨ï¼‰
        print("ğŸ” ä¿å­˜æ—¶æ£€æŸ¥ç‰¹æ®Štoken...")
        special_tokens = ["<recall>", "</recall>"]
        for token in special_tokens:
            if token in tokenizer.get_vocab():
                token_id = tokenizer.convert_tokens_to_ids(token)
                print(f"   âœ… {token} å­˜åœ¨ (ID: {token_id})")
            else:
                print(f"   âŒ {token} ä¸å­˜åœ¨äºè¯æ±‡è¡¨ä¸­ï¼")
                # é‡æ–°æ·»åŠ 
                num_added = tokenizer.add_tokens([token], special_tokens=True)
                if num_added > 0:
                    print(f"   ğŸ”§ é‡æ–°æ·»åŠ äº† {token}")

        # ç¡®ä¿ç‰¹æ®Štokenåœ¨ç‰¹æ®Štokenåˆ—è¡¨ä¸­
        if hasattr(tokenizer, 'special_tokens_map'):
            additional_special = tokenizer.special_tokens_map.get('additional_special_tokens', [])
            for token in special_tokens:
                if token not in additional_special:
                    print(f"   âš ï¸ {token} ä¸åœ¨ç‰¹æ®Štokenåˆ—è¡¨ä¸­ï¼Œé‡æ–°æ·»åŠ ")
                    if hasattr(tokenizer, 'add_special_tokens'):
                        tokenizer.add_special_tokens({"additional_special_tokens": [token]})

        tokenizer.save_pretrained(save_path)

        print(f"âœ… åˆå¹¶åçš„å®Œæ•´æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

        # ä¿å­˜å®Œæˆåç«‹å³æ¸…ç†merged_modelå¼•ç”¨ï¼Œé¿å…å†…å­˜æ³„æ¼
        try:
            merged_model.cpu()
        except:
            pass
        del merged_model

        return save_path  # è¿”å›ä¿å­˜è·¯å¾„ä¾›åç»­è®­ç»ƒä½¿ç”¨

    def cleanup(self):
        """æ¸…ç†è®­ç»ƒå™¨åˆ›å»ºçš„æ‰€æœ‰æ¨¡å‹å®ä¾‹ï¼ˆæ”¯æŒå¤šGPUï¼‰"""
        print("ğŸ§¹ æ¸…ç†è®­ç»ƒå™¨æ¨¡å‹å®ä¾‹...")

        try:
            # æ¸…ç†merged_modelï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(self, 'merged_model') and self.merged_model is not None:
                try:
                    # å¯¹äºå¤šGPUæ¨¡å‹ï¼Œéœ€è¦æ›´å½»åº•çš„æ¸…ç†
                    if hasattr(self.merged_model, 'hf_device_map') and self.merged_model.hf_device_map:
                        print("æ£€æµ‹åˆ°å¤šGPUæ¨¡å‹ï¼ˆmerged_modelï¼‰ï¼Œæ‰§è¡Œå½»åº•æ¸…ç†...")
                    self.merged_model.cpu()
                except:
                    pass
                del self.merged_model
                self.merged_model = None

            # æ¸…ç†LoRAåŒ…è£…æ¨¡å‹
            if hasattr(self, 'model') and self.model is not None:
                try:
                    # å¯¹äºå¤šGPUæ¨¡å‹ï¼Œéœ€è¦æ›´å½»åº•çš„æ¸…ç†
                    if hasattr(self.model, 'hf_device_map') and self.model.hf_device_map:
                        print("æ£€æµ‹åˆ°å¤šGPUæ¨¡å‹ï¼ˆLoRA modelï¼‰ï¼Œæ‰§è¡Œå½»åº•æ¸…ç†...")
                    self.model.cpu()
                except:
                    pass
                del self.model
                self.model = None

            # æ¸…ç†tokenizerï¼ˆå¦‚æœä¸æ˜¯é¢„åŠ è½½çš„ï¼‰
            if hasattr(self, 'tokenizer') and self.tokenizer is not None and not getattr(self, '_skip_model_loading', False):
                del self.tokenizer
                self.tokenizer = None

            # æ¸…ç†acceleratorï¼ˆé‡è¦ï¼šacceleratorå¯èƒ½æŒæœ‰æ¨¡å‹å¼•ç”¨ï¼‰
            if hasattr(self, 'accelerator') and self.accelerator is not None:
                try:
                    # å…ˆå°è¯•é‡Šæ”¾acceleratorç®¡ç†çš„æ˜¾å­˜
                    self.accelerator.free_memory()
                    # å¦‚æœacceleratoræœ‰æ¨¡å‹å¼•ç”¨ï¼Œä¹Ÿéœ€è¦æ¸…ç†
                    if hasattr(self.accelerator, 'device'):
                        print(f"æ¸…ç†acceleratorç®¡ç†çš„è®¾å¤‡: {self.accelerator.device}")
                except Exception as e:
                    print(f"æ¸…ç†acceleratoræ—¶å‡ºç°è­¦å‘Š: {e}")
                # æ³¨æ„ï¼šacceleratorå®ä¾‹æœ¬èº«é€šå¸¸ä¸éœ€è¦æ˜¾å¼åˆ é™¤

            # å¼ºåˆ¶åƒåœ¾å›æ”¶å’Œæ˜¾å­˜æ¸…ç†ï¼ˆå¤šæ¬¡æ¸…ç†ç¡®ä¿å½»åº•ï¼‰
            import gc
            for _ in range(5):  # å¢åŠ æ¸…ç†æ¬¡æ•°
                gc.collect()

            import torch
            if torch.cuda.is_available():
                # æ ¹æ®CUDA_VISIBLE_DEVICESè®¾ç½®å†³å®šæ¸…ç†ç­–ç•¥
                current_cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES')
                if current_cuda_visible and ',' not in current_cuda_visible:
                    # å•GPUæ¨¡å¼ï¼Œåªæ¸…ç†GPU 0ï¼ˆå› ä¸ºCUDA_VISIBLE_DEVICESé‡æ–°æ˜ å°„äº†ï¼‰
                    print(f"å•GPUæ¨¡å¼: åªæ¸…ç†å¯è§GPU 0")
                    try:
                        torch.cuda.synchronize()
                        torch.cuda.empty_cache()
                        torch.cuda.reset_peak_memory_stats()
                        print(f"âœ… å·²æ¸…ç†å¯è§GPUçš„æ˜¾å­˜")
                    except Exception as e:
                        print(f"æ¸…ç†GPUæ˜¾å­˜æ—¶å‡ºç°è­¦å‘Š: {e}")
                else:
                    # å¤šGPUæˆ–æœªè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œæ¸…ç†æ‰€æœ‰GPU
                    gpu_count = torch.cuda.device_count()
                    print(f"æ¸…ç† {gpu_count} å¼ GPUçš„æ˜¾å­˜...")
                    
                    # åŒæ­¥å¹¶æ¸…ç†æ‰€æœ‰GPU
                    for i in range(gpu_count):
                        try:
                            with torch.cuda.device(i):
                                torch.cuda.synchronize()
                                torch.cuda.empty_cache()
                                torch.cuda.reset_peak_memory_stats()
                        except Exception as e:
                            print(f"æ¸…ç†GPU {i} æ—¶å‡ºç°è­¦å‘Š: {e}")
                    
                    # å†æ¬¡æ¸…ç†æ‰€æœ‰GPU
                    for i in range(gpu_count):
                        try:
                            with torch.cuda.device(i):
                                torch.cuda.empty_cache()
                        except Exception as e:
                            print(f"æ¸…ç†GPU {i} æ—¶å‡ºç°è­¦å‘Š: {e}")
                    
                    print(f"âœ… å·²æ¸…ç†æ‰€æœ‰ {gpu_count} å¼ GPUçš„æ˜¾å­˜")

            print("âœ… è®­ç»ƒå™¨æ¸…ç†å®Œæˆ")

            # æ¢å¤åŸå§‹CUDA_VISIBLE_DEVICESï¼ˆå¦‚æœåœ¨åˆå§‹åŒ–æ—¶ä¿®æ”¹è¿‡ï¼‰
            if hasattr(self, '_original_cuda_visible_devices') and self._original_cuda_visible_devices is not None:
                original_value = self._original_cuda_visible_devices
                os.environ['CUDA_VISIBLE_DEVICES'] = original_value
                print(f"æ¢å¤åŸå§‹CUDA_VISIBLE_DEVICES: {original_value}")
            elif 'CUDA_VISIBLE_DEVICES' in os.environ and hasattr(self, '_original_cuda_visible_devices'):
                # å¦‚æœåˆå§‹åŒ–æ—¶è®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œç°åœ¨åˆ é™¤å®ƒ
                del os.environ['CUDA_VISIBLE_DEVICES']
                print("åˆ é™¤CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡")

        except Exception as e:
            print(f"âš ï¸ æ¸…ç†è®­ç»ƒå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")

    def train(self, pt_file_path, num_epochs=10, batch_size=2, learning_rate=1e-4, save_path="recall_model"):
        """å®Œæ•´è®­ç»ƒæµç¨‹ - è®­ç»ƒ/éªŒè¯é›†æ¨¡å¼"""
        if self.is_main_process():
            print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {self.recall_token} token")
        print(f"   æ•°æ®æ–‡ä»¶: {pt_file_path}")
        print(f"   è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   ä¿å­˜è·¯å¾„: {save_path}")

        # åŠ è½½æ•°æ®
        texts, embeddings = self.load_data(pt_file_path)

        if self.is_main_process():
            print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
            print(f"   æ€»æ ·æœ¬æ•°: {len(texts)}")

        # åˆ†ç¦»è®­ç»ƒé›†å’ŒéªŒè¯é›†
        # æ•°æ®æ ¼å¼ï¼šè®°å¿†æ¡ç›®å‘é‡åœ¨å‰ï¼ŒSFTå‘é‡åœ¨å
        # éªŒè¯é›†åªåŒ…å«ä»SFTå‘é‡ä¸­éšæœºæŠ½å–çš„éƒ¨åˆ†

        # ä»æ•°æ®ä¸­æå–å…ƒä¿¡æ¯
        data_info = self.data_info if hasattr(self, 'data_info') else {}
        memory_count = data_info.get('memory_count', len(texts))  # å¦‚æœæ²¡æœ‰å…ƒä¿¡æ¯ï¼Œå‡è®¾éƒ½æ˜¯è®°å¿†æ¡ç›®
        sft_count = data_info.get('sft_count', 0)

        if sft_count > 0:
            # æœ‰SFTå‘é‡ï¼šæŒ‰ç…§è¦æ±‚åˆ’åˆ†
            # å¦‚æœæœ‰xæ¡è®°å¿†æ¡ç›®ï¼Œåº”è¯¥æœ‰1.5xæ¡SFTå‘é‡ï¼Œå…¶ä¸­0.5xç”¨äºéªŒè¯ï¼Œ1.0xç”¨äºè®­ç»ƒ
            memory_indices = list(range(memory_count))  # æ‰€æœ‰è®°å¿†æ¡ç›®
            sft_indices = list(range(memory_count, memory_count + sft_count))  # SFTå‘é‡ç´¢å¼•

            # è®¡ç®—éªŒè¯é›†å’Œè®­ç»ƒé›†çš„SFTæ•°é‡
            # ç†æƒ³æƒ…å†µï¼šéªŒè¯é›†0.5å€è®°å¿†æ¡ç›®æ•°é‡ï¼Œè®­ç»ƒé›†1.0å€è®°å¿†æ¡ç›®æ•°é‡
            ideal_val_sft_size = int(memory_count * 0.5)
            ideal_train_sft_size = memory_count
            ideal_total_sft = ideal_val_sft_size + ideal_train_sft_size
            
            # å¦‚æœSFTæ•°é‡ä¸è¶³ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…
            if sft_count < ideal_total_sft:
                # æŒ‰æ¯”ä¾‹åˆ†é…ï¼šéªŒè¯é›†å 1/3ï¼Œè®­ç»ƒé›†å 2/3
                val_sft_size = max(1, int(sft_count / 3))
                train_sft_size = sft_count - val_sft_size
            else:
                # SFTæ•°é‡å……è¶³ï¼Œä½¿ç”¨ç†æƒ³åˆ†é…
                val_sft_size = ideal_val_sft_size
                train_sft_size = ideal_train_sft_size
                # å¦‚æœè¿˜æœ‰å‰©ä½™ï¼Œä¼˜å…ˆåˆ†é…ç»™è®­ç»ƒé›†
                if sft_count > ideal_total_sft:
                    train_sft_size += (sft_count - ideal_total_sft)

            # åˆ’åˆ†SFTå‘é‡ï¼šå‰train_sft_sizeç”¨äºè®­ç»ƒï¼Œåval_sft_sizeç”¨äºéªŒè¯
            train_sft_indices = sft_indices[:train_sft_size]
            val_sft_indices = sft_indices[train_sft_size:train_sft_size + val_sft_size]

            # è®­ç»ƒé›†ï¼šæ‰€æœ‰è®°å¿†æ¡ç›® + è®­ç»ƒç”¨çš„SFTå‘é‡
            train_indices = memory_indices + train_sft_indices
            # éªŒè¯é›†ï¼šéªŒè¯ç”¨çš„SFTå‘é‡
            val_indices = val_sft_indices
        else:
            # æ²¡æœ‰SFTå‘é‡ï¼šä½¿ç”¨ç®€å•çš„åˆ†å‰²
            total_samples = len(texts)
            val_size = max(1, total_samples // 5)  # 20%ä½œä¸ºéªŒè¯é›†
            train_indices = list(range(total_samples - val_size))
            val_indices = list(range(total_samples - val_size, total_samples))

        train_texts = [texts[i] for i in train_indices]
        train_embeddings = embeddings[train_indices]
        val_texts = [texts[i] for i in val_indices]
        val_embeddings = embeddings[val_indices]

        if self.is_main_process():
            if sft_count > 0:
                print(f"   æ•°æ®åˆ’åˆ†è¯¦æƒ…:")
                print(f"     - è®°å¿†æ¡ç›®: {memory_count} æ¡ï¼ˆå…¨éƒ¨ç”¨äºè®­ç»ƒï¼‰")
                print(f"     - SFTå‘é‡æ€»æ•°: {sft_count} æ¡")
                print(f"     - è®­ç»ƒé›†SFT: {len(train_sft_indices)} æ¡")
                print(f"     - éªŒè¯é›†SFT: {len(val_sft_indices)} æ¡")
                print(f"   è®­ç»ƒé›†: {len(train_texts)} æ ·æœ¬ï¼ˆ{memory_count} æ¡è®°å¿† + {len(train_sft_indices)} æ¡SFTï¼‰")
                print(f"   éªŒè¯é›†: {len(val_texts)} æ ·æœ¬ï¼ˆ{len(val_sft_indices)} æ¡SFTï¼‰")
            else:
                print(f"   è®­ç»ƒé›†: {len(train_texts)} æ ·æœ¬")
                print(f"   éªŒè¯é›†: {len(val_texts)} æ ·æœ¬")

        # è®­ç»ƒæ¨¡å‹
        best_loss = self._train_model(
            train_texts, train_embeddings,
            val_texts, val_embeddings,
            num_epochs, batch_size, learning_rate,
            save_path
        )

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        if self.is_main_process():
            final_model_path = save_path
            print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")

        return save_path

    def _train_model(self, train_texts, train_embeddings, val_texts, val_embeddings,
                    num_epochs, batch_size, learning_rate, save_path):
        """è®­ç»ƒå•ä¸ªæŠ˜"""
        # æ¸…ç†ä¸Šä¸€æŠ˜å¯èƒ½æ®‹ç•™çš„æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self.create_dataloader(train_texts, train_embeddings, batch_size, True)
        val_loader = self.create_dataloader(val_texts, val_embeddings, batch_size, False)

        # ç¡®ä¿æ¨¡å‹åªè¢«AcceleratoråŒ…è£…ä¸€æ¬¡ï¼Œé¿å…å¤šæ¬¡prepareå¯¼è‡´æ˜¾å­˜ç¿»å€
        self._prepare_model_once()

        # ä¼˜åŒ–å™¨
        optimizer_params = [p for p in self.model.parameters() if p.requires_grad]

        # ç¡®ä¿ç‰¹æ®Štoken embeddingè¢«åŒ…å«
        embedding_layer = self.model.get_input_embeddings()
        special_token_embedding = embedding_layer.weight[self.recall_token_id]
        if special_token_embedding.requires_grad == False:
            print("âš ï¸ ç‰¹æ®Štoken embeddingæœªè®¾ç½®ä¸ºå¯è®­ç»ƒï¼Œæ‰‹åŠ¨æ·»åŠ åˆ°ä¼˜åŒ–å™¨...")
            special_token_embedding.requires_grad_(True)
            optimizer_params.append(special_token_embedding)

        optimizer = optim.AdamW(
            optimizer_params,
            lr=learning_rate,
            weight_decay=0.01
        )

        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        model_save_path = save_path

        # è®© Accelerator æ¥ç®¡ä¼˜åŒ–å™¨ä¸æ•°æ®åŠ è½½å™¨ï¼ˆæ¨¡å‹å·²åœ¨é¦–æ¬¡æŠ˜ä¸­åŒ…è£…è¿‡ï¼‰
        optimizer, train_loader, val_loader = self.accelerator.prepare(
            optimizer, train_loader, val_loader
        )

        for epoch in range(num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            train_steps = 0
            accumulation_step = 0

            for batch in train_loader:
                # batch æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è¾“å…¥
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                recall_positions = batch['recall_position']
                target_embeddings = batch['target_embedding']

                # å‰å‘ä¼ æ’­
                backbone_outputs = forward_backbone(
                    self.model,
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    use_cache=False,
                    output_hidden_states=False,
                    return_dict=True,
                )
                last_hidden_states = ensure_last_hidden_state(backbone_outputs)
                loss = self.compute_loss(last_hidden_states, recall_positions, target_embeddings)

                # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°
                loss = loss / self.gradient_accumulation_steps

                # åå‘ä¼ æ’­
                self.accelerator.backward(loss)
                
                # é‡Šæ”¾backboneè¾“å‡ºå¼•ç”¨ï¼Œç«‹å³é‡Šæ”¾æ˜¾å­˜
                del backbone_outputs
                # æ¯éš”å‡ ä¸ªbatchæ¸…ç†ä¸€æ¬¡æ˜¾å­˜ç¼“å­˜ï¼ˆé¿å…é¢‘ç¹è°ƒç”¨å½±å“æ€§èƒ½ï¼Œä½†ç¡®ä¿æ˜¾å­˜å®‰å…¨ï¼‰
                if accumulation_step % max(1, self.gradient_accumulation_steps) == 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                accumulation_step += 1

                # æ¯accumulation_stepsæ­¥æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–å™¨æ­¥éª¤
                if accumulation_step % self.gradient_accumulation_steps == 0:
                    optimizer.step()
                    optimizer.zero_grad()
                    train_steps += 1
                    
                    # ä¼˜åŒ–å™¨æ­¥éª¤åæ¸…ç†æ˜¾å­˜ï¼ˆå…³é”®ä½ç½®ï¼Œç¡®ä¿æ¢¯åº¦æ›´æ–°åé‡Šæ”¾æ˜¾å­˜ï¼‰
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                # ç´¯ç§¯æŸå¤±ï¼ˆæ³¨æ„ï¼šè¿™é‡Œç´¯ç§¯çš„æ˜¯åŸå§‹æŸå¤±ï¼Œä¸æ˜¯é™¤ä»¥ç´¯ç§¯æ­¥æ•°çš„æŸå¤±ï¼‰
                train_loss += loss.item() * self.gradient_accumulation_steps
                
                # é‡Šæ”¾lossçš„å¼•ç”¨ï¼ˆè™½ç„¶å·²ç»è®¡ç®—äº†item()ï¼Œä½†å¯ä»¥æå‰é‡Šæ”¾ï¼‰
                del loss

            # å¤„ç†æœ€åä¸€ä¸ªepochä¸­å‰©ä½™çš„æ¢¯åº¦ç´¯ç§¯
            if accumulation_step % self.gradient_accumulation_steps != 0:
                optimizer.step()
                optimizer.zero_grad()
                train_steps += 1
                # æ¸…ç†æ˜¾å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            avg_train_loss = train_loss / train_steps

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0.0
            val_steps = 0

            with torch.no_grad():
                for batch in val_loader:
                    # batch æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰è¾“å…¥
                    input_ids = batch['input_ids']
                    attention_mask = batch['attention_mask']
                    recall_positions = batch['recall_position']
                    target_embeddings = batch['target_embedding']

                    # å‰å‘ä¼ æ’­
                    backbone_outputs = forward_backbone(
                        self.model,
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        use_cache=False,
                        output_hidden_states=False,
                        return_dict=True,
                    )
                    last_hidden_states = ensure_last_hidden_state(backbone_outputs)
                    loss = self.compute_loss(last_hidden_states, recall_positions, target_embeddings)

                    # éªŒè¯é˜¶æ®µï¼šç«‹å³æ¸…ç†backboneè¾“å‡ºå¼•ç”¨ï¼Œé‡Šæ”¾æ˜¾å­˜
                    del backbone_outputs
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                    val_loss += loss.item()
                    val_steps += 1

            avg_val_loss = val_loss / val_steps

            if self.is_main_process():
                print(f"   Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")

            # ä¿å­˜æœ€å¥½çš„æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                if self.is_main_process():
                    # ä¿å­˜å½“å‰æœ€å¥½çš„æ¨¡å‹
                    unwrapped = self.accelerator.unwrap_model(self.model)
                    unwrapped.save_pretrained(model_save_path)
                    tokenizer = self._get_tokenizer()
                    tokenizer.save_pretrained(model_save_path)
            # æ¯ä¸ªepochç»“æŸåçš„hookï¼ˆç”¨äºæ’å…¥SFTï¼‰
            try:
                if callable(self.epoch_end_hook):
                    self.epoch_end_hook(epoch, self)
            except Exception as hook_err:
                if self.is_main_process():
                    print(f"âš ï¸ epoch_end_hook æ‰§è¡Œå¤±è´¥ä½†å·²å¿½ç•¥: {hook_err}")

        # æ¸…ç†å½“å‰æŠ˜çš„èµ„æºï¼Œé¿å…KæŠ˜è¿‡ç¨‹ä¸­æ˜¾å­˜é€æ­¥æ”€å‡
        try:
            self.accelerator.wait_for_everyone()
            self.accelerator.free_memory()
        except Exception:
            pass

        # ä¸»åŠ¨é‡Šæ”¾æ•°æ®åŠ è½½å™¨ä¸ä¼˜åŒ–å™¨å¼•ç”¨
        del train_loader
        del val_loader
        del optimizer

        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        return best_val_loss

    def expose_training_handles(self):
        """æš´éœ²è®­ç»ƒå¥æŸ„ï¼Œä¾›å¤–éƒ¨SFTå¤ç”¨LoRAæ¨¡å‹"""
        return {
            "model": self.model,
            "tokenizer": self.tokenizer,
            "accelerator": getattr(self, "accelerator", None)
        }

def main():
    """ä¸»å‡½æ•° - æ”¯æŒè®¾å¤‡é€‰æ‹©"""
    
    # ğŸ”§ é…ç½®å‚æ•°
    MODEL_NAME = "./Qwen2.5-7B-Instruct-with-special-tokens"
    PT_FILE_PATH = "datasets/embeddings/text_embeddings.pt"
    
    # è®­ç»ƒå‚æ•°
    NUM_EPOCHS = 30
    BATCH_SIZE = 4
    LEARNING_RATE = 1e-4
    SAVE_PATH = "Qwen2.5-7B-Instruct-with-special-tokens-embedding-trained"
    
    # è®¾å¤‡é€‰æ‹© - æ”¯æŒå¤šç§æ¨¡å¼
    DEVICE = "cuda:5"  # å¯ä»¥æ˜¯ "auto", "cuda:2", "cpu", "cuda:0" ç­‰
    
    print("ğŸš€ è®°å¿†tokenè®­ç»ƒç¨‹åº")
    print("=" * 60)
    print(f"æ¨¡å‹: {MODEL_NAME}")
    print(f"æ•°æ®: {PT_FILE_PATH}")
    print(f"è®¾å¤‡: {DEVICE}")
    print("=" * 60)
    
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(PT_FILE_PATH):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {PT_FILE_PATH}")
        return
    
    if not os.path.exists(MODEL_NAME):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_NAME}")
        return
    
    try:
        # åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œä¼ é€’è®¾å¤‡å‚æ•°
        trainer = RecallMemoryTrainer(model_name=MODEL_NAME, device=DEVICE)
        
        # å¼€å§‹è®­ç»ƒ
        embedding_analysis = trainer.train(
            pt_file_path=PT_FILE_PATH,
            num_epochs=NUM_EPOCHS,
            batch_size=BATCH_SIZE,
            learning_rate=LEARNING_RATE,
            save_path=SAVE_PATH
        )
        
        if trainer.is_main_process():
            print("\nâœ… è®­ç»ƒæµç¨‹å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'LOCAL_RANK' in os.environ and dist.is_initialized():
            dist.destroy_process_group()

if __name__ == "__main__":
    main()