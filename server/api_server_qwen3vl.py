# -*- coding: utf-8 -*-
"""
APIæœåŠ¡å™¨ - Qwen3-VLå¤šæ¨¡æ€æ”¯æŒç‰ˆæœ¬
ä¸“é—¨ç”¨äºæµ‹è¯•å’ŒéªŒè¯æ–‡å­—+å›¾ç‰‡ä¿¡æ¯ä¼ é€’åˆ°å¤§æ¨¡å‹
"""

from flask import request, jsonify, url_for
import logging
import os
import sys
import time
import yaml
import json
import threading
import queue
from datetime import datetime
from typing import Dict, Optional, Any, List, Tuple
from dataclasses import dataclass
import base64
from uuid import uuid4
import requests
import mimetypes
from urllib.parse import unquote
import shutil
import re
from utils.cq import (
    extract_cq_image_urls as _u_extract_cq_image_urls,
    extract_cq_video_urls as _u_extract_cq_video_urls,
    extract_cq_audio_urls as _u_extract_cq_audio_urls,
    extract_cq_file_urls as _u_extract_cq_file_urls,
    extract_http_urls as _u_extract_http_urls,
    extract_cq_appshare_cards as _u_extract_cq_appshare_cards,
)
from services.media import (
    download_image_to_storage as svc_download_image_to_storage,
    download_video_to_storage as svc_download_video_to_storage,
    download_audio_to_storage as svc_download_audio_to_storage,
    download_file_to_storage as svc_download_file_to_storage,
)
from services.extractors import (
    extract_text_from_file as svc_extract_text_from_file,
    extract_text_and_images_from_file as svc_extract_text_and_images_from_file,
    download_and_extract_webpage as svc_download_and_extract_webpage,
)
from services.asr import (
    transcribe_audio as svc_transcribe_audio,
)
from services.generation import (
    InterruptStoppingCriteria as SvcInterruptStoppingCriteria,
)
from services import history as svc_history
from services import queueing as svc_queueing
from services import handler as svc_handler
from services.fetch import fetch_url_content as svc_fetch_url_content, fetch_file_content as svc_fetch_file_content

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# å¯¼å…¥Qwen3-VLç›¸å…³ç»„ä»¶
# å¼ºåˆ¶ä¼˜å…ˆä½¿ç”¨ torchvision/PyAV ä½œä¸ºè§†é¢‘è§£ç åç«¯ï¼Œé¿å…torchcodecèµ°BytesIOè·¯å¾„
os.environ.setdefault("TRANSFORMERS_VIDEO_BACKEND", "torchvision")
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor
from transformers.generation.stopping_criteria import StoppingCriteria, StoppingCriteriaList
from transformers.generation.logits_process import (
    LogitsProcessorList,
    TemperatureLogitsWarper,
    TopKLogitsWarper,
    TopPLogitsWarper,
    RepetitionPenaltyLogitsProcessor,
)
import torch

# å¯¼å…¥è®°å¿†æ¡†æ¶ç›¸å…³ç»„ä»¶
from memory.token_manager import MemoryTokenManager
from memory.vector_db import MemoryVectorDB
from memory.utils import inject_memory_embedding_to_inputs_embeds
from recall.model_utils import forward_backbone, ensure_last_hidden_state, build_causal_lm_output

_log = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# å…¨å±€æ¨¡å‹å’Œå¤„ç†å™¨
model = None
processor = None
device = None

# è®°å¿†æ¡†æ¶ç›¸å…³å…¨å±€å˜é‡
memory_db = None  # MemoryVectorDBå®ä¾‹
recall_token_ids = {}  # ç‰¹æ®Štoken IDæ˜ å°„ï¼Œå¦‚ {"<recall>": 123456, "</recall>": 123457}

# ç¾¤èŠå†å²è®°å½•ï¼ˆæ¯ä¸ªç¾¤ç»´æŠ¤30æ¡ï¼‰
group_chat_histories: Dict[str, list] = {}
private_chat_histories: Dict[str, list] = {}

# å…¨å±€é…ç½®
config = {}


def get_chat_history_token_limit() -> int:
    """
    è·å–èŠå¤©å†å²tokené•¿åº¦é™åˆ¶
    
    Returns:
        æœ€å¤§tokenæ•°é‡ï¼Œå¦‚æœé…ç½®è¯»å–å¤±è´¥åˆ™è¿”å›é»˜è®¤å€¼35000
    """
    try:
        chat_history_config = config.get("chat_history", {})
        if not isinstance(chat_history_config, dict):
            _log.warning(f"âš ï¸ chat_historyé…ç½®ä¸æ˜¯å­—å…¸ç±»å‹: {type(chat_history_config)}ï¼Œä½¿ç”¨é»˜è®¤å€¼35000")
            return 35000
        
        max_tokens = chat_history_config.get("max_input_tokens", 35000)
        result = int(max_tokens)
        
        if result <= 0:
            _log.warning(f"âš ï¸ max_input_tokensé…ç½®å€¼æ— æ•ˆ: {max_tokens}ï¼Œä½¿ç”¨é»˜è®¤å€¼35000")
            return 35000
        
        return result
    except Exception as e:
        _log.error(f"âŒ è¯»å–chat_history_token_limité…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼35000", exc_info=True)
        return 35000

# è®­ç»ƒè°ƒåº¦å™¨ï¼ˆåœ¨mainä¸­åˆå§‹åŒ–ï¼‰
training_scheduler = None

# è®­ç»ƒæ¨¡å¼æ ‡å¿—ï¼ˆç”¨äºé˜»æ­¢APIè¯·æ±‚å’Œæ¨¡å‹ç”Ÿæˆï¼‰
is_training = False
training_lock = threading.Lock()  # ç”¨äºä¿æŠ¤è®­ç»ƒæ¨¡å¼æ ‡å¿—

# çº¿ç¨‹é”ï¼Œç”¨äºä¿æŠ¤èŠå¤©è®°å½•çš„å¹¶å‘è®¿é—®
chat_history_lock = threading.Lock()

# æ¨¡å‹é”ï¼Œç”¨äºç¡®ä¿åŒä¸€æ—¶åˆ»åªæœ‰ä¸€ä¸ªçº¿ç¨‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸²è¡Œæ¨ç†ï¼‰
model_lock = threading.Lock()

# æ¶ˆæ¯å¤„ç†é˜Ÿåˆ—ï¼ˆç”¨äºä¸åŒèŠå¤©ä¹‹é—´çš„æ¶ˆæ¯æ’é˜Ÿï¼‰
message_queue = queue.Queue()

# å½“å‰æ­£åœ¨å¤„ç†çš„èŠå¤©ï¼ˆç”¨äºä¸­æ–­åŒä¸€èŠå¤©å†…çš„æ—§æ¶ˆæ¯ï¼‰
# {chat_id: {"interrupt_event": Event, "response_dict": dict, "lock": Lock}}
processing_chats: Dict[str, Dict[str, Any]] = {}

# å¤„ç†é˜Ÿåˆ—çš„çº¿ç¨‹é”
queue_lock = threading.Lock()

# å·¥ä½œçº¿ç¨‹æ˜¯å¦å·²å¯åŠ¨
worker_thread_started = False

# å¯¹å¤–è®¿é—®çš„åŸºç¡€URLï¼ˆåœ¨ä¸»å‡½æ•°ä¸­è®¾ç½®ï¼‰
server_base_url: Optional[str] = None

# å›¾ç‰‡ä¸Šä¼ ç›®å½•ï¼ˆç¡®ä¿å­˜åœ¨ï¼‰
IMAGE_UPLOAD_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "uploaded_images")
os.makedirs(IMAGE_UPLOAD_DIR, exist_ok=True)
# è§†é¢‘ä¸Šä¼ ç›®å½•ï¼ˆç¡®ä¿å­˜åœ¨ï¼‰
VIDEO_UPLOAD_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "uploaded_videos")
os.makedirs(VIDEO_UPLOAD_DIR, exist_ok=True)
# éŸ³é¢‘ä¸Šä¼ ç›®å½•ï¼ˆç¡®ä¿å­˜åœ¨ï¼‰
AUDIO_UPLOAD_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "uploaded_audios")
os.makedirs(AUDIO_UPLOAD_DIR, exist_ok=True)
# æ–‡ä»¶ä¸Šä¼ ç›®å½•ï¼ˆç¡®ä¿å­˜åœ¨ï¼‰
FILE_UPLOAD_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "uploaded_files")
os.makedirs(FILE_UPLOAD_DIR, exist_ok=True)

# è¿è¡ŒæœŸæŒ‡æ ‡
metrics_lock = threading.Lock()
metrics = {
    "requests_total": 0,
    "group_requests": 0,
    "private_requests": 0,
    "replies_sent": 0,
    "no_reply": 0,
    "interruptions": 0,
    "actions_total": 0,
    "image_cached": 0,
    "image_cache_fail": 0,
    "video_cached": 0,
    "video_cache_fail": 0,
    "audio_cached": 0,
    "audio_cache_fail": 0,
    "asr_success": 0,
    "asr_fail": 0,
    "file_cached": 0,
    "file_cache_fail": 0,
    "file_extract_success": 0,
    "file_extract_fail": 0,
    "web_extract_success": 0,
    "web_extract_fail": 0,
    "latency_ms": [],
}
MAX_LATENCY_BUCKET = 200

def _metrics_add(key: str, value: int = 1):
    with metrics_lock:
        if key in metrics and isinstance(metrics[key], int):
            metrics[key] += value

def _metrics_add_latency(ms: float):
    with metrics_lock:
        lst = metrics.get("latency_ms")
        if isinstance(lst, list):
            lst.append(float(ms))
            if len(lst) > MAX_LATENCY_BUCKET:
                del lst[: len(lst) - MAX_LATENCY_BUCKET]

# è·¯å¾„å·¥å…·ï¼šå°†ç›¸å¯¹è·¯å¾„ç»Ÿä¸€è§£æåˆ°é¡¹ç›®æ ¹ç›®å½•
def _resolve_project_path(path: Optional[str]) -> Optional[str]:
    if not path:
        return path
    if os.path.isabs(path):
        return path
    script_dir = os.path.dirname(os.path.abspath(__file__))  # server ç›®å½•
    project_root = os.path.dirname(script_dir)              # é¡¹ç›®æ ¹ç›®å½•
    normalized = path[2:] if path.startswith("./") else path
    return os.path.abspath(os.path.join(project_root, normalized))

# å¤šGPUåˆ†é…ä¼˜åŒ–å·¥å…·
def _optimize_multi_gpu_allocation(device_list: List[str], max_memory_config: Dict[int, str] = None, cuda_visible_set: bool = False) -> Dict[str, Any]:
    """
    ä¼˜åŒ–å¤šGPUåˆ†é…ç­–ç•¥ï¼Œç¡®ä¿æ¨¡å‹å’Œæ•°æ®æ›´å‡åŒ€åœ°åˆ†å¸ƒåœ¨å¤šå¼ GPUä¸Š
    
    Args:
        device_list: GPUè®¾å¤‡åˆ—è¡¨ï¼Œå¦‚ ["cuda:0", "cuda:1"] æˆ– ["cuda:6", "cuda:7"]
        max_memory_config: ç”¨æˆ·é…ç½®çš„max_memoryï¼Œæ ¼å¼å¦‚ {0: "20GB", 1: "20GB"}ï¼ˆç´¢å¼•æ˜¯å¯è§GPUçš„ç´¢å¼•ï¼Œä¸æ˜¯ç‰©ç†ç´¢å¼•ï¼‰
        cuda_visible_set: æ˜¯å¦å·²ç»è®¾ç½®äº†CUDA_VISIBLE_DEVICESï¼ˆå¦‚æœå·²è®¾ç½®ï¼Œéœ€è¦ä½¿ç”¨é‡æ–°æ˜ å°„åçš„ç´¢å¼•ï¼‰
    
    Returns:
        åŒ…å«ä¼˜åŒ–åçš„max_memoryå’Œdevice_mapçš„å­—å…¸
    """
    import torch
    
    if not torch.cuda.is_available():
        return {"device_map": "cpu", "max_memory": None}
    
    num_gpus = len(device_list)
    if num_gpus == 0:
        return {"device_map": "cpu", "max_memory": None}
    
    # æ£€æµ‹æ¯å¼ GPUçš„å¯ç”¨æ˜¾å­˜
    gpu_memories = {}
    for i, device in enumerate(device_list):
        if device.startswith("cuda:"):
            try:
                physical_gpu_idx = int(device.split(":")[1])
                
                # å¦‚æœCUDA_VISIBLE_DEVICESå·²ç»è®¾ç½®ï¼Œtorchåªèƒ½çœ‹åˆ°é‡æ–°æ˜ å°„åçš„ç´¢å¼•
                # æ­¤æ—¶éœ€è¦ä½¿ç”¨å¯è§GPUçš„ç´¢å¼•ï¼ˆ0, 1, 2...ï¼‰ï¼Œè€Œä¸æ˜¯ç‰©ç†ç´¢å¼•
                if cuda_visible_set:
                    # ä½¿ç”¨å¯è§GPUçš„ç´¢å¼•ï¼ˆiå°±æ˜¯é‡æ–°æ˜ å°„åçš„ç´¢å¼•ï¼‰
                    visible_gpu_idx = i
                    # è·å–GPUæ€»æ˜¾å­˜ï¼ˆMBï¼‰- ä½¿ç”¨å¯è§ç´¢å¼•
                    total_memory_mb = torch.cuda.get_device_properties(visible_gpu_idx).total_memory // (1024 * 1024)
                    # è·å–å½“å‰å·²ç”¨æ˜¾å­˜ï¼ˆMBï¼‰
                    torch.cuda.set_device(visible_gpu_idx)
                    allocated_mb = torch.cuda.memory_allocated(visible_gpu_idx) // (1024 * 1024)
                    reserved_mb = torch.cuda.memory_reserved(visible_gpu_idx) // (1024 * 1024)
                    available_mb = total_memory_mb - reserved_mb
                    _log.info(f"ğŸ” GPU {i} (ç‰©ç†ç´¢å¼• {physical_gpu_idx}, å¯è§ç´¢å¼• {visible_gpu_idx}): æ€»æ˜¾å­˜={total_memory_mb}MB, å¯ç”¨={available_mb}MB, å·²ä¿ç•™={reserved_mb}MB")
                else:
                    # CUDA_VISIBLE_DEVICESæœªè®¾ç½®ï¼Œä½¿ç”¨ç‰©ç†ç´¢å¼•
                    # è·å–GPUæ€»æ˜¾å­˜ï¼ˆMBï¼‰
                    total_memory_mb = torch.cuda.get_device_properties(physical_gpu_idx).total_memory // (1024 * 1024)
                    # è·å–å½“å‰å·²ç”¨æ˜¾å­˜ï¼ˆMBï¼‰
                    torch.cuda.set_device(physical_gpu_idx)
                    allocated_mb = torch.cuda.memory_allocated(physical_gpu_idx) // (1024 * 1024)
                    reserved_mb = torch.cuda.memory_reserved(physical_gpu_idx) // (1024 * 1024)
                    available_mb = total_memory_mb - reserved_mb
                    _log.info(f"ğŸ” GPU {i} (ç‰©ç†ç´¢å¼• {physical_gpu_idx}): æ€»æ˜¾å­˜={total_memory_mb}MB, å¯ç”¨={available_mb}MB, å·²ä¿ç•™={reserved_mb}MB")
                
                gpu_memories[i] = {
                    "total_mb": total_memory_mb,
                    "available_mb": available_mb,
                    "reserved_mb": reserved_mb,
                    "allocated_mb": allocated_mb
                }
            except Exception as e:
                _log.warning(f"âš ï¸ æ— æ³•æ£€æµ‹GPU {i}çš„æ˜¾å­˜: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼
                gpu_memories[i] = {"total_mb": 24000, "available_mb": 20000, "reserved_mb": 0, "allocated_mb": 0}
    
    # è®¡ç®—ä¼˜åŒ–çš„max_memoryé…ç½®
    optimized_max_memory = {}
    if max_memory_config:
        # å¦‚æœç”¨æˆ·æä¾›äº†é…ç½®ï¼Œä½¿ç”¨ç”¨æˆ·é…ç½®ï¼Œä½†ç¡®ä¿æ‰€æœ‰GPUéƒ½æœ‰é…ç½®
        for i in range(num_gpus):
            if i in max_memory_config:
                optimized_max_memory[i] = max_memory_config[i]
            else:
                # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œä½¿ç”¨å¯ç”¨æ˜¾å­˜çš„90%ï¼ˆç•™10%ç»™ç³»ç»Ÿå’Œå…¶ä»–æ“ä½œï¼‰
                if i in gpu_memories:
                    available_gb = gpu_memories[i]["available_mb"] / 1024
                    optimized_max_memory[i] = f"{int(available_gb * 0.9)}GB"
                else:
                    optimized_max_memory[i] = "20GB"  # é»˜è®¤å€¼
    else:
        # å¦‚æœæ²¡æœ‰ç”¨æˆ·é…ç½®ï¼Œè‡ªåŠ¨è®¡ç®—ï¼šä½¿ç”¨æ¯å¼ GPUå¯ç”¨æ˜¾å­˜çš„90%
        for i in range(num_gpus):
            if i in gpu_memories:
                available_gb = gpu_memories[i]["available_mb"] / 1024
                optimized_max_memory[i] = f"{int(available_gb * 0.9)}GB"
            else:
                optimized_max_memory[i] = "20GB"  # é»˜è®¤å€¼
    
    _log.info(f"âœ… ä¼˜åŒ–çš„max_memoryé…ç½®: {optimized_max_memory}")
    
    # ä½¿ç”¨ "balanced" device_mapï¼Œå°½å¯èƒ½å‡åŒ€åœ°åˆ†é…æ¨¡å‹å±‚åˆ°æ‰€æœ‰GPU
    # è¿™æ ·å¯ä»¥æœ€å¤§åŒ–åˆ©ç”¨æ‰€æœ‰GPUçš„æ˜¾å­˜ï¼Œé¿å…å•å¼ GPUè¿‡è½½
    # æ³¨æ„ï¼šå¦‚æœé‡åˆ°OOMï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨ "balanced_low_0" è®©cuda:0åˆ†é…æ›´å°‘
    # å‚è€ƒï¼šhttps://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained.device_map
    if num_gpus > 1:
        device_map_strategy = "balanced"
        _log.info(f"ğŸ”§ å¤šGPUæ¨¡å¼ï¼šä½¿ç”¨ device_map='balanced'ï¼Œå‡åŒ€åˆ†é…æ¨¡å‹å±‚åˆ°æ‰€æœ‰ {num_gpus} å¼ GPU")
    else:
        device_map_strategy = "auto"
        _log.info(f"ğŸ”§ å•GPUæ¨¡å¼ï¼šä½¿ç”¨ device_map='auto'")
    
    return {
        "device_map": device_map_strategy,
        "max_memory": optimized_max_memory
    }

# ç»‘å®šåˆ° services è–„å°è£…ï¼Œä¾›åç»­åˆ†ç¦»å®ç°æ—¶å¯æ— æ„Ÿåˆ‡æ¢
svc_history.bind_backing_stores(group_chat_histories, private_chat_histories, chat_history_lock)
svc_queueing.bind_queue(message_queue)


# é™æ€æ–‡ä»¶è·¯ç”±å·²è¿ç§»è‡³ app.pyï¼ˆè“å›¾æ¨¡å¼ï¼‰ï¼Œæœ¬æ–‡ä»¶ä¸å†æä¾›é™æ€ç«¯ç‚¹


def _ensure_processor_files(model_path: str):
    """
    ç¡®ä¿æ¨¡å‹ç›®å½•å†…åŒ…å«å¤„ç†å™¨é…ç½®ï¼›è‹¥ç¼ºå¤±åˆ™ä»åŸºç¡€æ¨¡å‹å¤åˆ¶
    """
    try:
        AutoProcessor.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True
        )
        return
    except Exception as missing_error:
        _log.warning(f"âš ï¸ æ¨¡å‹ç›®å½•ç¼ºå°‘å¤„ç†å™¨é…ç½®ï¼Œå°è¯•è¡¥å…¨: {missing_error}")

    fallback_base = (
        config.get("memory", {}).get("base_model_path")
        or config.get("model", {}).get("path")
        or "./models/Qwen3-VL-4B-Thinking"
    )
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    if not os.path.isabs(fallback_base):
        fallback_base = os.path.abspath(os.path.join(project_root, fallback_base))
    if not os.path.exists(fallback_base):
        raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œæ— æ³•è¡¥å…¨å¤„ç†å™¨é…ç½®: {fallback_base}")
    try:
        processor = AutoProcessor.from_pretrained(
            fallback_base,
            trust_remote_code=True,
            local_files_only=True
        )
        processor.save_pretrained(model_path)
        _log.info(f"âœ… å·²ä»åŸºç¡€æ¨¡å‹è¡¥å…¨å¤„ç†å™¨é…ç½®åˆ°: {model_path}")

        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶éƒ½è¢«æ­£ç¡®ä¿å­˜ï¼ˆåœ¨save_pretrainedä¹‹åï¼‰
        import shutil
        essential_files = [
            "chat_template.json",
            "preprocessor_config.json",
            "video_preprocessor_config.json"
        ]
        for file_name in essential_files:
            source_file = os.path.join(fallback_base, file_name)
            target_file = os.path.join(model_path, file_name)
            if os.path.exists(source_file) and not os.path.exists(target_file):
                try:
                    shutil.copy2(source_file, target_file)
                    _log.info(f"âœ… å·²å¤åˆ¶{file_name}åˆ°: {model_path}")
                except Exception as e:
                    _log.warning(f"âš ï¸ å¤åˆ¶{file_name}å¤±è´¥: {e}")
    except Exception as fallback_error:
        _log.error(
            f"âŒ æ— æ³•è¡¥å…¨å¤„ç†å™¨é…ç½®ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥æ¨¡å‹ç›®å½•: {model_path} "
            f"(åŸºç¡€æ¨¡å‹è·¯å¾„: {fallback_base})ï¼Œé”™è¯¯: {fallback_error}"
        )
        raise


def _is_image_url_valid(image_url: str) -> bool:
    """
    æ£€æŸ¥å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆï¼ˆä¸ä¸‹è½½å®Œæ•´å†…å®¹ï¼Œåªæ£€æŸ¥æ˜¯å¦èƒ½è®¿é—®ï¼‰
    """
    try:
        # åªè·å–å¤´éƒ¨ä¿¡æ¯ï¼Œä¸ä¸‹è½½å®Œæ•´å›¾ç‰‡
        resp = requests.head(image_url, timeout=5, allow_redirects=True)
        if resp.status_code == 200:
            content_type = resp.headers.get("Content-Type", "").lower()
            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡ç±»å‹
            if any(img_type in content_type for img_type in ["image/", "application/octet-stream"]):
                return True
        return False
    except Exception:
        return False


# æœ¬æ–‡ä»¶ä¸å†å®ç°æœ¬åœ°ä¸‹è½½/æ­£æ–‡æŠ½å–/ASRï¼Œå‡å·²è¿ç§»åˆ° services å¹¶é€šè¿‡ svc_* å§”æ‰˜

_asr_backend = None  # ä»…ä¿ç•™å ä½ç¬¦ï¼Œé¿å…å†å²å¼•ç”¨ï¼›å®é™…ASRå§”æ‰˜è‡³ services.asr


@dataclass
class MessageTask:
    """æ¶ˆæ¯å¤„ç†ä»»åŠ¡"""
    chat_type: str  # "group" æˆ– "private"
    chat_id: str  # group_id æˆ– user_id
    data: Dict[str, Any]  # åŸå§‹è¯·æ±‚æ•°æ®
    response_dict: Dict[str, Any]  # ç”¨äºè¿”å›å“åº”çš„å­—å…¸ï¼ˆçº¿ç¨‹é—´é€šä¿¡ï¼‰

# èŠå¤©è®°å½•å­˜å‚¨ç›®å½•ï¼ˆç”¨äºä¿å­˜è®­ç»ƒæ•°æ®ï¼‰
CHAT_HISTORY_STORAGE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'chat_history_storage')
os.makedirs(CHAT_HISTORY_STORAGE_DIR, exist_ok=True)


def load_config(config_path: str = None) -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    
    Returns:
        é…ç½®å­—å…¸
    """
    if config_path is None:
        # é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºå½“å‰æ–‡ä»¶ï¼‰
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, "config_qwen3vl.yaml")
    
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    if not os.path.exists(config_path):
        _log.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return get_default_config()
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        _log.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except Exception as e:
        _log.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return get_default_config()


def get_default_config() -> Dict[str, Any]:
    """è·å–é»˜è®¤é…ç½®"""
    return {
        "server": {
            "host": "0.0.0.0",
            "port": 9999
        },
        "model": {
            "path": "./models/Qwen3-VL-4B-Thinking",
            "device": "cuda:0"
        },
        "generation": {
            "max_new_tokens": 1000,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "do_sample": True
        },
        "chat_history": {
            "max_history_length": 30
        },
        "logging": {
            "level": "INFO"
        }
    }


def initialize_model(model_path: str = "./models/Qwen3-VL-4B-Thinking", device_id = "cuda:0"):
    """
    åˆå§‹åŒ–Qwen3-VLæ¨¡å‹å’Œå¤„ç†å™¨ï¼Œå¹¶è®¾ç½®è®°å¿†æ¡†æ¶
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨é€‰æ‹©æœ€æ–°è®­ç»ƒæ¨¡å‹
        device_id: è®¾å¤‡IDï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "cuda:0", "cpu"ï¼‰æˆ–åˆ—è¡¨ï¼ˆå¦‚ ["cuda:0", "cuda:1"]ï¼‰
    """
    global model, processor, device, memory_db, recall_token_ids, config

    # è·å–å¤šGPUé…ç½®
    multi_gpu_config = config.get("model", {}).get("multi_gpu", {})

    # å¦‚æœå·²æœ‰æ¨¡å‹ï¼Œå…ˆå¸è½½ä»¥é‡Šæ”¾æ˜¾å­˜
    if model is not None:
        _log.info("æ£€æµ‹åˆ°å·²æœ‰æ¨¡å‹ï¼Œå…ˆå¸è½½æ—§æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜...")
        try:
            # å°†æ¨¡å‹ç§»åˆ°CPUï¼Œç„¶ååˆ é™¤
            model = model.cpu()
        except:
            pass
        del model
        model = None
    if processor is not None:
        del processor
        processor = None
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶å’Œæ˜¾å­˜æ¸…ç†
    import gc
    import os
    gc.collect()
    torch.cuda.empty_cache()
    # å†æ¬¡åŒæ­¥å’Œæ¸…ç†
    if torch.cuda.is_available():
        torch.cuda.synchronize()
        torch.cuda.empty_cache()
    _log.info("âœ… æ—§æ¨¡å‹å·²å¸è½½ï¼Œæ˜¾å­˜å·²é‡Šæ”¾")
    
    # å¦‚æœmodel_pathä¸ºNoneï¼Œå°è¯•æŸ¥æ‰¾æœ€æ–°è®­ç»ƒæ¨¡å‹
    if model_path is None:
        _log.info("=" * 60)
        _log.info("ğŸ” initialize_model: model_pathä¸ºNoneï¼Œå¼€å§‹æŸ¥æ‰¾æœ€æ–°è®­ç»ƒæ¨¡å‹")
        _log.info("=" * 60)
        memory_config = config.get("memory", {}).get("training", {})
        trained_model_dir = memory_config.get("trained_model_dir", "./server/models/trained")
        
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
        script_dir = os.path.dirname(os.path.abspath(__file__))  # serverç›®å½•
        project_root = os.path.dirname(script_dir)  # é¡¹ç›®æ ¹ç›®å½•
        if not os.path.isabs(trained_model_dir):
            # è·¯å¾„ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼Œç›´æ¥æ‹¼æ¥
            trained_model_dir = os.path.abspath(os.path.join(project_root, trained_model_dir))
        
        _log.info(f"ğŸ“ è®­ç»ƒæ¨¡å‹ç›®å½•: {trained_model_dir}")
        
        # è·å–token_added_model_dirå’Œbase_model_path
        token_added_model_dir = memory_config.get("token_added_model_dir", "./server/models/token_added")
        base_model_path = memory_config.get("base_model_path", "./models/Qwen3-VL-4B-Thinking")
        
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not os.path.isabs(token_added_model_dir):
            token_added_model_dir = os.path.abspath(os.path.join(project_root, token_added_model_dir))
        if not os.path.isabs(base_model_path):
            base_model_path = os.path.abspath(os.path.join(project_root, base_model_path))
        
        # ä¼˜å…ˆçº§ï¼šè®­ç»ƒåçš„æ¨¡å‹ > æ·»åŠ äº†tokençš„æ¨¡å‹ > åŸºç¡€æ¨¡å‹
        model_path = None
        
        # 1. ä¼˜å…ˆæŸ¥æ‰¾è®­ç»ƒåçš„æ¨¡å‹
        if os.path.exists(trained_model_dir):
            model_dirs = [
                d for d in os.listdir(trained_model_dir)
                if os.path.isdir(os.path.join(trained_model_dir, d)) and d.startswith("model_")
            ]
            if model_dirs:
                model_dirs.sort(reverse=True)
                model_path = os.path.join(trained_model_dir, model_dirs[0])
                _log.info("=" * 60)
                _log.info(f"âœ… æ‰¾åˆ°æœ€æ–°è®­ç»ƒæ¨¡å‹: {model_path}")
                _log.info(f"ğŸ“… æ¨¡å‹æ—¶é—´æˆ³: {model_dirs[0]}")
                _log.info("=" * 60)
        
        # 2. å¦‚æœæ²¡æœ‰è®­ç»ƒæ¨¡å‹ï¼ŒæŸ¥æ‰¾æ·»åŠ äº†tokençš„æ¨¡å‹
        if model_path is None and os.path.exists(token_added_model_dir):
            model_dirs = [
                d for d in os.listdir(token_added_model_dir)
                if os.path.isdir(os.path.join(token_added_model_dir, d)) and d.startswith("model_")
            ]
            if model_dirs:
                model_dirs.sort(reverse=True)
                model_path = os.path.join(token_added_model_dir, model_dirs[0])
                _log.info("=" * 60)
                _log.info(f"âœ… æ‰¾åˆ°æ·»åŠ äº†tokençš„æ¨¡å‹: {model_path}")
                _log.info(f"ğŸ“… æ¨¡å‹æ—¶é—´æˆ³: {model_dirs[0]}")
                _log.info("=" * 60)

        # 3. å¦‚æœéƒ½æ²¡æœ‰ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹
        if model_path is None:
            model_path = base_model_path
            _log.warning(f"âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ¨¡å‹æˆ–æ·»åŠ äº†tokençš„æ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹: {model_path}")
        else:
            # å¦‚æœmodel_pathä¸ä¸ºNoneï¼Œè¯´æ˜å·²ç»åœ¨app.pyä¸­æ‰¾åˆ°äº†è®­ç»ƒæ¨¡å‹
            _log.info("=" * 60)
            _log.info("âœ… initialize_model: ä½¿ç”¨ä¼ å…¥çš„æ¨¡å‹è·¯å¾„ï¼ˆå·²åœ¨app.pyä¸­æŸ¥æ‰¾ï¼‰")
            _log.info(f"ğŸ“¦ æ¨¡å‹è·¯å¾„: {model_path}")
            _log.info("=" * 60)
    
    # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
    if not os.path.isabs(model_path):
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆserverç›®å½•çš„çˆ¶ç›®å½•ï¼‰
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        model_path = os.path.abspath(os.path.join(project_root, model_path))
    
    # åˆ¤æ–­æ˜¯è®­ç»ƒæ¨¡å‹è¿˜æ˜¯åŸºç¡€æ¨¡å‹
    is_trained_model = "trained" in model_path and "model_" in os.path.basename(model_path)
    model_type = "è®­ç»ƒæ¨¡å‹" if is_trained_model else "åŸºç¡€æ¨¡å‹"
    
    _log.info("=" * 60)
    _log.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–Qwen3-VLæ¨¡å‹...")
    _log.info(f"ğŸ“¦ æ¨¡å‹ç±»å‹: {model_type}")
    _log.info(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    _log.info(f"ğŸ–¥ï¸  è®¾å¤‡: {device_id}")
    if is_trained_model:
        model_name = os.path.basename(model_path)
        _log.info(f"ğŸ“… è®­ç»ƒæ—¶é—´æˆ³: {model_name}")
    _log.info("=" * 60)
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")

    # ç¡®ä¿å¤„ç†å™¨æ–‡ä»¶å­˜åœ¨
    _ensure_processor_files(model_path)
    
    try:
        # åŠ è½½å¤„ç†å™¨
        _log.info("åŠ è½½AutoProcessor...")
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ä¸”å­˜åœ¨ï¼Œä½¿ç”¨local_files_only=True
        processor = AutoProcessor.from_pretrained(
            model_path, 
            trust_remote_code=True,
            local_files_only=os.path.isabs(model_path) and os.path.exists(model_path)
        )
        _log.info("âœ… ProcessoråŠ è½½æˆåŠŸ")
        
        # ç¡®ä¿chat_templateè¢«æ­£ç¡®åŠ è½½ï¼ˆQwen-VLçš„ç‰¹æ®Šå¤„ç†ï¼‰
        if processor.chat_template is None:
            import json
            chat_template_path = os.path.join(model_path, "chat_template.json")
            if os.path.exists(chat_template_path):
                try:
                    with open(chat_template_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        processor.chat_template = data["chat_template"]
                except Exception as e:
                    _log.warning(f"âš ï¸ æ‰‹åŠ¨åŠ è½½chat_templateå¤±è´¥: {e}")
        
        # åŠ è½½æ¨¡å‹ - æ”¯æŒå¤šGPU
        _log.info("åŠ è½½Qwen3VLForConditionalGeneration...")
        load_kwargs = {
            "torch_dtype": "auto",
            "trust_remote_code": True,
            "local_files_only": os.path.isabs(model_path) and os.path.exists(model_path)
        }

        # æ£€æŸ¥CUDA_VISIBLE_DEVICESè®¾ç½®çŠ¶æ€ï¼ˆåœ¨æ‰€æœ‰è®¾å¤‡é…ç½®ä¹‹å‰ï¼‰
        cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
        cuda_visible_set = bool(cuda_visible)
        cuda_visible_devices = cuda_visible

        # æ ¹æ®è®¾å¤‡é…ç½®å†³å®šdevice_map
        if isinstance(device_id, list):
            # å¤šGPUé…ç½®
            # æ³¨æ„ï¼šCUDA_VISIBLE_DEVICESåº”è¯¥åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ï¼ˆåœ¨app.pyä¸­å·²è®¾ç½®ï¼‰
            # è¿™é‡Œåªéœ€è¦æ£€æŸ¥æ˜¯å¦å·²ç»è®¾ç½®ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™è®¾ç½®ï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
            
            if cuda_visible:
                _log.info(f"ğŸ”§ æ£€æµ‹åˆ°CUDA_VISIBLE_DEVICES={cuda_visible}ï¼ˆå·²åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ï¼‰")
            else:
                # å¦‚æœæœªè®¾ç½®ï¼Œåˆ™åœ¨è¿™é‡Œè®¾ç½®ï¼ˆè™½ç„¶å¯èƒ½å·²ç»å¤ªæ™šäº†ï¼‰
                gpu_indices = []
                for device in device_id:
                    if device.startswith("cuda:"):
                        try:
                            gpu_idx = int(device.split(":")[1])
                            gpu_indices.append(str(gpu_idx))
                        except (ValueError, IndexError):
                            _log.warning(f"âš ï¸ æ— æ•ˆçš„GPUè®¾å¤‡åç§°: {device}ï¼Œè·³è¿‡")
                            continue
                if gpu_indices:
                    cuda_visible_devices = ",".join(gpu_indices)
                    os.environ["CUDA_VISIBLE_DEVICES"] = cuda_visible_devices
                    _log.warning(f"âš ï¸ CUDA_VISIBLE_DEVICESæœªåœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ï¼Œç°åœ¨è®¾ç½®={cuda_visible_devices}ï¼ˆå¯èƒ½æ— æ•ˆï¼‰")
                    # æ³¨æ„ï¼šå¦‚æœåœ¨è¿™é‡Œè®¾ç½®ï¼Œtorchå¯èƒ½å·²ç»åˆå§‹åŒ–ï¼Œæ‰€ä»¥å¯èƒ½æ— æ•ˆ
                    # ä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä»ç„¶è®¾ç½®å®ƒ

            # ä½¿ç”¨ä¼˜åŒ–çš„å¤šGPUåˆ†é…ç­–ç•¥
            # æ³¨æ„ï¼šå¦‚æœCUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œéœ€è¦ä½¿ç”¨é‡æ–°æ˜ å°„åçš„ç´¢å¼•
            max_memory_config = multi_gpu_config.get("max_memory", {})
            allocation = _optimize_multi_gpu_allocation(device_id, max_memory_config, cuda_visible_set=cuda_visible_set)
            load_kwargs["device_map"] = allocation["device_map"]
            if allocation["max_memory"]:
                load_kwargs["max_memory"] = allocation["max_memory"]
            _log.info(f"ğŸ”§ å¤šGPUæ¨¡å¼: æŒ‡å®šè®¾å¤‡{device_id}ï¼Œä½¿ç”¨ä¼˜åŒ–çš„åˆ†é…ç­–ç•¥")
        elif device_id.startswith("cuda"):
            # å•GPUé…ç½®
            # å¦‚æœè®¾ç½®äº†CUDA_VISIBLE_DEVICESï¼Œéœ€è¦ä½¿ç”¨é‡æ–°æ˜ å°„åçš„ç´¢å¼•
            if cuda_visible_set and cuda_visible_devices:
                # CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„åçš„ç´¢å¼•
                device_map_device = "cuda:0"
                _log.info(f"ğŸ”§ å•GPUæ¨¡å¼: CUDA_VISIBLE_DEVICES={cuda_visible_devices}ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„è®¾å¤‡ {device_map_device}ï¼ˆå¯¹åº”ç‰©ç†GPU {device_id}ï¼‰")
            else:
                # æœªè®¾ç½®CUDA_VISIBLE_DEVICESï¼Œç›´æ¥ä½¿ç”¨ç‰©ç†è®¾å¤‡
                device_map_device = device_id
            _log.info(f"ğŸ”§ å•GPUæ¨¡å¼: è®¾å¤‡æ˜ å°„åˆ° {device_id}")
            load_kwargs["device_map"] = {"": device_map_device}
        else:
            # CPUé…ç½®
            load_kwargs["device_map"] = "cpu"
            _log.info("ğŸ”§ CPUæ¨¡å¼: åŠ è½½åˆ°CPU")

        model = Qwen3VLForConditionalGeneration.from_pretrained(
            model_path,
            **load_kwargs
        )
        
        # è·å–å®é™…è®¾å¤‡
        device = next(model.parameters()).device
        _log.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå®é™…è®¾å¤‡: {device}")
        
        # æ£€æŸ¥å¹¶æ·»åŠ ç‰¹æ®Štoken
        _log.info("æ£€æŸ¥å¹¶æ·»åŠ è®°å¿†ç›¸å…³ç‰¹æ®Štoken...")
        token_manager = MemoryTokenManager(model, processor.tokenizer)
        recall_token_ids = token_manager.check_and_add_tokens(perturbation_std=0.02)
        _log.info(f"âœ… ç‰¹æ®Štokenå¤„ç†å®Œæˆ: {recall_token_ids}")
        
        # åˆå§‹åŒ–MemoryVectorDB
        memory_config = config.get("memory", {})
        memory_enabled = memory_config.get("enabled", False)
        if memory_enabled:
            _log.info("åˆå§‹åŒ–MemoryVectorDB...")
            # è·å–embeddingç»´åº¦ï¼ˆä»æ¨¡å‹é…ç½®ä¸­ï¼‰
            embedding_dim = model.config.hidden_size if hasattr(model.config, 'hidden_size') else 4096
            memory_db = MemoryVectorDB(embedding_dim=embedding_dim, device=device)
            
            # åŠ è½½è®°å¿†æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            memory_db_path = memory_config.get("memory_db_path")
            if memory_db_path:
                resolved_path = _resolve_project_path(memory_db_path)
                if resolved_path and os.path.exists(resolved_path):
                    memory_db.load_from_pt(resolved_path)
                else:
                    _log.warning(f"è®°å¿†æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {resolved_path or memory_db_path}")
            else:
                _log.info("æœªé…ç½®è®°å¿†æ•°æ®åº“è·¯å¾„ï¼Œä½¿ç”¨ç©ºæ•°æ®åº“")
        else:
            _log.info("è®°å¿†åŠŸèƒ½æœªå¯ç”¨")
        
        _log.info("=" * 60)
        
    except Exception as e:
        _log.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}", exc_info=True)
        raise


def format_multimodal_message(content: str, image_urls: List[str], video_urls: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    """
    æ ¼å¼åŒ–å¤šæ¨¡æ€æ¶ˆæ¯ä¸ºQwen3-VLæ ¼å¼ï¼ˆä½¿ç”¨URLæ ¼å¼ï¼‰
    
    Args:
        content: æ–‡æœ¬å†…å®¹
        image_urls: å›¾ç‰‡URLåˆ—è¡¨ï¼Œæ ¼å¼ï¼š["https://multimedia.nt.qq.com.cn/download?...", ...]
    
    Returns:
        Qwen3-VLæ ¼å¼çš„æ¶ˆæ¯å†…å®¹åˆ—è¡¨
    """
    message_content = []
    
    # æ·»åŠ æ–‡æœ¬éƒ¨åˆ†
    if content:
        message_content.append({"type": "text", "text": content})
    
    # æ·»åŠ å›¾ç‰‡éƒ¨åˆ†ï¼ˆä½¿ç”¨URLæ ¼å¼ï¼Œå‚è€ƒæ ·ä¾‹ï¼‰
    for img_url in image_urls:
        message_content.append({"type": "image", "image": img_url})
    # æ·»åŠ è§†é¢‘éƒ¨åˆ†
    if video_urls:
        for v_url in video_urls:
            message_content.append({"type": "video", "video": v_url})
    
    return message_content


def _parse_action_commands(output_text: str) -> List[Dict[str, Any]]:
    """
    ä»æ¨¡å‹è¾“å‡ºä¸­è§£æåŠ¨ä½œæŒ‡ä»¤ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼çš„ä»»æ„ä¸€ç§ï¼š
    1) <action>...</action> å†…çš„ JSONï¼ˆå¯¹è±¡æˆ–æ•°ç»„ï¼‰
    2) ```json ... ``` ä»£ç å—ä¸­å¸¦æœ‰typeå­—æ®µçš„å¯¹è±¡æˆ–æ•°ç»„
    3) è¡Œå†… ACTION: { ... } æˆ– ACTIONS: [ ... ]
    è¿”å›æ ‡å‡†åŒ–åçš„åˆ—è¡¨ï¼š[{"type": "...", ...}, ...]
    """
    import re
    import json
    candidates: List[str] = []
    # 1) <action>...</action>
    for m in re.finditer(r'<action>([\s\S]+?)</action>', output_text, flags=re.IGNORECASE):
        candidates.append(m.group(1))
    # 2) ```json ... ```
    for m in re.finditer(r'```json\s*([\s\S]+?)\s*```', output_text, flags=re.IGNORECASE):
        candidates.append(m.group(1))
    # 3) ACTION(S): { ... } / [ ... ]
    for m in re.finditer(r'ACTI(?:ON|ONS)\s*:\s*([\s\S]+)$', output_text, flags=re.IGNORECASE | re.MULTILINE):
        candidates.append(m.group(1))
    # 4) æ ‡å‡†MCPé£æ ¼ï¼š<tool_call name="FETCH_URL">{...}</tool_call> æˆ– <toolcall name="...">...</toolcall>
    mcp_calls: List[Dict[str, Any]] = []
    for m in re.finditer(r'<tool_?call\s+name\s*=\s*"([^"]+)"\s*>\s*([\s\S]+?)\s*</tool_?call\s*>', output_text, flags=re.IGNORECASE):
        tool_name = m.group(1).strip().upper()
        payload = m.group(2).strip()
        try:
            obj = json.loads(payload)
            if isinstance(obj, dict):
                obj["type"] = tool_name  # å½’ä¸€åˆ°ç°æœ‰åŠ¨ä½œtype
                mcp_calls.append(obj)
            elif isinstance(obj, list):
                for it in obj:
                    if isinstance(it, dict):
                        it["type"] = tool_name
                        mcp_calls.append(it)
        except Exception:
            # å°è¯•ä»ç‰‡æ®µä¸­æå–JSON
            try:
                start = min((i for i in [payload.find("{"), payload.find("[")] if i != -1), default=-1)
                end = max(payload.rfind("}"), payload.rfind("]"))
                if start != -1 and end != -1 and end > start:
                    parsed = json.loads(payload[start:end+1])
                    if isinstance(parsed, dict):
                        parsed["type"] = tool_name
                        mcp_calls.append(parsed)
                    elif isinstance(parsed, list):
                        for it in parsed:
                            if isinstance(it, dict):
                                it["type"] = tool_name
                                mcp_calls.append(it)
            except Exception:
                continue
    actions: List[Dict[str, Any]] = []
    def normalize_one(obj: Any):
        if isinstance(obj, dict):
            item = obj
            t = str(item.get("type", "")).upper().strip()
            # åªæ”¯æŒ EMOJI_LIKE å’Œ POKEï¼ˆå·²ç§»é™¤ IMAGE å’Œ FORWARDï¼‰
            if t in {"EMOJI_LIKE", "POKE"}:
                actions.append(item)
        elif isinstance(obj, list):
            for it in obj:
                normalize_one(it)
    for snippet in candidates:
        try:
            parsed = json.loads(snippet)
            normalize_one(parsed)
        except Exception:
            # å°è¯•æå–æœ€å¤–å±‚JSONå¯¹è±¡/æ•°ç»„
            try:
                start = min((i for i in [snippet.find("{"), snippet.find("[")] if i != -1), default=-1)
                end = max(snippet.rfind("}"), snippet.rfind("]"))
                if start != -1 and end != -1 and end > start:
                    parsed = json.loads(snippet[start:end+1])
                    normalize_one(parsed)
            except Exception:
                continue
    # 5) è£¸JSONå®¹é”™ï¼šæ‰«æå¯èƒ½çš„JSONå¯¹è±¡/æ•°ç»„ï¼Œè§£æå«æœ‰typeå­—æ®µçš„åŠ¨ä½œ
    json_like_matches = re.findall(r'(\{[\s\S]*?\}|\[[\s\S]*?\])', output_text, flags=re.IGNORECASE)
    for jtxt in json_like_matches:
        try:
            parsed = json.loads(jtxt)
            normalize_one(parsed)
        except Exception:
            continue
    # åˆå¹¶MCPè§£æå‡ºçš„è°ƒç”¨
    for call in mcp_calls:
        normalize_one(call)
    return actions

def extract_final_reply(output_text: str) -> Tuple[str, bool, List[Dict[str, Any]]]:
    """
    ä»thinkingæ¨¡å‹çš„è¾“å‡ºä¸­æå–æ­£å¼å›å¤ï¼ˆ</think>æ ‡ç­¾åçš„å†…å®¹ï¼‰
    
    Args:
        output_text: æ¨¡å‹çš„å®Œæ•´è¾“å‡º
    
    Returns:
        (å›å¤å†…å®¹, æ˜¯å¦éœ€è¦å›å¤, åŠ¨ä½œæŒ‡ä»¤åˆ—è¡¨)
        - å¦‚æœåŒ…å«<no_reply>æ ‡ç­¾ï¼ˆä»…åœ¨thinkç»“æŸåï¼‰ï¼Œè¿”å›("", False)
        - å¦‚æœåŒ…å«æ­£å¸¸å›å¤ï¼Œè¿”å›(å›å¤å†…å®¹, True)
        - å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œè¿”å›(å®Œæ•´è¾“å‡º, True)
    """
    import re
    
    # å®šä¹‰no_replyæ ‡ç­¾æ¨¡å¼
    no_reply_patterns = [
        r'<no_reply>',
        r'<no_reply/>',
        r'<no_reply\s*/>',
    ]
    
    # å°è¯•åŒ¹é… </think> æ ‡ç­¾ï¼ˆthinkingæ¨¡å‹ä½¿ç”¨çš„æ ‡ç­¾ï¼‰
    thinking_patterns = [
        r'</think>\s*',
        r'</thinking>\s*'
    ]
    
    # æŸ¥æ‰¾æ‰€æœ‰thinkingç»“æŸæ ‡ç­¾ï¼Œé€‰æ‹©æœ€åä¸€ä¸ªï¼ˆä»æœ€åä¸€ä¸ªæ ‡ç­¾å¼€å§‹æå–æ­£å¼å›å¤ï¼‰
    last_match = None
    last_pattern = None
    
    for pattern in thinking_patterns:
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹
        matches = list(re.finditer(pattern, output_text, re.IGNORECASE))
        if matches:
            # é€‰æ‹©æœ€åä¸€ä¸ªåŒ¹é…é¡¹
            current_match = matches[-1]
            # å¦‚æœè¿™ä¸ªåŒ¹é…é¡¹æ¯”ä¹‹å‰çš„æ›´é åï¼Œåˆ™æ›´æ–°
            if last_match is None or current_match.end() > last_match.end():
                last_match = current_match
                last_pattern = pattern
    
    if last_match:
        # æå–æœ€åä¸€ä¸ªæ ‡ç­¾åçš„å†…å®¹ï¼ˆè¿™æ˜¯æ­£å¼å›å¤éƒ¨åˆ†ï¼‰
        final_reply = output_text[last_match.end():].strip()
        
        # åªåœ¨thinkç»“æŸåçš„æ­£å¼å›å¤éƒ¨åˆ†æ£€æŸ¥<no_reply>æ ‡ç­¾
        # è¿™æ ·å¯ä»¥é¿å…è¯¯è¯†åˆ«æ€è€ƒè¿‡ç¨‹ä¸­æåˆ°çš„no_replyæ ‡ç­¾
        for no_reply_pattern in no_reply_patterns:
            if re.search(no_reply_pattern, final_reply, re.IGNORECASE):
                _log.info("âœ… æ¨¡å‹åˆ¤æ–­ä¸éœ€è¦å›å¤ï¼ˆåœ¨thinkç»“æŸåçš„æ­£å¼å›ç­”ä¸­åŒ…å«<no_reply>æ ‡ç­¾ï¼‰")
                return "", False, _parse_action_commands(output_text)
        
        # ç§»é™¤ä»»ä½•é—ç•™çš„å·¥å…·è°ƒç”¨ç‰‡æ®µï¼Œé˜²æ­¢æ³„æ¼åˆ°æœ€ç»ˆå¯è§è¾“å‡º
        # å…¼å®¹æ ‡å‡†MCPæ ¼å¼ï¼š<tool_call name="...">{...}</tool_call>
        final_reply = re.sub(r'<tool_call\\b[^>]*>.*?</tool_call>', '', final_reply, flags=re.IGNORECASE | re.DOTALL).strip()
        
        # æ¸…ç†åŠ¨ä½œæ³„æ¼ï¼šå»é™¤ <action>â€¦</action>ã€```json``` ä¸­çš„JSONï¼Œä»¥åŠè£¸JSONåŠ¨ä½œç‰‡æ®µ
        final_reply = re.sub(r'<action>[\s\S]*?</action>', '', final_reply, flags=re.IGNORECASE)
        final_reply = re.sub(r'```json[\s\S]*?```', '', final_reply, flags=re.IGNORECASE)
        # ç²—æ¸…ç†ï¼šç§»é™¤åŒ…å« "type" å…³é”®å­—çš„é¡¶å±‚å¯¹è±¡/æ•°ç»„ï¼ˆé˜²æ­¢æŠŠåŠ¨ä½œJSONå›ç»™ç”¨æˆ·ï¼‰
        final_reply = re.sub(r'\{[^{}]*"type"[^{}]*\}', '', final_reply, flags=re.IGNORECASE)
        final_reply = re.sub(r'\[[^\[\]]*"type"[^\[\]]*\]', '', final_reply, flags=re.IGNORECASE)
        # å¦‚æœæ²¡æœ‰no_replyæ ‡ç­¾ï¼Œè¿”å›æ­£å¼å›å¤
        _log.info(f"âœ… æå–åˆ°æ­£å¼å›å¤ï¼ˆä»æœ€åä¸€ä¸ª{last_match.group(0).strip()}æ ‡ç­¾å¼€å§‹ï¼‰")
        return final_reply.strip(), True, _parse_action_commands(output_text)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°thinkingæ ‡ç­¾ï¼Œä½œä¸ºfallbackæ£€æŸ¥æ•´ä¸ªè¾“å‡º
    # è¿™ç§æƒ…å†µåº”è¯¥å¾ˆå°‘è§ï¼Œå› ä¸ºæ¨¡å‹åº”è¯¥è¾“å‡ºthinkingæ ‡ç­¾
    _log.warning("âš ï¸ æœªæ‰¾åˆ°thinkingæ ‡ç­¾ï¼Œæ£€æŸ¥æ•´ä¸ªè¾“å‡ºä¸­çš„<no_reply>æ ‡ç­¾")
    for pattern in no_reply_patterns:
        if re.search(pattern, output_text, re.IGNORECASE):
            _log.info("âœ… æ¨¡å‹åˆ¤æ–­ä¸éœ€è¦å›å¤ï¼ˆæ•´ä¸ªè¾“å‡ºä¸­åŒ…å«<no_reply>æ ‡ç­¾ï¼Œä½†æœªæ‰¾åˆ°thinkingæ ‡ç­¾ï¼‰")
            return "", False, _parse_action_commands(output_text)
    
    # å¦‚æœæ—¢æ²¡æœ‰thinkingæ ‡ç­¾ä¹Ÿæ²¡æœ‰no_replyæ ‡ç­¾ï¼Œè¿”å›å®Œæ•´è¾“å‡ºï¼ˆåŒæ—¶ç§»é™¤å·¥å…·/åŠ¨ä½œè°ƒç”¨ç‰‡æ®µï¼‰
    cleaned = re.sub(r'<tool_call\\b[^>]*>.*?</tool_call>', '', output_text, flags=re.IGNORECASE | re.DOTALL).strip()
    cleaned = re.sub(r'<action>[\s\S]*?</action>', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'```json[\s\S]*?```', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'\{[^{}]*"type"[^{}]*\}', '', cleaned, flags=re.IGNORECASE)
    cleaned = re.sub(r'\[[^\[\]]*"type"[^\[\]]*\]', '', cleaned, flags=re.IGNORECASE)
    return cleaned, True, _parse_action_commands(output_text)


def extract_cq_image_urls(content: str) -> Tuple[str, List[str]]:
    """å§”æ‰˜åˆ° utils.cq.extract_cq_image_urls"""
    return _u_extract_cq_image_urls(content)

def extract_cq_video_urls(content: str) -> Tuple[str, List[str]]:
    """å§”æ‰˜åˆ° utils.cq.extract_cq_video_urls"""
    return _u_extract_cq_video_urls(content)

def extract_cq_audio_urls(content: str) -> Tuple[str, List[str]]:
    """å§”æ‰˜åˆ° utils.cq.extract_cq_audio_urls"""
    return _u_extract_cq_audio_urls(content)

def extract_cq_file_urls(content: str) -> Tuple[str, List[str]]:
    """å§”æ‰˜åˆ° utils.cq.extract_cq_file_urls"""
    return _u_extract_cq_file_urls(content)

def _extract_http_urls(text: str, max_urls: int = 5) -> List[str]:
    """å§”æ‰˜åˆ° utils.cq.extract_http_urls"""
    return _u_extract_http_urls(text, max_urls)

def extract_cq_appshare_cards(content: str):
    """å§”æ‰˜åˆ° utils.cq.extract_cq_appshare_cards"""
    return _u_extract_cq_appshare_cards(content)
# ç½‘é¡µæŠ“å–é€»è¾‘å·²è¿ç§»åˆ° services.extractors.download_and_extract_webpage


def build_system_prompt(chat_type: str = None, chat_context: Dict[str, str] = None) -> str:
    """
    æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆä»configæ–‡ä»¶ä¸­è¯»å–å¹¶ç»„åˆï¼‰
    
    Args:
        chat_type: "group" æˆ– "private"ï¼Œè¡¨ç¤ºå¯¹è¯ç±»å‹
        chat_context: å¯¹è¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
            - å¯¹äºç¾¤èŠï¼š{"group_name": "ç¾¤åç§°", "user_nickname": "ç”¨æˆ·æ˜µç§°"}
            - å¯¹äºç§èŠï¼š{"user_nickname": "ç”¨æˆ·æ˜µç§°"}
    
    Returns:
        å®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
    """
    global config, recall_token_ids
    
    prompt_config = config.get("prompt", {})
    
    # è·å–æç¤ºè¯ç»„åˆé¡ºåºï¼ˆå¦‚æœæœªé…ç½®ï¼Œä½¿ç”¨é»˜è®¤é¡ºåºï¼‰
    prompt_order = prompt_config.get("prompt_order", [
        "context",
        "recall_instruction",
        "output_structure",
        "role_playing"
    ])
    
    # æ„å»ºå„éƒ¨åˆ†æç¤ºè¯
    prompt_parts = {}
    
    # 1. å¯¹è¯ä¸Šä¸‹æ–‡æç¤º
    context_template = prompt_config.get("context_template", {})
    if chat_type and chat_context:
        if chat_type == "group":
            template = context_template.get("group", "å½“å‰ï¼Œä½ æ­£åœ¨ç¾¤èŠã€Œ{group_name}ã€(ç¾¤å·ï¼š{group_id})ä¸­è¿›è¡Œå¯¹è¯ã€‚")
            group_id = chat_context.get("group_id", "")
            group_name = chat_context.get("group_name", "ç¾¤èŠ")
            user_id = chat_context.get("user_id", "")
            user_nickname = chat_context.get("user_nickname", "ç”¨æˆ·")
            try:
                prompt_parts["context"] = template.format(
                    group_id=group_id,
                    group_name=group_name,
                    user_id=user_id,
                    user_nickname=user_nickname
                )
            except KeyError:
                # å¦‚æœæ¨¡æ¿ä¸­æ²¡æœ‰æŸäº›å˜é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼
                prompt_parts["context"] = template.format(
                    group_id=group_id or "æœªçŸ¥",
                    group_name=group_name,
                    user_id=user_id or "æœªçŸ¥",
                    user_nickname=user_nickname
                )
        elif chat_type == "private":
            template = context_template.get("private", "å½“å‰ï¼Œä½ æ­£åœ¨ä¸ç”¨æˆ·ã€Œ{user_nickname}ã€(QQå·ï¼š{user_id})è¿›è¡Œç§èŠå¯¹è¯ã€‚")
            user_id = chat_context.get("user_id", "")
            user_nickname = chat_context.get("user_nickname", "ç”¨æˆ·")
            try:
                prompt_parts["context"] = template.format(
                    user_id=user_id,
                    user_nickname=user_nickname
                )
            except KeyError:
                # å¦‚æœæ¨¡æ¿ä¸­æ²¡æœ‰æŸäº›å˜é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼
                prompt_parts["context"] = template.format(
                    user_id=user_id or "æœªçŸ¥",
                    user_nickname=user_nickname
                )
    else:
        prompt_parts["context"] = ""
    
    # 2. å›å¿†æœºåˆ¶è¯´æ˜ï¼ˆå¦‚æœå¯ç”¨ä¸”tokenå­˜åœ¨ï¼‰
    memory_config = config.get("memory", {})
    memory_enabled = memory_config.get("enabled", False)
    if memory_enabled and recall_token_ids:
        prompt_parts["recall_instruction"] = prompt_config.get("recall_instruction", "").strip()
    else:
        prompt_parts["recall_instruction"] = ""
    
    # 3. è¾“å‡ºç»“æ„æç¤ºè¯
    prompt_parts["output_structure"] = prompt_config.get("output_structure", "").strip()
    
    # 4. è§’è‰²æ‰®æ¼”æç¤ºè¯
    prompt_parts["role_playing"] = prompt_config.get("role_playing", "").strip()
    
    # 5. å·¥å…·ä½¿ç”¨æç¤ºï¼ˆä»é…ç½®è¯»å–ï¼Œå¯é€‰ï¼‰
    tool_guidance = prompt_config.get("tool_guidance", "").strip()
    if tool_guidance:
        prompt_parts["tool_guidance"] = tool_guidance
    
    # 6. å¤šæ ·åŒ–å›å¤åŠ¨ä½œæç¤ºï¼ˆä»é…ç½®è¯»å–ï¼Œå¯é€‰ï¼‰
    reply_actions = prompt_config.get("reply_actions", "").strip()
    if reply_actions:
        prompt_parts["reply_actions"] = reply_actions
    
    # æŒ‰ç…§é…ç½®çš„é¡ºåºç»„åˆæç¤ºè¯
    system_prompt_parts = []
    part_labels = {
        "context": "ã€å¯¹è¯ä¸Šä¸‹æ–‡ã€‘",
        "recall_instruction": "ã€å›å¿†æœºåˆ¶è¯´æ˜ã€‘",
        "output_structure": "ã€è¾“å‡ºç»“æ„è¦æ±‚ã€‘",
        "role_playing": "ã€è§’è‰²è®¾å®šã€‘",
        "tool_guidance": "ã€å·¥å…·ä½¿ç”¨è¯´æ˜ã€‘",
        "reply_actions": "ã€å¤šæ ·åŒ–äº’åŠ¨ã€‘"
    }
    
    for part_name in prompt_order:
        if part_name in prompt_parts and prompt_parts[part_name]:
            part_content = prompt_parts[part_name].strip()
            if part_content:
                # æ·»åŠ åˆ†éš”æ ‡ç­¾
                label = part_labels.get(part_name, f"ã€{part_name}ã€‘")
                # å¦‚æœå†…å®¹å·²ç»ä»¥ç›¸åŒæ ‡ç­¾å¼€å¤´ï¼Œåˆ™ä¸å†é‡å¤æ·»åŠ æ ‡ç­¾
                if part_content.startswith(label):
                    system_prompt_parts.append(part_content)
                else:
                    system_prompt_parts.append(f"{label}\n{part_content}")
    
    # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†ï¼Œä½¿ç”¨æ›´æ¸…æ™°çš„åˆ†éš”ç¬¦
    # æ¯ä¸ªéƒ¨åˆ†ä¹‹é—´ç”¨åˆ†éš”çº¿åˆ†éš”
    separator = "\n\n" + "="*60 + "\n\n"
    system_prompt = separator.join(system_prompt_parts)
    
    return system_prompt


def save_chat_history_to_storage(chat_type: str, chat_id: str, messages: List[Dict[str, Any]]):
    """
    ä¿å­˜èŠå¤©è®°å½•åˆ°å­˜å‚¨æ–‡ä»¶ï¼ˆä¾›è®­ç»ƒç”¨ï¼‰
    ä½¿ç”¨å›ºå®šæ–‡ä»¶åï¼Œå¢é‡è¿½åŠ æ¨¡å¼
    
    Args:
        chat_type: "group" æˆ– "private"
        chat_id: ç¾¤IDæˆ–ç”¨æˆ·ID
        messages: è¦ä¿å­˜çš„æ¶ˆæ¯åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨å›ºå®šæ–‡ä»¶åï¼ˆä¸å¸¦æ—¶é—´æˆ³ï¼‰ï¼Œä¾¿äºå¢é‡è¿½åŠ 
        filename = f"{chat_type}_{chat_id}.json"
        filepath = os.path.join(CHAT_HISTORY_STORAGE_DIR, filename)
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½ç°æœ‰æ¶ˆæ¯
        existing_messages = []
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    if isinstance(existing_data, dict) and "messages" in existing_data:
                        existing_messages = existing_data.get("messages", [])
                    elif isinstance(existing_data, list):
                        existing_messages = existing_data
                _log.info(f"ğŸ“‚ åŠ è½½ç°æœ‰æ–‡ä»¶ {filename}ï¼Œå·²æœ‰ {len(existing_messages)} æ¡æ¶ˆæ¯")
            except Exception as e:
                _log.warning(f"åŠ è½½ç°æœ‰æ–‡ä»¶ {filename} å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
        
        # åˆå¹¶æ¶ˆæ¯ï¼ˆå»é‡ï¼šæ¯”è¾ƒæ¶ˆæ¯å†…å®¹å’Œæ—¶é—´æˆ³ï¼‰
        # ä½¿ç”¨æ¶ˆæ¯çš„æ–‡æœ¬å†…å®¹å’Œæ—¶é—´æˆ³ä½œä¸ºå”¯ä¸€æ ‡è¯†
        existing_message_keys = set()
        for msg in existing_messages:
            # ç”Ÿæˆæ¶ˆæ¯çš„å”¯ä¸€æ ‡è¯†
            msg_key = _generate_message_key(msg)
            existing_message_keys.add(msg_key)
        
        # åªæ·»åŠ ä¸åœ¨ç°æœ‰æ¶ˆæ¯ä¸­çš„æ–°æ¶ˆæ¯
        new_messages = []
        for msg in messages:
            msg_key = _generate_message_key(msg)
            if msg_key not in existing_message_keys:
                new_messages.append(msg)
                existing_message_keys.add(msg_key)
        
        if not new_messages:
            _log.info(f"â„¹ï¸ {filename} æ²¡æœ‰æ–°æ¶ˆæ¯éœ€è¦è¿½åŠ ")
            return
        
        # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯
        all_messages = existing_messages + new_messages
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„å­—å…¸æ ¼å¼ï¼‰
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump({
                "chat_type": chat_type,
                "chat_id": chat_id,
                "last_updated": datetime.now().strftime('%Y%m%d_%H%M%S'),
                "messages": all_messages
            }, f, ensure_ascii=False, indent=2)
        
        _log.info(f"âœ… å·²ä¿å­˜ {len(new_messages)} æ¡æ–°æ¶ˆæ¯åˆ° {filename}ï¼ˆæ€»è®¡ {len(all_messages)} æ¡ï¼‰")
    except Exception as e:
        _log.error(f"ä¿å­˜èŠå¤©è®°å½•å¤±è´¥: {e}", exc_info=True)


def _generate_message_key(message: Dict[str, Any]) -> str:
    """
    ç”Ÿæˆæ¶ˆæ¯çš„å”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºå»é‡ï¼‰
    
    Args:
        message: æ¶ˆæ¯å­—å…¸
        
    Returns:
        å”¯ä¸€æ ‡è¯†å­—ç¬¦ä¸²
    """
    # æå–æ¶ˆæ¯å†…å®¹
    content = message.get("content", "")
    if isinstance(content, list):
        # å¤šæ¨¡æ€å†…å®¹ï¼Œæå–æ–‡æœ¬éƒ¨åˆ†
        text_parts = [item.get("text", "") for item in content if item.get("type") == "text"]
        content = " ".join(text_parts)
    
    # ä½¿ç”¨è§’è‰²å’Œå†…å®¹çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºå”¯ä¸€æ ‡è¯†
    role = message.get("role", "unknown")
    content_preview = str(content)[:100] if content else ""
    
    return f"{role}:{content_preview}"


def maintain_chat_history(chat_type: str, chat_id: str, history: List[Dict[str, Any]]):
    """
    ç»´æŠ¤èŠå¤©è®°å½•é•¿åº¦ï¼Œè¶…å‡ºéƒ¨åˆ†ä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        chat_type: "group" æˆ– "private"
        chat_id: ç¾¤IDæˆ–ç”¨æˆ·ID
        history: èŠå¤©å†å²è®°å½•
    """
    global config, recall_token_ids
    
    max_history = config.get("chat_history", {}).get("max_history_length", 30)
    
    if len(history) > max_history:
        # è®¡ç®—éœ€è¦ç§»é™¤çš„æ¶ˆæ¯æ•°é‡
        removed_count = len(history) - max_history
        removed_messages = history[:removed_count]
        
        # ä¿å­˜è¢«ç§»é™¤çš„æ¶ˆæ¯
        if removed_messages:
            save_chat_history_to_storage(chat_type, chat_id, removed_messages)
        
        # åªä¿ç•™æœ€æ–°çš„max_historyæ¡
        history[:] = history[-max_history:]


def process_message_task(task: MessageTask):
    """
    å¤„ç†å•ä¸ªæ¶ˆæ¯ä»»åŠ¡ï¼ˆåœ¨é˜Ÿåˆ—å·¥ä½œçº¿ç¨‹ä¸­æ‰§è¡Œï¼‰
    
    Args:
        task: æ¶ˆæ¯å¤„ç†ä»»åŠ¡
    """
    global processing_chats, is_training, training_lock
    
    # æ£€æŸ¥æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
    with training_lock:
        if is_training:
            _log.warning("âš ï¸ å½“å‰å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œæ‹’ç»å¤„ç†æ¶ˆæ¯è¯·æ±‚")
            if task.response_dict:
                task.response_dict["reply"] = ""
                task.response_dict["should_reply"] = False
                task.response_dict["error"] = "æœåŠ¡å™¨æ­£åœ¨è®­ç»ƒä¸­ï¼Œæš‚æ—¶æ— æ³•å¤„ç†æ¶ˆæ¯"
            return
    
    chat_id = task.chat_id
    chat_type = task.chat_type
    data = task.data
    response_dict = task.response_dict
    
    try:
        import time as _t
        _req_t0 = _t.time()
        _metrics_add("requests_total", 1)
        if chat_type == "group":
            _metrics_add("group_requests", 1)
        elif chat_type == "private":
            _metrics_add("private_requests", 1)
        # æ£€æŸ¥æ˜¯å¦åŒä¸€èŠå¤©æœ‰æ–°æ¶ˆæ¯ï¼ˆä¸­æ–­æ—§æ¶ˆæ¯å¤„ç†ï¼‰
        old_task_interrupted = False
        with queue_lock:
            if chat_id in processing_chats:
                # ä¸­æ–­æ—§æ¶ˆæ¯å¤„ç†
                old_processing = processing_chats[chat_id]
                old_interrupt = old_processing["interrupt_event"]
                old_interrupt.set()
                old_task_interrupted = True
                _log.info(f"âš ï¸ ä¸­æ–­èŠå¤© {chat_id} çš„æ—§æ¶ˆæ¯å¤„ç†ï¼ˆæ—§ä»»åŠ¡æ­£åœ¨å¤„ç†ä¸­ï¼‰")
            
            # åˆ›å»ºæ–°çš„ä¸­æ–­äº‹ä»¶ï¼ˆæ–°ä»»åŠ¡ä½¿ç”¨ï¼‰
            interrupt_event = threading.Event()
            processing_chats[chat_id] = {
                "interrupt_event": interrupt_event,
                "response_dict": response_dict,
                "lock": threading.Lock()
            }
        
        # å¦‚æœä¸­æ–­äº†æ—§ä»»åŠ¡ï¼Œç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©æ—§ä»»åŠ¡æ£€æµ‹åˆ°ä¸­æ–­å¹¶é€€å‡º
        # åŒæ—¶ï¼Œåœ¨å°†æ¶ˆæ¯åŠ å…¥å†å²ä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„ä»»åŠ¡
        if old_task_interrupted:
            time.sleep(0.3)  # ç»™æ—§ä»»åŠ¡ä¸€äº›æ—¶é—´æ£€æµ‹ä¸­æ–­å¹¶é€€å‡º
            
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„ä»»åŠ¡ï¼ˆå¯èƒ½åœ¨è¿™æœŸé—´åˆæœ‰æ–°æ¶ˆæ¯åˆ°è¾¾ï¼‰
            with queue_lock:
                current_processing = processing_chats.get(chat_id)
                if current_processing and current_processing["response_dict"] is not response_dict:
                    # å·²ç»æœ‰æ›´æ–°çš„ä»»åŠ¡äº†ï¼Œå½“å‰ä»»åŠ¡åº”è¯¥é€€å‡º
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç­‰å¾…æœŸé—´å·²è¢«æ›´æ–°çš„æ¶ˆæ¯æ›¿æ¢ï¼Œé€€å‡ºå¤„ç†")
                    return
                
                # å¦‚æœå½“å‰ä»»åŠ¡ä»ç„¶æ˜¯æœ€æ–°çš„ï¼Œç¡®ä¿interrupt_eventæ²¡æœ‰è¢«é”™è¯¯è®¾ç½®
                # å› ä¸ºæˆ‘ä»¬æ˜¯æ–°ä»»åŠ¡ï¼Œinterrupt_eventåº”è¯¥æ˜¯æœªè®¾ç½®çš„
                if interrupt_event.is_set():
                    _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„æ–°ä»»åŠ¡interrupt_eventè¢«é”™è¯¯è®¾ç½®ï¼Œé‡ç½®")
                    interrupt_event.clear()
        
        # æå–æ•°æ®
        if chat_type == "group":
            group_id = str(data.get("group_id", ""))
            group_name = data.get("group_name", f"ç¾¤{group_id}")
            user_id = str(data.get("user_id", ""))
            user_nickname = data.get("user_nickname", f"ç”¨æˆ·{user_id}")
            user_card = data.get("user_card", user_nickname)
            content = data.get("content", "")
            timestamp = data.get("timestamp", time.time())
            
            # è°ƒè¯•ï¼šæŸ¥çœ‹å®Œæ•´çš„æ¶ˆæ¯æ•°æ®ç»“æ„
            _log.debug(f"ğŸ“‹ ç¾¤èŠæ¶ˆæ¯å®Œæ•´æ•°æ®: {data}")
            
            # ä»contentä¸­æå–CQå›¾ç‰‡/è§†é¢‘/è¯­éŸ³URLï¼ˆç”¨äºå¤šæ¨¡æ€å¤„ç†ï¼‰
            _log.info(f"ğŸ” æ¶ˆæ¯å†…å®¹åˆ†æ: {content}")
            cleaned_content, image_urls = extract_cq_image_urls(content)
            _log.info(f"ğŸ“· å›¾ç‰‡CQç æå–: æ‰¾åˆ° {len(image_urls)} ä¸ª - {image_urls}")
            cleaned_content, video_urls = extract_cq_video_urls(cleaned_content)
            _log.info(f"ğŸ¥ è§†é¢‘CQç æå–: æ‰¾åˆ° {len(video_urls)} ä¸ª - {video_urls}")
            cleaned_content, audio_urls = extract_cq_audio_urls(cleaned_content)
            _log.info(f"ğŸµ è¯­éŸ³CQç æå–: æ‰¾åˆ° {len(audio_urls)} ä¸ª")
            # æå–æ–‡ä»¶URLï¼ˆä»…ç”¨äºæ—¥å¿—è®°å½•ï¼Œä¸ä¿®æ”¹contentï¼Œä¿ç•™åŸå§‹CQç ï¼‰
            _, file_urls = extract_cq_file_urls(content)
            # æ³¨æ„ï¼šä¸ä¿®æ”¹contentï¼Œä¿ç•™æ–‡ä»¶ã€é“¾æ¥ã€å¡ç‰‡çš„åŸå§‹CQç 
            content = cleaned_content
            
            # æœ¬åœ°åŒ–å›¾ç‰‡URL
            if image_urls:
                _log.info(f"âœ… ä»CQç ä¸­æå–åˆ° {len(image_urls)} ä¸ªå›¾ç‰‡URL")
                cached_urls = []
                for original_url in image_urls:
                    cached = svc_download_image_to_storage(original_url, IMAGE_UPLOAD_DIR, server_base_url, _metrics_add, _log)
                    cached_urls.append(cached or original_url)
                image_urls = cached_urls
            # åˆå¹¶å®¢æˆ·ç«¯ç›´é“¾è§†é¢‘ï¼ˆè‹¥æä¾›ï¼‰
            req_video_urls = data.get("video_urls") or []
            _log.debug(f"ğŸ¥ å®¢æˆ·ç«¯æä¾›çš„video_urlså­—æ®µ: {req_video_urls}")
            if req_video_urls:
                video_urls = list(set((video_urls or []) + req_video_urls))
                _log.info(f"âœ… åˆå¹¶å®¢æˆ·ç«¯ç›´é“¾è§†é¢‘: {len(req_video_urls)} ä¸ª")
            # é¢„è¿‡æ»¤æ— æ•ˆè§†é¢‘æºï¼ˆä¿ç•™HTTP/HTTPS URLå’Œæœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼‰
            def _is_valid_video_source_prefilter(p: str) -> bool:
                try:
                    if not p:
                        return False
                    # HTTP/HTTPS URL
                    if p.startswith(("http://", "https://")):
                        return True
                    # æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆWindowsæˆ–Linuxï¼‰
                    import os as _os
                    return _os.path.isfile(p)
                except Exception:
                    return False
            if video_urls:
                video_urls = [v for v in video_urls if _is_valid_video_source_prefilter(v)]
                _log.debug(f"é¢„è¿‡æ»¤åçš„è§†é¢‘URLs: {video_urls}")
            # æœ¬åœ°åŒ–è§†é¢‘URLï¼ˆæ”¯æŒä¸­æ–­æ£€æŸ¥ï¼Œä½†è§†é¢‘ä¸‹è½½ä¼šç»§ç»­ï¼‰
            if video_urls:
                _log.info(f"âœ… ä»CQç ä¸­æå–åˆ° {len(video_urls)} ä¸ªè§†é¢‘URL")
                cached_videos = []
                for i, v in enumerate(video_urls):
                    # æ£€æŸ¥æ˜¯å¦è¢«æ–°æ¶ˆæ¯ä¸­æ–­
                    if interrupt_event and interrupt_event.is_set():
                        if chat_id and response_dict:
                            with queue_lock:
                                current_processing = processing_chats.get(chat_id)
                                if current_processing and current_processing["response_dict"] is not response_dict:
                                    _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„è§†é¢‘ä¸‹è½½è¿‡ç¨‹ä¸­è¢«æ–°æ¶ˆæ¯ä¸­æ–­ï¼Œé€€å‡ºå¤„ç†ï¼ˆè§†é¢‘ä¸‹è½½ä¼šç»§ç»­ï¼‰")
                                    return
                    
                    _log.info(f"ğŸ“¥ æ­£åœ¨å¤„ç†è§†é¢‘ {i+1}/{len(video_urls)}: {v[:80]}...")
                    # æ£€æµ‹Windowsè·¯å¾„ï¼Œç›´æ¥è·³è¿‡ï¼ˆå®¢æˆ·ç«¯åº”è¯¥åœ¨ä¸Šä¼ å‰å¤„ç†ï¼‰
                    if re.match(r'^[a-zA-Z]:\\', v) or re.match(r'^\\\\', v):
                        _log.error(f"âŒ è·³è¿‡Windowsæœ¬åœ°è·¯å¾„ï¼ˆæœåŠ¡å™¨æ— æ³•è®¿é—®ï¼‰: {v}")
                        _log.error(f"ğŸ’¡ å®¢æˆ·ç«¯åº”è¯¥åœ¨å‘é€æ¶ˆæ¯å‰å°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ åˆ°æœåŠ¡å™¨")
                        continue  # è·³è¿‡è¿™ä¸ªè§†é¢‘
                    cached = svc_download_video_to_storage(v, VIDEO_UPLOAD_DIR, server_base_url, _metrics_add, _log)
                    if cached:
                        cached_videos.append(cached)
                    else:
                        _log.warning(f"âš ï¸ è§†é¢‘ä¸‹è½½/ç¼“å­˜å¤±è´¥: {v}")
                        # å¦‚æœæ˜¯HTTP URLï¼Œä»ç„¶ä¿ç•™ï¼ˆå¯èƒ½å¯ä»¥è®¿é—®ï¼‰
                        if v.startswith(('http://', 'https://')):
                            cached_videos.append(v)
                        else:
                            _log.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè§†é¢‘URL: {v}")
                    
                    # å†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆä¸‹è½½å®Œæˆåï¼‰
                    if interrupt_event and interrupt_event.is_set():
                        if chat_id and response_dict:
                            with queue_lock:
                                current_processing = processing_chats.get(chat_id)
                                if current_processing and current_processing["response_dict"] is not response_dict:
                                    _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„è§†é¢‘ä¸‹è½½å®Œæˆåè¢«æ–°æ¶ˆæ¯ä¸­æ–­ï¼Œé€€å‡ºå¤„ç†")
                                    return
                # ä»…ä¿ç•™http(s)ç›´é“¾æˆ–æœ¬æœºå¯è®¿é—®çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œè¿‡æ»¤æ‰æ— æ•ˆçš„ç³»ç»Ÿè·¯å¾„ï¼ˆå¦‚Windowsç›˜ç¬¦ï¼‰
                def _is_valid_video_source(p: str) -> bool:
                    try:
                        if not p:
                            return False
                        if p.lower().startswith(("http://", "https://")):
                            return True
                        import os as _os
                        return _os.path.exists(p)
                    except Exception:
                        return False
                # å°†æœåŠ¡å™¨é™æ€URLè½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆtransformersåº“éœ€è¦æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œä¸æ”¯æŒHTTP URLï¼‰
                valid_video_paths = []
                base_static = (server_base_url or "http://127.0.0.1:9999").rstrip("/") + "/static/videos/"
                for v in cached_videos:
                    if v and v.startswith(base_static):
                        # è¿™æ˜¯æœåŠ¡å™¨URLï¼Œè½¬æ¢ä¸ºå¯¹åº”çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„
                        filename = v.split("/")[-1]
                        local_path = os.path.join(VIDEO_UPLOAD_DIR, filename)
                        if os.path.exists(local_path):
                            # æœ¬åœ°æ–‡ä»¶å­˜åœ¨ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶è·¯å¾„
                            valid_video_paths.append(local_path)
                            _log.debug(f"ğŸ¥ ä½¿ç”¨æœ¬åœ°è§†é¢‘æ–‡ä»¶è·¯å¾„: {local_path}")
                        else:
                            # æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿ç•™æœåŠ¡å™¨URLï¼ˆè™½ç„¶å¯èƒ½æ— æ³•è®¿é—®ï¼‰
                            valid_video_paths.append(v)
                            _log.warning(f"âš ï¸ è§†é¢‘æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨URL: {local_path}")
                    elif v and (v.startswith('http://') or v.startswith('https://')):
                        # å¤–éƒ¨URLï¼Œä¿ç•™ï¼ˆè™½ç„¶å¯èƒ½æ— æ³•è®¿é—®ï¼Œä½†è‡³å°‘æ ¼å¼æ­£ç¡®ï¼‰
                        valid_video_paths.append(v)
                        _log.debug(f"ğŸ¥ ä¿ç•™å¤–éƒ¨è§†é¢‘URL: {v}")
                    else:
                        # æ— æ•ˆURLï¼Œè·³è¿‡
                        _log.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè§†é¢‘URL: {v}")
                video_urls = valid_video_paths
            
            # æœ¬åœ°åŒ–è¯­éŸ³URLå¹¶è¿›è¡ŒASRè½¬å†™
            asr_texts: List[str] = []
            if 'audio_urls' in locals() and audio_urls:
                _log.info(f"âœ… ä»CQç ä¸­æå–åˆ° {len(audio_urls)} ä¸ªè¯­éŸ³URL")
                cached_audios = []
                for a in audio_urls:
                    cached = svc_download_audio_to_storage(a, AUDIO_UPLOAD_DIR, server_base_url, _metrics_add, _log)
                    cached_audios.append(cached or a)
                # å¯¹æœ¬åœ°åŒ–åçš„å¯è®¿é—®æ–‡ä»¶æ‰§è¡ŒASRï¼ˆä»…å¯¹æœ¬åœ°ç¼“å­˜çš„æ–‡ä»¶æ‰§è¡Œï¼‰
                for ca in cached_audios:
                    if ca and ca.startswith((server_base_url or "http://127.0.0.1:9999").rstrip('/') + "/static/audios/"):
                        # å°†URLè½¬æˆæœ¬åœ°æ–‡ä»¶è·¯å¾„
                        filename = ca.rsplit('/', 1)[-1]
                        local_fp = os.path.join(AUDIO_UPLOAD_DIR, filename)
                        text = svc_transcribe_audio(local_fp, _metrics_add, _log)
                        if text:
                            asr_texts.append(text)
                # å°†è½¬å†™æ–‡æœ¬æ³¨å…¥åˆ°content
                if asr_texts:
                    content = (content + "\n" if content else "") + "ã€è¯­éŸ³è½¬å†™ã€‘" + " ".join(asr_texts)
            
            # å¤„ç†æ–‡ä»¶ï¼šä¸‹è½½å¹¶æå–æ–‡æœ¬å’Œå›¾ç‰‡å†…å®¹ï¼ˆæ”¯æŒä¸­æ–­ï¼‰
            file_texts: List[str] = []
            file_image_paths: List[str] = []
            if 'file_urls' in locals() and file_urls:
                _log.info(f"âœ… æ£€æµ‹åˆ° {len(file_urls)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹æå–å†…å®¹")
                for file_url in file_urls:
                    # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
                    if interrupt_event and interrupt_event.is_set():
                        _log.info(f"âš ï¸ æ–‡ä»¶å¤„ç†è¢«ä¸­æ–­ï¼Œåœæ­¢å¤„ç†å‰©ä½™æ–‡ä»¶")
                        break
                    
                    try:
                        # ä¸‹è½½æ–‡ä»¶åˆ°æœåŠ¡å™¨
                        cached_file_url = svc_download_file_to_storage(file_url, FILE_UPLOAD_DIR, server_base_url, _metrics_add, _log)
                        
                        # å†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆä¸‹è½½å¯èƒ½è€—æ—¶ï¼‰
                        if interrupt_event and interrupt_event.is_set():
                            _log.info(f"âš ï¸ æ–‡ä»¶ä¸‹è½½åè¢«ä¸­æ–­ï¼Œåœæ­¢å¤„ç†")
                            break
                        
                        if cached_file_url and cached_file_url.startswith((server_base_url or "http://127.0.0.1:9999").rstrip('/') + "/static/files/"):
                            # å°†URLè½¬æˆæœ¬åœ°æ–‡ä»¶è·¯å¾„
                            filename = cached_file_url.rsplit('/', 1)[-1]
                            local_fp = os.path.join(FILE_UPLOAD_DIR, filename)
                            if os.path.exists(local_fp):
                                # æå–æ–‡æœ¬å’Œå›¾ç‰‡
                                file_text, file_images = svc_extract_text_and_images_from_file(
                                    local_fp, IMAGE_UPLOAD_DIR, _metrics_add, _log
                                )
                                
                                # å†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆæå–å¯èƒ½è€—æ—¶ï¼‰
                                if interrupt_event and interrupt_event.is_set():
                                    _log.info(f"âš ï¸ æ–‡ä»¶æå–åè¢«ä¸­æ–­ï¼Œåœæ­¢å¤„ç†")
                                    break
                                
                                if file_text:
                                    file_texts.append(file_text)
                                # å°†æå–çš„å›¾ç‰‡è·¯å¾„è½¬æ¢ä¸ºURLå¹¶æ·»åŠ åˆ°image_urls
                                for img_path in file_images:
                                    if interrupt_event and interrupt_event.is_set():
                                        break
                                    if os.path.exists(img_path):
                                        img_filename = os.path.basename(img_path)
                                        img_url = f"{server_base_url.rstrip('/')}/static/images/{img_filename}"
                                        if image_urls is None:
                                            image_urls = []
                                        if img_url not in image_urls:
                                            image_urls.append(img_url)
                                            file_image_paths.append(img_path)
                                if file_text or file_images:
                                    _log.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: æ–‡æœ¬é•¿åº¦={len(file_text)}, å›¾ç‰‡æ•°={len(file_images)}")
                    except Exception as file_err:
                        _log.warning(f"âš ï¸ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_url}: {file_err}")
                
                # å°†æå–çš„æ–‡æœ¬å†…å®¹æ·»åŠ åˆ°contentï¼ˆå¦‚æœæœªè¢«ä¸­æ–­ï¼‰
                if not (interrupt_event and interrupt_event.is_set()) and file_texts:
                    file_content = "\n\n".join([f"ã€æ–‡ä»¶å†…å®¹{i+1}ã€‘\n{t}" for i, t in enumerate(file_texts)])
                    content = (content + "\n\n" if content else "") + file_content

            media_info = ""
            if image_urls:
                media_info += f" [åŒ…å«{len(image_urls)}å¼ å›¾ç‰‡]"
            if video_urls:
                media_info += f" [åŒ…å«{len(video_urls)}ä¸ªè§†é¢‘]"
            if 'audio_urls' in locals() and audio_urls:
                media_info += f" [åŒ…å«{len(audio_urls)}æ®µè¯­éŸ³]"
            if 'file_urls' in locals() and file_urls:
                media_info += f" [åŒ…å«{len(file_urls)}ä¸ªæ–‡ä»¶]"
            _log.info(f"æ”¶åˆ°ç¾¤æ¶ˆæ¯ [ç¾¤:{group_id}({group_name})] [ç”¨æˆ·:{user_id}({user_card})]: {content[:50] if content else '(ä»…å¤šåª’ä½“)'}{media_info}...")
            
            # æ ¼å¼åŒ–æ—¶é—´æˆ³
            time_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
            
            # æ ¼å¼åŒ–ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«QQå·ä¿¡æ¯ï¼Œæ–¹ä¾¿æ¨¡å‹åœ¨å†å²æ¶ˆæ¯ä¸­è¯†åˆ«ç”¨æˆ·ï¼‰
            if user_card and user_card != user_nickname:
                formatted_message = f"[{time_str}] {user_card}({user_nickname}, QQ:{user_id}): {content}" if content else f"[{time_str}] {user_card}({user_nickname}, QQ:{user_id})"
            else:
                formatted_message = f"[{time_str}] {user_nickname}(QQ:{user_id}): {content}" if content else f"[{time_str}] {user_nickname}(QQ:{user_id})"
            
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            if image_urls or video_urls:
                message_content = format_multimodal_message(formatted_message, image_urls, video_urls)
            else:
                message_content = [{"type": "text", "text": formatted_message}]
            
            # ä¸å†æ„å»º/æ³¨å…¥ç›®å½•ï¼Œå ä½é€»è¾‘ç§»é™¤ï¼Œç›´æ¥ä¿ç•™åŸå§‹CQæ–‡æœ¬
            
            # æ›´æ–°èŠå¤©è®°å½•
            with chat_history_lock:
                if group_id not in group_chat_histories:
                    group_chat_histories[group_id] = []
                
                group_chat_histories[group_id].append({
                    "role": "user",
                    "content": message_content
                })
                
                maintain_chat_history("group", group_id, group_chat_histories[group_id])
            
            # åœ¨ç”Ÿæˆå›å¤ä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„ä»»åŠ¡ï¼ˆé˜²æ­¢åœ¨åŠ å…¥å†å²æœŸé—´æœ‰æ–°æ¶ˆæ¯åˆ°è¾¾ï¼‰
            # åŒæ—¶æ£€æŸ¥interrupt_eventæ˜¯å¦è¢«é”™è¯¯è®¾ç½®
            with queue_lock:
                current_processing = processing_chats.get(chat_id)
                if current_processing and current_processing["response_dict"] is not response_dict:
                    # å·²ç»æœ‰æ›´æ–°çš„ä»»åŠ¡äº†ï¼Œå½“å‰ä»»åŠ¡åº”è¯¥é€€å‡ºï¼ˆä½†æ¶ˆæ¯å·²ç»åŠ å…¥å†å²ï¼Œè¿™æ˜¯æ­£ç¡®çš„ï¼‰
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå›å¤å‰å·²è¢«æ›´æ–°çš„æ¶ˆæ¯æ›¿æ¢ï¼Œé€€å‡ºå¤„ç†")
                    return
                
                # å¦‚æœå½“å‰ä»»åŠ¡ä»ç„¶æ˜¯æœ€æ–°çš„ï¼Œä½†interrupt_eventè¢«è®¾ç½®äº†ï¼Œæ¸…é™¤å®ƒ
                # è¿™å¯èƒ½æ˜¯è¯¯è®¾ç½®ï¼ˆæ¯”å¦‚åœ¨ç­‰å¾…æœŸé—´è¢«ä¸­æ–­ï¼Œä½†ä¹‹ååˆæˆä¸ºæœ€æ–°ä»»åŠ¡ï¼‰
                if interrupt_event.is_set():
                    _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå›å¤å‰æ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†ä»»åŠ¡ä»æ˜¯æœ€æ–°çš„ï¼Œæ¸…é™¤ä¸­æ–­ä¿¡å·")
                    interrupt_event.clear()
            
            # ç”Ÿæˆå›å¤ï¼ˆæ”¯æŒæœ€å¤š1è½®å·¥å…·è°ƒç”¨å†ç”Ÿæˆï¼‰
            _log.info(f"ğŸ§  å¼€å§‹ç”Ÿæˆå›å¤ï¼ˆç¾¤ {group_id}ï¼‰...")
            display_name = user_card if user_card and user_card != user_nickname else user_nickname
            chat_context = {
                "group_id": group_id,
                "group_name": group_name,
                "user_id": user_id,
                "user_nickname": display_name
            }
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = build_system_prompt("group", chat_context)
            
            # åœ¨ç”Ÿæˆå‰ï¼Œå…ˆå¯¹åŸå§‹å†å²è¿›è¡Œtokené•¿åº¦æ£€æŸ¥å’Œæˆªæ–­
            with chat_history_lock:
                # ç¡®ä¿group_idå­˜åœ¨äºgroup_chat_historiesä¸­
                if group_id not in group_chat_histories:
                    _log.warning(f"âš ï¸ ç¾¤ {group_id} çš„èŠå¤©å†å²ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨")
                    group_chat_histories[group_id] = []
                
                # å¯¹åŸå§‹å†å²è¿›è¡Œæˆªæ–­ï¼ˆä¼šä¿®æ”¹åŸå§‹å†å²å¹¶ä¿å­˜è¢«åˆ é™¤çš„æ¶ˆæ¯ï¼‰
                # ç¡®ä¿group_idå­˜åœ¨äºgroup_chat_historiesä¸­
                if group_id not in group_chat_histories:
                    _log.warning(f"âš ï¸ ç¾¤ {group_id} çš„èŠå¤©å†å²ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨")
                    group_chat_histories[group_id] = []

                truncated_history = group_chat_histories[group_id].copy()  # é»˜è®¤å€¼ï¼Œä½¿ç”¨å‰¯æœ¬
                _log.debug(f"ğŸ“Š åŸå§‹èŠå¤©å†å²é•¿åº¦: {len(group_chat_histories[group_id])}ï¼ˆç¾¤ {group_id}ï¼‰")

                try:
                    max_tokens_limit = get_chat_history_token_limit()
                    _log.debug(f"ğŸ“Š è·å–åˆ°çš„max_tokensé™åˆ¶: {max_tokens_limit}ï¼ˆç¾¤ {group_id}ï¼‰")

                    max_tokens_limit = get_chat_history_token_limit()
                    _log.info(f"ğŸ“Š è·å–åˆ°çš„max_tokensé™åˆ¶: {max_tokens_limit}, ç±»å‹: {type(max_tokens_limit)}ï¼ˆç¾¤ {group_id}ï¼‰")
                    
                    # éªŒè¯max_tokens_limit
                    if max_tokens_limit is None:
                        _log.error(f"âŒ max_tokens_limitä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼35000ï¼ˆç¾¤ {group_id}ï¼‰")
                        max_tokens_limit = 35000
                    elif not isinstance(max_tokens_limit, int) or max_tokens_limit <= 0:
                        _log.error(f"âŒ max_tokens_limitæ— æ•ˆ: {max_tokens_limit}ï¼Œä½¿ç”¨é»˜è®¤å€¼35000ï¼ˆç¾¤ {group_id}ï¼‰")
                        max_tokens_limit = 35000

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æˆªæ–­
                    if len(group_chat_histories[group_id]) == 0:
                        _log.info("ğŸ“Š èŠå¤©å†å²ä¸ºç©ºï¼Œæ— éœ€æˆªæ–­")
                        truncated_history = []
                    else:
                        _log.info(f"ğŸ“Š å¼€å§‹è°ƒç”¨truncate_history_by_tokensï¼ˆç¾¤ {group_id}ï¼‰ï¼Œmax_tokens={max_tokens_limit}")
                        result = truncate_history_by_tokens(
                            group_chat_histories[group_id],
                            system_prompt,
                            "group",
                            group_id,
                            max_tokens=max_tokens_limit,
                            interrupt_event=interrupt_event
                        )
                        _log.info(f"ğŸ“Š truncate_history_by_tokensè¿”å›: ç±»å‹={type(result)}, æ˜¯å¦ä¸ºNone={result is None}, é•¿åº¦={len(result) if result is not None else 'N/A'}ï¼ˆç¾¤ {group_id}ï¼‰")
                        truncated_history = result

                    # ç¡®ä¿è¿”å›å€¼ä¸ä¸ºNoneä¸”æ˜¯åˆ—è¡¨ç±»å‹
                    if truncated_history is None:
                        _log.error(f"âŒ æˆªæ–­å†å²è¿”å›Noneï¼ˆç¾¤ {group_id}ï¼‰ï¼Œå›é€€åˆ°åŸå§‹å†å²")
                        truncated_history = group_chat_histories[group_id].copy()
                    elif not isinstance(truncated_history, list):
                        _log.error(f"âŒ æˆªæ–­å†å²è¿”å›éåˆ—è¡¨ç±»å‹: {type(truncated_history)}ï¼ˆç¾¤ {group_id}ï¼‰ï¼Œå›é€€åˆ°åŸå§‹å†å²")
                        truncated_history = group_chat_histories[group_id].copy()
                    else:
                        _log.info(f"âœ… æˆªæ–­å†å²æˆåŠŸï¼Œé•¿åº¦: {len(truncated_history)}ï¼ˆç¾¤ {group_id}ï¼‰")
                except Exception as e:
                    _log.error(f"âŒ æˆªæ–­å†å²æ—¶å‘ç”Ÿå¼‚å¸¸ï¼ˆç¾¤ {group_id}ï¼‰: {e}", exc_info=True)
                    # å¼‚å¸¸æƒ…å†µä¸‹ï¼Œä½¿ç”¨åŸå§‹å†å²
                    truncated_history = group_chat_histories[group_id].copy()
                
                # ä½¿ç”¨æˆªæ–­åçš„å†å²ï¼ˆå¤åˆ¶ä¸€ä»½ç”¨äºç”Ÿæˆï¼Œé¿å…åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¢«ä¿®æ”¹ï¼‰
                current_history = truncated_history.copy()
            
            # å…³é—­FETCHç›¸å…³å¾ªç¯ï¼ˆæŒ‰å½“å‰éœ€æ±‚æ”¾å¼ƒé“¾æ¥/å¡ç‰‡/æ–‡ä»¶è®¿é—®ï¼‰
            tool_iterations = 0
            action_cmds = []
            for _iter in range(tool_iterations + 1):
                _gen_ret = generate_reply(
                current_history, 
                chat_type="group", 
                chat_context=chat_context,
                interrupt_event=interrupt_event,
                chat_id=chat_id,
                    response_dict=response_dict,
                    log_full_io=True
            )
                if isinstance(_gen_ret, tuple) and len(_gen_ret) == 3:
                    reply, should_reply, was_interrupted = _gen_ret
                    action_cmds = []
                else:
                    reply, should_reply, was_interrupted, action_cmds = _gen_ret
                if was_interrupted:
                    break
                # å·²ç¦ç”¨FETCHåŠ¨ä½œ
                pending_fetch = []
                # for act in (action_cmds or []):
                #     t = str(act.get("type", "")).upper().strip()
                #     if t in ("FETCH_URL", "FETCH_FILE"):
                #         pending_fetch.append(act)
                if not pending_fetch or _iter >= tool_iterations:
                    break
                # FETCHåŠŸèƒ½å·²ç¦ç”¨ï¼Œæ— éœ€å¤„ç†å·¥å…·è°ƒç”¨
            
            # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
            if was_interrupted:
                _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„æ¶ˆæ¯å¤„ç†è¢«ä¸­æ–­ï¼Œè·³è¿‡å›å¤")
                _metrics_add("interruptions", 1)
                # è¢«ä¸­æ–­çš„æ¶ˆæ¯éœ€è¦æ›´æ–°response_dictï¼Œå¦åˆ™å®¢æˆ·ç«¯ä¼šä¸€ç›´ç­‰å¾…
                # ä½†éœ€è¦ç¡®ä¿è¿™æ˜¯å½“å‰ä»»åŠ¡ï¼Œé¿å…æ—§ä»»åŠ¡è¦†ç›–æ–°ä»»åŠ¡çš„response_dict
                with queue_lock:
                    current_processing = processing_chats.get(chat_id)
                    if current_processing and current_processing["response_dict"] is response_dict:
                        response_dict.update({
                            "status": "success",
                            "should_reply": False,
                            "reply": ""
                        })
                        _log.info(f"âœ… å·²æ›´æ–°ä¸­æ–­å“åº”ï¼ˆèŠå¤© {chat_id}ï¼‰")
                return
            
            # åœ¨æ›´æ–°èŠå¤©è®°å½•ä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„ä»»åŠ¡
            # å› ä¸ºå¯èƒ½åœ¨ç”ŸæˆæœŸé—´æœ‰æ–°æ¶ˆæ¯åˆ°è¾¾å¹¶è®¾ç½®äº†interrupt_event
            with queue_lock:
                current_processing = processing_chats.get(chat_id)
                if current_processing and current_processing["response_dict"] is not response_dict:
                    # å·²ç»æœ‰æ›´æ–°çš„ä»»åŠ¡äº†ï¼Œå½“å‰ä»»åŠ¡åº”è¯¥é€€å‡º
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå®Œæˆåè¢«æ›´æ–°çš„æ¶ˆæ¯æ›¿æ¢ï¼Œè·³è¿‡æ›´æ–°å†å²")
                    return
                # å†æ¬¡æ£€æŸ¥ä¸­æ–­äº‹ä»¶ï¼ˆåŒé‡ä¿é™©ï¼‰
                if interrupt_event.is_set():
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå®Œæˆåè¢«ä¸­æ–­ï¼Œè·³è¿‡æ›´æ–°å†å²")
                    return
            
            # åœ¨æ›´æ–°èŠå¤©è®°å½•ä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆé˜²æ­¢åœ¨ç”Ÿæˆå®Œæˆåã€æ›´æ–°å‰æœ‰æ–°æ¶ˆæ¯åˆ°è¾¾ï¼‰
            with queue_lock:
                current_processing = processing_chats.get(chat_id)
                if current_processing and current_processing["response_dict"] is not response_dict:
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ›´æ–°èŠå¤©è®°å½•å‰å·²è¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œè·³è¿‡æ›´æ–°")
                    return
                if interrupt_event.is_set():
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ›´æ–°èŠå¤©è®°å½•å‰è¢«ä¸­æ–­ï¼Œè·³è¿‡æ›´æ–°")
                    return
            
            # æ›´æ–°èŠå¤©è®°å½•ï¼ˆåªæœ‰åœ¨æ²¡æœ‰è¢«ä¸­æ–­çš„æƒ…å†µä¸‹ï¼‰
            with chat_history_lock:
                # åœ¨æŒæœ‰chat_history_lockæœŸé—´å†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆåŒé‡ä¿é™©ï¼‰
                if interrupt_event and interrupt_event.is_set():
                    with queue_lock:
                        current_processing = processing_chats.get(chat_id)
                        if current_processing and current_processing["response_dict"] is not response_dict:
                            _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ›´æ–°èŠå¤©è®°å½•æœŸé—´è¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œè·³è¿‡æ›´æ–°")
                            return
                
                if should_reply:
                    _metrics_add("replies_sent", 1)
                    group_chat_histories[group_id].append({
                        "role": "assistant",
                        "content": [{"type": "text", "text": reply}]
                    })
                    maintain_chat_history("group", group_id, group_chat_histories[group_id])
                    _log.info(f"ğŸ’¬ ç”Ÿæˆå›å¤ï¼ˆç¾¤ {group_id}ï¼‰ï¼š{reply[:100]}...")
                else:
                    _metrics_add("no_reply", 1)
                    group_chat_histories[group_id].append({
                        "role": "assistant",
                        "content": [{"type": "text", "text": "<no_reply>"}]
                    })
                    maintain_chat_history("group", group_id, group_chat_histories[group_id])
                    _log.info(f"ğŸ’¬ æ¨¡å‹åˆ¤æ–­ä¸éœ€è¦å›å¤ï¼ˆç¾¤ {group_id}ï¼‰")
            
            # æ›´æ–°å“åº”ï¼ˆåªæœ‰åœ¨æ²¡æœ‰è¢«ä¸­æ–­çš„æƒ…å†µä¸‹ï¼‰
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„ä»»åŠ¡ï¼ˆé˜²æ­¢åœ¨æ›´æ–°èŠå¤©è®°å½•æ—¶è¢«æ–°æ¶ˆæ¯ä¸­æ–­ï¼‰
            with queue_lock:
                current_processing = processing_chats.get(chat_id)
                if current_processing and current_processing["response_dict"] is response_dict:
                    # å†æ¬¡æ£€æŸ¥ä¸­æ–­äº‹ä»¶ï¼ˆé˜²æ­¢åœ¨æ›´æ–°èŠå¤©è®°å½•æ—¶è¢«ä¸­æ–­ï¼‰
                    if interrupt_event.is_set():
                        _log.warning(f"âš ï¸ ä»»åŠ¡åœ¨æ›´æ–°å“åº”å‰è¢«ä¸­æ–­ï¼ˆç¾¤ {group_id}ï¼‰")
                        return
                    
                    response_dict.update({
                        "status": "success",
                        "should_reply": should_reply,
                        "reply": reply if should_reply else "",
                        "actions": action_cmds if should_reply else []
                    })
                    _log.info(f"âœ… å·²æ›´æ–°å“åº”ï¼ˆç¾¤ {group_id}ï¼‰ï¼Œshould_reply={should_reply}, replyé•¿åº¦={len(reply) if reply else 0}")
                else:
                    _log.warning(f"âš ï¸ ä»»åŠ¡å·²è¢«æ–°æ¶ˆæ¯ä¸­æ–­ï¼Œè·³è¿‡å“åº”æ›´æ–°ï¼ˆç¾¤ {group_id}ï¼‰")
            
        elif chat_type == "private":
            user_id = str(data.get("user_id", ""))
            user_nickname = data.get("user_nickname", f"ç”¨æˆ·{user_id}")
            content = data.get("content", "")
            timestamp = data.get("timestamp", time.time())
            
            # è°ƒè¯•ï¼šæŸ¥çœ‹å®Œæ•´çš„æ¶ˆæ¯æ•°æ®ç»“æ„
            _log.debug(f"ğŸ“‹ ç§èŠæ¶ˆæ¯å®Œæ•´æ•°æ®: {data}")

            # ä»contentä¸­æå–CQå›¾ç‰‡/è§†é¢‘/è¯­éŸ³URLï¼ˆç”¨äºå¤šæ¨¡æ€å¤„ç†ï¼‰
            _log.info(f"ğŸ” ç§èŠæ¶ˆæ¯å†…å®¹åˆ†æ: {content}")
            cleaned_content, image_urls = extract_cq_image_urls(content)
            _log.info(f"ğŸ“· å›¾ç‰‡CQç æå–: æ‰¾åˆ° {len(image_urls)} ä¸ª - {image_urls}")
            cleaned_content, video_urls = extract_cq_video_urls(cleaned_content)
            _log.info(f"ğŸ¥ è§†é¢‘CQç æå–: æ‰¾åˆ° {len(video_urls)} ä¸ª - {video_urls}")
            cleaned_content, audio_urls = extract_cq_audio_urls(cleaned_content)
            _log.info(f"ğŸµ è¯­éŸ³CQç æå–: æ‰¾åˆ° {len(audio_urls)} ä¸ª")
            # æå–æ–‡ä»¶URLï¼ˆä»…ç”¨äºæ—¥å¿—è®°å½•ï¼Œä¸ä¿®æ”¹contentï¼Œä¿ç•™åŸå§‹CQç ï¼‰
            _, file_urls = extract_cq_file_urls(content)
            # æ³¨æ„ï¼šä¸ä¿®æ”¹contentï¼Œä¿ç•™æ–‡ä»¶ã€é“¾æ¥ã€å¡ç‰‡çš„åŸå§‹CQç 
            content = cleaned_content
            
            if image_urls:
                _log.info(f"âœ… ä»CQç ä¸­æå–åˆ° {len(image_urls)} ä¸ªå›¾ç‰‡URL")
                cached_urls = []
                for original_url in image_urls:
                    cached = svc_download_image_to_storage(original_url, IMAGE_UPLOAD_DIR, server_base_url, _metrics_add, _log)
                    cached_urls.append(cached or original_url)
                image_urls = cached_urls
            # åˆå¹¶å®¢æˆ·ç«¯ç›´é“¾è§†é¢‘ï¼ˆè‹¥æä¾›ï¼‰
            req_video_urls = data.get("video_urls") or []
            _log.debug(f"ğŸ¥ å®¢æˆ·ç«¯æä¾›çš„video_urlså­—æ®µ: {req_video_urls}")
            if req_video_urls:
                video_urls = list(set((video_urls or []) + req_video_urls))
                _log.info(f"âœ… åˆå¹¶å®¢æˆ·ç«¯ç›´é“¾è§†é¢‘: {len(req_video_urls)} ä¸ª")
            # é¢„è¿‡æ»¤æ— æ•ˆè§†é¢‘æºï¼ˆä¿ç•™HTTP/HTTPS URLå’Œæœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼‰
            def _is_valid_video_source_prefilter_priv(p: str) -> bool:
                try:
                    if not p:
                        return False
                    # HTTP/HTTPS URL
                    if p.startswith(("http://", "https://")):
                        return True
                    # æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆWindowsæˆ–Linuxï¼‰
                    import os as _os
                    return _os.path.isfile(p)
                except Exception:
                    return False
            if video_urls:
                video_urls = [v for v in video_urls if _is_valid_video_source_prefilter_priv(v)]
                _log.debug(f"é¢„è¿‡æ»¤åçš„è§†é¢‘URLs: {video_urls}")
            # å¤„ç†è§†é¢‘URLæœ¬åœ°åŒ–ï¼ˆæ”¯æŒä¸­æ–­æ£€æŸ¥ï¼Œä½†è§†é¢‘ä¸‹è½½ä¼šç»§ç»­ï¼‰
            if video_urls:
                _log.info(f"âœ… ä»CQç ä¸­æå–åˆ° {len(video_urls)} ä¸ªè§†é¢‘URL")
                _log.debug(f"è§†é¢‘URLsè¯¦æƒ…: {video_urls}")
                cached_videos = []
                for i, v in enumerate(video_urls):
                    # æ£€æŸ¥æ˜¯å¦è¢«æ–°æ¶ˆæ¯ä¸­æ–­
                    if interrupt_event and interrupt_event.is_set():
                        if chat_id and response_dict:
                            with queue_lock:
                                current_processing = processing_chats.get(chat_id)
                                if current_processing and current_processing["response_dict"] is not response_dict:
                                    _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„è§†é¢‘ä¸‹è½½è¿‡ç¨‹ä¸­è¢«æ–°æ¶ˆæ¯ä¸­æ–­ï¼Œé€€å‡ºå¤„ç†ï¼ˆè§†é¢‘ä¸‹è½½ä¼šç»§ç»­ï¼‰")
                                    return
                    
                    _log.info(f"ğŸ“¥ æ­£åœ¨å¤„ç†è§†é¢‘ {i+1}/{len(video_urls)}: {v[:80]}...")
                    # æ£€æµ‹Windowsè·¯å¾„ï¼Œç›´æ¥è·³è¿‡ï¼ˆå®¢æˆ·ç«¯åº”è¯¥åœ¨ä¸Šä¼ å‰å¤„ç†ï¼‰
                    if re.match(r'^[a-zA-Z]:\\', v) or re.match(r'^\\\\', v):
                        _log.error(f"âŒ è·³è¿‡Windowsæœ¬åœ°è·¯å¾„ï¼ˆæœåŠ¡å™¨æ— æ³•è®¿é—®ï¼‰: {v}")
                        _log.error(f"ğŸ’¡ å®¢æˆ·ç«¯åº”è¯¥åœ¨å‘é€æ¶ˆæ¯å‰å°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ åˆ°æœåŠ¡å™¨")
                        continue  # è·³è¿‡è¿™ä¸ªè§†é¢‘
                    vc = svc_download_video_to_storage(v, VIDEO_UPLOAD_DIR, server_base_url, _metrics_add, _log)
                    if vc:
                        # ä¸‹è½½æˆåŠŸï¼Œä½¿ç”¨æœåŠ¡å™¨URL
                        cached_videos.append(vc)
                    else:
                        # ä¸‹è½½å¤±è´¥ï¼Œå¦‚æœæ˜¯HTTP URLä»ç„¶ä¿ç•™ï¼ˆå¯èƒ½å¯ä»¥è®¿é—®ï¼‰
                        if v.startswith(('http://', 'https://')):
                            _log.warning(f"âš ï¸ è§†é¢‘ä¸‹è½½å¤±è´¥ï¼Œä¿ç•™åŸå§‹HTTP URL: {v}")
                            cached_videos.append(v)
                        else:
                            _log.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè§†é¢‘URL: {v}")
                    
                    # å†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆä¸‹è½½å®Œæˆåï¼‰
                    if interrupt_event and interrupt_event.is_set():
                        if chat_id and response_dict:
                            with queue_lock:
                                current_processing = processing_chats.get(chat_id)
                                if current_processing and current_processing["response_dict"] is not response_dict:
                                    _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„è§†é¢‘ä¸‹è½½å®Œæˆåè¢«æ–°æ¶ˆæ¯ä¸­æ–­ï¼Œé€€å‡ºå¤„ç†")
                                    return
                # è¿‡æ»¤æ— æ•ˆè§†é¢‘è·¯å¾„/åè®®
                def _is_valid_video_source_priv(p: str) -> bool:
                    try:
                        if not p:
                            return False
                        if p.lower().startswith(("http://", "https://")):
                            return True
                        import os as _os
                        return _os.path.exists(p)
                    except Exception:
                        return False
                # å°†æœåŠ¡å™¨é™æ€URLè½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆtransformersåº“éœ€è¦æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œä¸æ”¯æŒHTTP URLï¼‰
                valid_video_paths = []
                base_static = (server_base_url or "http://127.0.0.1:9999").rstrip("/") + "/static/videos/"
                for v in cached_videos:
                    if v and v.startswith(base_static):
                        # è¿™æ˜¯æœåŠ¡å™¨URLï¼Œè½¬æ¢ä¸ºå¯¹åº”çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„
                        filename = v.split("/")[-1]
                        local_path = os.path.join(VIDEO_UPLOAD_DIR, filename)
                        if os.path.exists(local_path):
                            # æœ¬åœ°æ–‡ä»¶å­˜åœ¨ï¼Œä½¿ç”¨æœ¬åœ°æ–‡ä»¶è·¯å¾„
                            valid_video_paths.append(local_path)
                            _log.debug(f"ğŸ¥ ä½¿ç”¨æœ¬åœ°è§†é¢‘æ–‡ä»¶è·¯å¾„: {local_path}")
                        else:
                            # æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¿ç•™æœåŠ¡å™¨URLï¼ˆè™½ç„¶å¯èƒ½æ— æ³•è®¿é—®ï¼‰
                            valid_video_paths.append(v)
                            _log.warning(f"âš ï¸ è§†é¢‘æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨URL: {local_path}")
                    elif v and (v.startswith('http://') or v.startswith('https://')):
                        # å¤–éƒ¨URLï¼Œä¿ç•™ï¼ˆè™½ç„¶å¯èƒ½æ— æ³•è®¿é—®ï¼Œä½†è‡³å°‘æ ¼å¼æ­£ç¡®ï¼‰
                        valid_video_paths.append(v)
                        _log.debug(f"ğŸ¥ ä¿ç•™å¤–éƒ¨è§†é¢‘URL: {v}")
                    else:
                        # æ— æ•ˆURLï¼Œè·³è¿‡
                        _log.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè§†é¢‘URL: {v}")
                video_urls = valid_video_paths
            
            # æœ¬åœ°åŒ–è¯­éŸ³URLå¹¶è¿›è¡ŒASRè½¬å†™
            asr_texts: List[str] = []
            if 'audio_urls' in locals() and audio_urls:
                _log.info(f"âœ… ä»CQç ä¸­æå–åˆ° {len(audio_urls)} ä¸ªè¯­éŸ³URL")
                cached_audios = []
                for a in audio_urls:
                    cached = svc_download_audio_to_storage(a, AUDIO_UPLOAD_DIR, server_base_url, _metrics_add, _log)
                    cached_audios.append(cached or a)
                for ca in cached_audios:
                    if ca and ca.startswith((server_base_url or "http://127.0.0.1:9999").rstrip('/') + "/static/audios/"):
                        filename = ca.rsplit('/', 1)[-1]
                        local_fp = os.path.join(AUDIO_UPLOAD_DIR, filename)
                        text = svc_transcribe_audio(local_fp, _metrics_add, _log)
                        if text:
                            asr_texts.append(text)
                if asr_texts:
                    content = (content + "\n" if content else "") + "ã€è¯­éŸ³è½¬å†™ã€‘" + " ".join(asr_texts)
            
            # å¤„ç†æ–‡ä»¶ï¼šä¸‹è½½å¹¶æå–æ–‡æœ¬å’Œå›¾ç‰‡å†…å®¹ï¼ˆæ”¯æŒä¸­æ–­ï¼‰
            file_texts: List[str] = []
            file_image_paths: List[str] = []
            if 'file_urls' in locals() and file_urls:
                _log.info(f"âœ… æ£€æµ‹åˆ° {len(file_urls)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹æå–å†…å®¹")
                for file_url in file_urls:
                    # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
                    if interrupt_event and interrupt_event.is_set():
                        _log.info(f"âš ï¸ æ–‡ä»¶å¤„ç†è¢«ä¸­æ–­ï¼Œåœæ­¢å¤„ç†å‰©ä½™æ–‡ä»¶")
                        break
                    
                    try:
                        # ä¸‹è½½æ–‡ä»¶åˆ°æœåŠ¡å™¨
                        cached_file_url = svc_download_file_to_storage(file_url, FILE_UPLOAD_DIR, server_base_url, _metrics_add, _log)
                        
                        # å†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆä¸‹è½½å¯èƒ½è€—æ—¶ï¼‰
                        if interrupt_event and interrupt_event.is_set():
                            _log.info(f"âš ï¸ æ–‡ä»¶ä¸‹è½½åè¢«ä¸­æ–­ï¼Œåœæ­¢å¤„ç†")
                            break
                        
                        if cached_file_url and cached_file_url.startswith((server_base_url or "http://127.0.0.1:9999").rstrip('/') + "/static/files/"):
                            # å°†URLè½¬æˆæœ¬åœ°æ–‡ä»¶è·¯å¾„
                            filename = cached_file_url.rsplit('/', 1)[-1]
                            local_fp = os.path.join(FILE_UPLOAD_DIR, filename)
                            if os.path.exists(local_fp):
                                # æå–æ–‡æœ¬å’Œå›¾ç‰‡
                                file_text, file_images = svc_extract_text_and_images_from_file(
                                    local_fp, IMAGE_UPLOAD_DIR, _metrics_add, _log
                                )
                                
                                # å†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆæå–å¯èƒ½è€—æ—¶ï¼‰
                                if interrupt_event and interrupt_event.is_set():
                                    _log.info(f"âš ï¸ æ–‡ä»¶æå–åè¢«ä¸­æ–­ï¼Œåœæ­¢å¤„ç†")
                                    break
                                
                                if file_text:
                                    file_texts.append(file_text)
                                # å°†æå–çš„å›¾ç‰‡è·¯å¾„è½¬æ¢ä¸ºURLå¹¶æ·»åŠ åˆ°image_urls
                                for img_path in file_images:
                                    if interrupt_event and interrupt_event.is_set():
                                        break
                                    if os.path.exists(img_path):
                                        img_filename = os.path.basename(img_path)
                                        img_url = f"{server_base_url.rstrip('/')}/static/images/{img_filename}"
                                        if image_urls is None:
                                            image_urls = []
                                        if img_url not in image_urls:
                                            image_urls.append(img_url)
                                            file_image_paths.append(img_path)
                                if file_text or file_images:
                                    _log.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: æ–‡æœ¬é•¿åº¦={len(file_text)}, å›¾ç‰‡æ•°={len(file_images)}")
                    except Exception as file_err:
                        _log.warning(f"âš ï¸ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_url}: {file_err}")
                
                # å°†æå–çš„æ–‡æœ¬å†…å®¹æ·»åŠ åˆ°contentï¼ˆå¦‚æœæœªè¢«ä¸­æ–­ï¼‰
                if not (interrupt_event and interrupt_event.is_set()) and file_texts:
                    file_content = "\n\n".join([f"ã€æ–‡ä»¶å†…å®¹{i+1}ã€‘\n{t}" for i, t in enumerate(file_texts)])
                    content = (content + "\n\n" if content else "") + file_content

            media_info = ""
            if image_urls:
                media_info += f" [åŒ…å«{len(image_urls)}å¼ å›¾ç‰‡]"
            if video_urls:
                media_info += f" [åŒ…å«{len(video_urls)}ä¸ªè§†é¢‘]"
            if 'audio_urls' in locals() and audio_urls:
                media_info += f" [åŒ…å«{len(audio_urls)}æ®µè¯­éŸ³]"
            if 'file_urls' in locals() and file_urls:
                media_info += f" [åŒ…å«{len(file_urls)}ä¸ªæ–‡ä»¶]"
            _log.info(f"æ”¶åˆ°ç§èŠæ¶ˆæ¯ [ç”¨æˆ·:{user_id}({user_nickname})]: {content[:50] if content else '(ä»…å¤šåª’ä½“)'}{media_info}...")
            
            # æ ¼å¼åŒ–æ—¶é—´æˆ³
            time_str = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
            # æ ¼å¼åŒ–ç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«QQå·ä¿¡æ¯ï¼Œæ–¹ä¾¿æ¨¡å‹åœ¨å†å²æ¶ˆæ¯ä¸­è¯†åˆ«ç”¨æˆ·ï¼‰
            message_prefix = f"[{time_str}] {user_nickname}(QQ:{user_id})"
            formatted_message = f"{message_prefix}: {content}" if content else f"{message_prefix}:"
            
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            if image_urls or video_urls:
                message_content = format_multimodal_message(formatted_message, image_urls, video_urls if 'video_urls' in locals() else [])
            else:
                message_content = [{"type": "text", "text": formatted_message}]
            
            # é“¾æ¥å’Œå¡ç‰‡ä¿¡æ¯ç›´æ¥ä¿ç•™åŸå§‹CQç åœ¨contentä¸­ï¼Œä¸æå–å†…å®¹ï¼Œä¸æ„å»ºç›®å½•
            # ï¼ˆå·²ç§»é™¤ç›®å½•æ„å»ºé€»è¾‘ï¼Œä¿ç•™åŸå§‹ä¿¡æ¯ï¼‰
            
            # æ›´æ–°èŠå¤©è®°å½•
            with chat_history_lock:
                if user_id not in private_chat_histories:
                    private_chat_histories[user_id] = []
                
                private_chat_histories[user_id].append({
                    "role": "user",
                    "content": message_content
                })
                
                maintain_chat_history("private", user_id, private_chat_histories[user_id])
            
            # åœ¨ç”Ÿæˆå›å¤ä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„ä»»åŠ¡ï¼ˆé˜²æ­¢åœ¨åŠ å…¥å†å²æœŸé—´æœ‰æ–°æ¶ˆæ¯åˆ°è¾¾ï¼‰
            # åŒæ—¶æ£€æŸ¥interrupt_eventæ˜¯å¦è¢«é”™è¯¯è®¾ç½®
            with queue_lock:
                current_processing = processing_chats.get(chat_id)
                if current_processing and current_processing["response_dict"] is not response_dict:
                    # å·²ç»æœ‰æ›´æ–°çš„ä»»åŠ¡äº†ï¼Œå½“å‰ä»»åŠ¡åº”è¯¥é€€å‡ºï¼ˆä½†æ¶ˆæ¯å·²ç»åŠ å…¥å†å²ï¼Œè¿™æ˜¯æ­£ç¡®çš„ï¼‰
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå›å¤å‰å·²è¢«æ›´æ–°çš„æ¶ˆæ¯æ›¿æ¢ï¼Œé€€å‡ºå¤„ç†")
                    return
                
                # å¦‚æœå½“å‰ä»»åŠ¡ä»ç„¶æ˜¯æœ€æ–°çš„ï¼Œä½†interrupt_eventè¢«è®¾ç½®äº†ï¼Œæ¸…é™¤å®ƒ
                # è¿™å¯èƒ½æ˜¯è¯¯è®¾ç½®ï¼ˆæ¯”å¦‚åœ¨ç­‰å¾…æœŸé—´è¢«ä¸­æ–­ï¼Œä½†ä¹‹ååˆæˆä¸ºæœ€æ–°ä»»åŠ¡ï¼‰
                if interrupt_event.is_set():
                    _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå›å¤å‰æ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†ä»»åŠ¡ä»æ˜¯æœ€æ–°çš„ï¼Œæ¸…é™¤ä¸­æ–­ä¿¡å·")
                    interrupt_event.clear()
            
            # ç”Ÿæˆå›å¤ï¼ˆæ”¯æŒæœ€å¤š1è½®å·¥å…·è°ƒç”¨å†ç”Ÿæˆï¼‰
            _log.info(f"ğŸ§  å¼€å§‹ç”Ÿæˆç§èŠå›å¤ï¼ˆç”¨æˆ· {user_id}ï¼‰...")
            chat_context = {
                "user_id": user_id,
                "user_nickname": user_nickname
            }
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = build_system_prompt("private", chat_context)
            
            # åœ¨ç”Ÿæˆå‰ï¼Œå…ˆå¯¹åŸå§‹å†å²è¿›è¡Œtokené•¿åº¦æ£€æŸ¥å’Œæˆªæ–­
            with chat_history_lock:
                # ç¡®ä¿user_idå­˜åœ¨äºprivate_chat_historiesä¸­
                if user_id not in private_chat_histories:
                    _log.warning(f"âš ï¸ ç”¨æˆ· {user_id} çš„èŠå¤©å†å²ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨")
                    private_chat_histories[user_id] = []
                
                # å¯¹åŸå§‹å†å²è¿›è¡Œæˆªæ–­ï¼ˆä¼šä¿®æ”¹åŸå§‹å†å²å¹¶ä¿å­˜è¢«åˆ é™¤çš„æ¶ˆæ¯ï¼‰
                # ç¡®ä¿user_idå­˜åœ¨äºprivate_chat_historiesä¸­
                if user_id not in private_chat_histories:
                    _log.warning(f"âš ï¸ ç”¨æˆ· {user_id} çš„èŠå¤©å†å²ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨")
                    private_chat_histories[user_id] = []

                truncated_history = private_chat_histories[user_id].copy()  # é»˜è®¤å€¼ï¼Œä½¿ç”¨å‰¯æœ¬
                _log.info(f"ğŸ“Š åŸå§‹èŠå¤©å†å²é•¿åº¦: {len(private_chat_histories[user_id])}ï¼ˆç§èŠ {user_id}ï¼‰")

                try:
                    max_tokens_limit = get_chat_history_token_limit()
                    _log.info(f"ğŸ“Š è·å–åˆ°çš„max_tokensé™åˆ¶: {max_tokens_limit}, ç±»å‹: {type(max_tokens_limit)}ï¼ˆç§èŠ {user_id}ï¼‰")
                    
                    # éªŒè¯max_tokens_limit
                    if max_tokens_limit is None:
                        _log.error(f"âŒ max_tokens_limitä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼35000ï¼ˆç§èŠ {user_id}ï¼‰")
                        max_tokens_limit = 35000
                    elif not isinstance(max_tokens_limit, int) or max_tokens_limit <= 0:
                        _log.error(f"âŒ max_tokens_limitæ— æ•ˆ: {max_tokens_limit}ï¼Œä½¿ç”¨é»˜è®¤å€¼35000ï¼ˆç§èŠ {user_id}ï¼‰")
                        max_tokens_limit = 35000

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æˆªæ–­
                    if len(private_chat_histories[user_id]) == 0:
                        _log.info("ğŸ“Š èŠå¤©å†å²ä¸ºç©ºï¼Œæ— éœ€æˆªæ–­")
                        truncated_history = []
                    else:
                        _log.info(f"ğŸ“Š å¼€å§‹è°ƒç”¨truncate_history_by_tokensï¼ˆç§èŠ {user_id}ï¼‰ï¼Œmax_tokens={max_tokens_limit}")
                        result = truncate_history_by_tokens(
                            private_chat_histories[user_id],
                            system_prompt,
                            "private",
                            user_id,
                            max_tokens=max_tokens_limit,
                            interrupt_event=interrupt_event
                        )
                        _log.info(f"ğŸ“Š truncate_history_by_tokensè¿”å›: ç±»å‹={type(result)}, æ˜¯å¦ä¸ºNone={result is None}, é•¿åº¦={len(result) if result is not None else 'N/A'}ï¼ˆç§èŠ {user_id}ï¼‰")
                        truncated_history = result

                    # ç¡®ä¿è¿”å›å€¼ä¸ä¸ºNoneä¸”æ˜¯åˆ—è¡¨ç±»å‹
                    if truncated_history is None:
                        _log.error(f"âŒ æˆªæ–­å†å²è¿”å›Noneï¼ˆç§èŠ {user_id}ï¼‰ï¼Œå›é€€åˆ°åŸå§‹å†å²")
                        truncated_history = private_chat_histories[user_id].copy()
                    elif not isinstance(truncated_history, list):
                        _log.error(f"âŒ æˆªæ–­å†å²è¿”å›éåˆ—è¡¨ç±»å‹: {type(truncated_history)}ï¼ˆç§èŠ {user_id}ï¼‰ï¼Œå›é€€åˆ°åŸå§‹å†å²")
                        truncated_history = private_chat_histories[user_id].copy()
                    else:
                        _log.info(f"âœ… æˆªæ–­å†å²æˆåŠŸï¼Œé•¿åº¦: {len(truncated_history)}ï¼ˆç§èŠ {user_id}ï¼‰")
                except Exception as e:
                    _log.error(f"âŒ æˆªæ–­å†å²æ—¶å‘ç”Ÿå¼‚å¸¸ï¼ˆç§èŠ {user_id}ï¼‰: {e}", exc_info=True)
                    # å¼‚å¸¸æƒ…å†µä¸‹ï¼Œä½¿ç”¨åŸå§‹å†å²
                    truncated_history = private_chat_histories[user_id].copy()
                
                # ä½¿ç”¨æˆªæ–­åçš„å†å²ï¼ˆå·²ç»æ˜¯åŸå§‹å†å²çš„å¼•ç”¨ï¼‰
                current_history = truncated_history.copy()
            
            # å…³é—­FETCHç›¸å…³å¾ªç¯ï¼ˆæŒ‰å½“å‰éœ€æ±‚æ”¾å¼ƒé“¾æ¥/å¡ç‰‡/æ–‡ä»¶è®¿é—®ï¼‰
            tool_iterations = 0
            action_cmds = []
            for _iter in range(tool_iterations + 1):
                _gen_ret = generate_reply(
                current_history,
                chat_type="private",
                chat_context=chat_context,
                interrupt_event=interrupt_event,
                chat_id=chat_id,
                    response_dict=response_dict,
                    log_full_io=True
            )
                if isinstance(_gen_ret, tuple) and len(_gen_ret) == 3:
                    reply, should_reply, was_interrupted = _gen_ret
                    action_cmds = []
                else:
                    reply, should_reply, was_interrupted, action_cmds = _gen_ret
                if was_interrupted:
                    break
                pending_fetch = []
                if not pending_fetch or _iter >= tool_iterations:
                    break
                # FETCHåŠŸèƒ½å·²ç¦ç”¨ï¼Œæ— éœ€å¤„ç†å·¥å…·è°ƒç”¨
            
            # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
            if was_interrupted:
                _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„æ¶ˆæ¯å¤„ç†è¢«ä¸­æ–­ï¼Œè·³è¿‡å›å¤")
                _metrics_add("interruptions", 1)
                # è¢«ä¸­æ–­çš„æ¶ˆæ¯éœ€è¦æ›´æ–°response_dictï¼Œå¦åˆ™å®¢æˆ·ç«¯ä¼šä¸€ç›´ç­‰å¾…
                # ä½†éœ€è¦ç¡®ä¿è¿™æ˜¯å½“å‰ä»»åŠ¡ï¼Œé¿å…æ—§ä»»åŠ¡è¦†ç›–æ–°ä»»åŠ¡çš„response_dict
                with queue_lock:
                    current_processing = processing_chats.get(chat_id)
                    if current_processing and current_processing["response_dict"] is response_dict:
                        response_dict.update({
                            "status": "success",
                            "should_reply": False,
                            "reply": ""
                        })
                        _log.info(f"âœ… å·²æ›´æ–°ä¸­æ–­å“åº”ï¼ˆèŠå¤© {chat_id}ï¼‰")
                return
            
            # åœ¨æ›´æ–°èŠå¤©è®°å½•ä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„ä»»åŠ¡
            # å› ä¸ºå¯èƒ½åœ¨ç”ŸæˆæœŸé—´æœ‰æ–°æ¶ˆæ¯åˆ°è¾¾å¹¶è®¾ç½®äº†interrupt_event
            with queue_lock:
                current_processing = processing_chats.get(chat_id)
                if current_processing and current_processing["response_dict"] is not response_dict:
                    # å·²ç»æœ‰æ›´æ–°çš„ä»»åŠ¡äº†ï¼Œå½“å‰ä»»åŠ¡åº”è¯¥é€€å‡º
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå®Œæˆåè¢«æ›´æ–°çš„æ¶ˆæ¯æ›¿æ¢ï¼Œè·³è¿‡æ›´æ–°å†å²")
                    return
                # å†æ¬¡æ£€æŸ¥ä¸­æ–­äº‹ä»¶ï¼ˆåŒé‡ä¿é™©ï¼‰
                if interrupt_event.is_set():
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå®Œæˆåè¢«ä¸­æ–­ï¼Œè·³è¿‡æ›´æ–°å†å²")
                    return
            
            # åœ¨æ›´æ–°èŠå¤©è®°å½•ä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆé˜²æ­¢åœ¨ç”Ÿæˆå®Œæˆåã€æ›´æ–°å‰æœ‰æ–°æ¶ˆæ¯åˆ°è¾¾ï¼‰
            with queue_lock:
                current_processing = processing_chats.get(chat_id)
                if current_processing and current_processing["response_dict"] is not response_dict:
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ›´æ–°èŠå¤©è®°å½•å‰å·²è¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œè·³è¿‡æ›´æ–°")
                    return
                if interrupt_event.is_set():
                    _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ›´æ–°èŠå¤©è®°å½•å‰è¢«ä¸­æ–­ï¼Œè·³è¿‡æ›´æ–°")
                    return
            
            # æ›´æ–°èŠå¤©è®°å½•ï¼ˆåªæœ‰åœ¨æ²¡æœ‰è¢«ä¸­æ–­çš„æƒ…å†µä¸‹ï¼‰
            with chat_history_lock:
                # åœ¨æŒæœ‰chat_history_lockæœŸé—´å†æ¬¡æ£€æŸ¥ä¸­æ–­ï¼ˆåŒé‡ä¿é™©ï¼‰
                if interrupt_event and interrupt_event.is_set():
                    with queue_lock:
                        current_processing = processing_chats.get(chat_id)
                        if current_processing and current_processing["response_dict"] is not response_dict:
                            _log.info(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ›´æ–°èŠå¤©è®°å½•æœŸé—´è¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œè·³è¿‡æ›´æ–°")
                            return
                
                if should_reply:
                    _metrics_add("replies_sent", 1)
                    private_chat_histories[user_id].append({
                        "role": "assistant",
                        "content": [{"type": "text", "text": reply}]
                    })
                    maintain_chat_history("private", user_id, private_chat_histories[user_id])
                    _log.info(f"ğŸ’¬ ç”Ÿæˆå›å¤ï¼ˆç§èŠ {user_id}ï¼‰ï¼š{reply[:100]}...")
                else:
                    _metrics_add("no_reply", 1)
                    private_chat_histories[user_id].append({
                        "role": "assistant",
                        "content": [{"type": "text", "text": "<no_reply>"}]
                    })
                    maintain_chat_history("private", user_id, private_chat_histories[user_id])
                    _log.info(f"ğŸ’¬ æ¨¡å‹åˆ¤æ–­ä¸éœ€è¦å›å¤ï¼ˆç§èŠ {user_id}ï¼‰")
            
            # æ›´æ–°å“åº”ï¼ˆåªæœ‰åœ¨æ²¡æœ‰è¢«ä¸­æ–­çš„æƒ…å†µä¸‹ï¼‰
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„ä»»åŠ¡ï¼ˆé˜²æ­¢åœ¨æ›´æ–°èŠå¤©è®°å½•æ—¶è¢«æ–°æ¶ˆæ¯ä¸­æ–­ï¼‰
            with queue_lock:
                current_processing = processing_chats.get(chat_id)
                if current_processing and current_processing["response_dict"] is response_dict:
                    # å†æ¬¡æ£€æŸ¥ä¸­æ–­äº‹ä»¶ï¼ˆé˜²æ­¢åœ¨æ›´æ–°èŠå¤©è®°å½•æ—¶è¢«ä¸­æ–­ï¼‰
                    if interrupt_event.is_set():
                        _log.warning(f"âš ï¸ ä»»åŠ¡åœ¨æ›´æ–°å“åº”å‰è¢«ä¸­æ–­ï¼ˆç§èŠ {user_id}ï¼‰")
                        return
                    
                    response_dict.update({
                        "status": "success",
                        "should_reply": should_reply,
                        "reply": reply if should_reply else "",
                        "actions": action_cmds if should_reply else []
                    })
                    _log.info(f"âœ… å·²æ›´æ–°å“åº”ï¼ˆç§èŠ {user_id}ï¼‰ï¼Œshould_reply={should_reply}, replyé•¿åº¦={len(reply) if reply else 0}")
                else:
                    _log.warning(f"âš ï¸ ä»»åŠ¡å·²è¢«æ–°æ¶ˆæ¯ä¸­æ–­ï¼Œè·³è¿‡å“åº”æ›´æ–°ï¼ˆç§èŠ {user_id}ï¼‰")
        # è®°å½•è¯·æ±‚å¤„ç†ç«¯åˆ°ç«¯æ—¶å»¶
        try:
            _metrics_add_latency((_t.time() - _req_t0) * 1000.0)
        except Exception:
            pass
        
    except Exception as e:
        _log.error(f"å¤„ç†æ¶ˆæ¯ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        response_dict.update({
            "status": "error",
            "message": str(e),
            "status_code": 500
        })
    finally:
        # æ¸…ç†å¤„ç†çŠ¶æ€ï¼ˆåªæœ‰åœ¨è¿™æ˜¯å½“å‰ä»»åŠ¡æ—¶æ‰æ¸…ç†ï¼‰
        with queue_lock:
            current_processing = processing_chats.get(chat_id)
            if current_processing and current_processing["response_dict"] is response_dict:
                del processing_chats[chat_id]
                _log.debug(f"æ¸…ç†å¤„ç†çŠ¶æ€ï¼ˆ{chat_type} {chat_id}ï¼‰")
            else:
                _log.debug(f"è·³è¿‡æ¸…ç†ï¼ˆä»»åŠ¡å·²è¢«æ–°æ¶ˆæ¯æ›¿æ¢ï¼Œ{chat_type} {chat_id}ï¼‰")


def message_queue_worker():
    """æ¶ˆæ¯é˜Ÿåˆ—å·¥ä½œçº¿ç¨‹ï¼ˆå¤„ç†é˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯ï¼‰"""
    global message_queue
    _log.info("ğŸ“‹ æ¶ˆæ¯é˜Ÿåˆ—å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨")
    
    while True:
        try:
            # ä»é˜Ÿåˆ—è·å–æ¶ˆæ¯ï¼ˆé˜»å¡ç­‰å¾…ï¼‰
            task = message_queue.get()
            
            if task is None:  # é€€å‡ºä¿¡å·
                break
            
            _log.info(f"ğŸ”„ å¼€å§‹å¤„ç†æ¶ˆæ¯ä»»åŠ¡: {task.chat_type} {task.chat_id}")
            
            # åœ¨æ–°çº¿ç¨‹ä¸­å¤„ç†ä»»åŠ¡ï¼Œè¿™æ ·åŒä¸€èŠå¤©çš„å¤šä¸ªæ¶ˆæ¯å¯ä»¥å¹¶å‘å¤„ç†
            # ä¸­æ–­æœºåˆ¶ä¼šåœ¨process_message_taskå†…éƒ¨å¤„ç†
            task_thread = threading.Thread(
                target=svc_handler.run_process_message_task,
                args=(task,),
                daemon=True
            )
            task_thread.start()
            
            # ä¸ç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ¶ˆæ¯
            # è¿™æ ·åŒä¸€èŠå¤©çš„å¤šæ¡æ¶ˆæ¯å¯ä»¥å¹¶å‘å¤„ç†ï¼Œæ–°æ¶ˆæ¯ä¼šä¸­æ–­æ—§æ¶ˆæ¯
            message_queue.task_done()
            
        except Exception as e:
            _log.error(f"æ¶ˆæ¯é˜Ÿåˆ—å·¥ä½œçº¿ç¨‹å‡ºé”™: {e}", exc_info=True)


# ä½¿ç”¨ services.generation ä¸­çš„å®ç°
InterruptStoppingCriteria = SvcInterruptStoppingCriteria


def custom_generate(
    model,
    inputs,
    max_new_tokens: int = 1000,
    stopping_criteria: StoppingCriteriaList = None,
    logits_processor: LogitsProcessorList = None,
    temperature: float = 1.0,
    top_k: int = None,
    top_p: float = None,
    do_sample: bool = True,
    pad_token_id: int = None,
    eos_token_id: int = None,
    interrupt_event: threading.Event = None,
    early_stop_on_tool_call: bool = False,
):
    """
    å®Œå…¨å¤åˆ»transformersåº“çš„model.generate()æ–¹æ³•å®ç°çš„è‡ªå®šä¹‰ç”Ÿæˆå‡½æ•°
    
    å‚è€ƒå®˜æ–¹æºç å®ç°ï¼Œå®Œå…¨æŒ‰ç…§å®˜æ–¹é€»è¾‘ï¼š
    1. _get_initial_cache_position() - åˆå§‹åŒ–cache_positionï¼ˆå®˜æ–¹æ–¹æ³•ï¼‰
    2. prepare_inputs_for_generation() - å‡†å¤‡æ¨¡å‹è¾“å…¥ï¼ˆè‡ªåŠ¨å¤„ç†KV cacheå’Œattention_maskï¼‰
    3. _update_model_kwargs_for_generation() - æ›´æ–°model_kwargsï¼ˆåŒ…æ‹¬past_key_valuesã€attention_maskã€cache_positionï¼‰
    4. LogitsProcessorå¤„ç†ï¼ˆå¦‚repetition_penaltyï¼‰
    5. LogitsWarperå¤„ç†ï¼ˆå¦‚temperature, top_k, top_pï¼‰
    6. StoppingCriteriaæ£€æŸ¥ï¼ˆæ¯ä¸ªtokenåæ£€æŸ¥ï¼‰
    7. EOS tokenæ£€æŸ¥ï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹é€»è¾‘ï¼‰
    8. æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼ˆpixel_valuesç­‰ï¼‰
    
    è¿™ä¸ªå®ç°å®Œå…¨æŒ‰ç…§å®˜æ–¹æºç é€»è¾‘ï¼Œå¯ä»¥æ–¹ä¾¿åç»­è¿›è¡Œé­”æ”¹ã€‚
    """
    # è·å–è¾“å…¥
    input_ids = inputs.get('input_ids')
    attention_mask = inputs.get('attention_mask', None)
    
    # åˆå§‹åŒ–ç”ŸæˆçŠ¶æ€
    batch_size = input_ids.shape[0]
    cur_len = input_ids.shape[-1]
    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)
    
    # åˆå§‹åŒ–stopping_criteria
    if stopping_criteria is None:
        stopping_criteria = StoppingCriteriaList()
    
    # åˆå§‹åŒ–logits_processor
    if logits_processor is None:
        logits_processor = LogitsProcessorList()
    
    # æ„å»ºlogits_warperï¼ˆé‡‡æ ·æ—¶ä½¿ç”¨ï¼‰
    logits_warper = None
    if do_sample:
        logits_warper_list = []
        if temperature is not None and temperature != 1.0:
            logits_warper_list.append(TemperatureLogitsWarper(temperature=temperature))
        if top_k is not None and top_k > 0:
            logits_warper_list.append(TopKLogitsWarper(top_k=top_k))
        if top_p is not None and top_p < 1.0:
            logits_warper_list.append(TopPLogitsWarper(top_p=top_p))
        if logits_warper_list:
            logits_warper = LogitsProcessorList(logits_warper_list)
    
    # å‡†å¤‡model_kwargsï¼ˆå®Œå…¨æŒ‰ç…§transformerså®˜æ–¹å®ç°ï¼‰
    # ä»inputsä¸­æå–æ‰€æœ‰éinput_idså’Œattention_maskçš„å­—æ®µ
    model_kwargs = {}
    for key, value in inputs.items():
        if key not in ['input_ids', 'attention_mask']:
            model_kwargs[key] = value
    
    # å¦‚æœæä¾›äº†attention_maskï¼Œæ·»åŠ åˆ°model_kwargs
    if attention_mask is not None:
        model_kwargs['attention_mask'] = attention_mask
    
    # ç¡®ä¿use_cacheå­˜åœ¨ï¼ˆé»˜è®¤Trueï¼‰
    if 'use_cache' not in model_kwargs:
        model_kwargs['use_cache'] = True
    
    # åˆå§‹åŒ–cache_positionï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹_get_initial_cache_positioné€»è¾‘ï¼‰
    # å‚è€ƒå®˜æ–¹æºç ï¼š_get_initial_cache_positionæ–¹æ³•
    # è¿™æ˜¯prefillingé˜¶æ®µï¼Œç¡®å®šè¾“å…¥çš„é•¿åº¦
    if not model_kwargs.get("use_cache", True):
        model_kwargs["cache_position"] = None
    else:
        past_length = 0
        # å¦‚æœè¾“å…¥äº† past_key_valuesï¼Œåˆ™æ ¹æ® past_key_values ç¡®å®šç¼“å­˜åºåˆ—çš„é•¿åº¦
        if "past_key_values" in model_kwargs and model_kwargs["past_key_values"] is not None:
            try:
                from transformers.cache_utils import Cache
                if isinstance(model_kwargs["past_key_values"], Cache):
                    past_length = model_kwargs["past_key_values"].get_seq_length()
                else:
                    past_length = model_kwargs["past_key_values"][0][0].shape[2]
            except (ImportError, AttributeError):
                # å¦‚æœä¸æ˜¯Cacheç±»å‹ï¼Œç›´æ¥å–shape
                past_length = model_kwargs["past_key_values"][0][0].shape[2]
        
        # å¦‚æœè¾“å…¥ inputs_embeds åˆ™æ ¹æ®è¿™ä¸ªç¡®å®š
        if "inputs_embeds" in model_kwargs:
            input_seq_len = model_kwargs["inputs_embeds"].shape[1]
        else:
            # éƒ½æ²¡æœ‰å°±æ ¹æ® input_ids ç¡®å®š
            input_seq_len = input_ids.shape[-1]
        
        # åˆ›å»ºè¾“å…¥åºåˆ—çš„ä½ç½®ç´¢å¼•ï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹é€»è¾‘ï¼‰
        # cache_position = torch.arange(past_length, input_seq_len, device=input_ids.device)
        model_kwargs["cache_position"] = torch.arange(past_length, input_seq_len, device=input_ids.device)
    
    # å¤„ç†EOS tokenï¼ˆè½¬æ¢ä¸ºåˆ—è¡¨å½¢å¼ï¼‰
    if eos_token_id is not None:
        if isinstance(eos_token_id, (list, tuple)):
            eos_token_ids = torch.tensor(list(eos_token_id), device=input_ids.device)
        else:
            eos_token_ids = torch.tensor([eos_token_id], device=input_ids.device)
    else:
        eos_token_ids = None
    
    # æ£€æŸ¥æ˜¯å¦æœ‰EOSåœæ­¢æ¡ä»¶ï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
    has_eos_stopping_criteria = eos_token_ids is not None
    
    # è·å–recall token IDï¼ˆç”¨äºæ£€æµ‹å›å¿†è§¦å‘ï¼‰
    global recall_token_ids, memory_db, processor
    recall_token_id = recall_token_ids.get("<recall>") if recall_token_ids else None
    memory_pad_token_id = recall_token_ids.get("<|memory_pad|>") if recall_token_ids else None
    
    # è®°å½•è®°å¿†å‘é‡æ’å…¥ä½ç½®ï¼ˆç”¨äºè¿”å›ï¼Œä½†ä¸å†ç”¨äºæ‰“å°æ ‡æ³¨ï¼Œå› ä¸º<|memory_pad|>ä¼šåŸç”Ÿæ˜¾ç¤ºï¼‰
    memory_injection_positions = []  # å­˜å‚¨ (token_position, memory_score) å…ƒç»„
    
    memory_cfg = config.get("memory", {}).get("autoregressive_recall", {})
    autorecall_enabled = bool(memory_cfg.get("enabled", False))
    autorecall_top_k = max(1, int(memory_cfg.get("top_k", 5)))
    autorecall_temperature = float(memory_cfg.get("temperature", 1.0))
    autorecall_top_p = float(memory_cfg.get("top_p", 1.0))
    autorecall_use_sampling = bool(memory_cfg.get("use_sampling", True))  # é»˜è®¤ä½¿ç”¨é‡‡æ ·
    autorecall_debug = bool(memory_cfg.get("debug", False))
    recall_pending = False

    def _update_model_kwargs_helper(outputs_obj):
        """å®‰å…¨æ›´æ–°model_kwargsï¼Œå…¼å®¹ä¸åŒç‰ˆæœ¬transformers"""
        nonlocal model_kwargs
        try:
            model_kwargs = model._update_model_kwargs_for_generation(
                outputs_obj,
                model_kwargs,
                is_encoder_decoder=False,
                standardize_cache_format=True,
            )
        except TypeError:
            try:
                model_kwargs = model._update_model_kwargs_for_generation(
                    outputs_obj,
                    model_kwargs,
                    is_encoder_decoder=False,
                )
            except TypeError:
                model_kwargs = model._update_model_kwargs_for_generation(
                    outputs_obj,
                    model_kwargs,
                )
            
    def _forward_with_last_hidden_state(forward_inputs):
        """
        ä½¿ç”¨backboneæ‰§è¡Œä¸€æ¬¡forwardï¼Œè¿”å›ç­‰ä»·äºCausalLMOutputWithPastçš„ç»“æœï¼Œ
        å¹¶é™„å¸¦last_hidden_stateä¾›å›å¿†æœºåˆ¶ä½¿ç”¨ã€‚
        """
        local_inputs = dict(forward_inputs)
        use_cache_flag = local_inputs.pop("use_cache", True)
        output_hidden_flag = local_inputs.pop("output_hidden_states", False)

        backbone_outputs = forward_backbone(
            model,
            use_cache=use_cache_flag,
            output_hidden_states=output_hidden_flag,
            return_dict=True,
            **local_inputs,
        )
        outputs = build_causal_lm_output(model, backbone_outputs)
        last_hidden_state = ensure_last_hidden_state(backbone_outputs)
        outputs.last_hidden_state = last_hidden_state
        return outputs
            
    def _sample_memory_embedding_from_db(query_vec):
        """æ ¹æ®æŸ¥è¯¢å‘é‡ä»è®°å¿†åº“ä¸­é‡‡æ ·è®°å¿†embedding"""
        if memory_db is None or len(memory_db) == 0:
            _log.info("ğŸ” [å‘é‡åŒ¹é…] è®°å¿†å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…")
            return None, None

        _log.info(f"ğŸ” [å‘é‡åŒ¹é…] å¼€å§‹æœç´¢è®°å¿†åº“ï¼ŒæŸ¥è¯¢å‘é‡shape: {query_vec.shape}, top_k={autorecall_top_k}")
        search_results = memory_db.search(
            query_vec.detach().clone(),
            top_k=max(autorecall_top_k, 1),
            debug=autorecall_debug
        )
        if not search_results:
            _log.info("ğŸ” [å‘é‡åŒ¹é…] æœªæ‰¾åˆ°åŒ¹é…çš„è®°å¿†å‘é‡")
            return None, None

        _log.info(f"ğŸ” [å‘é‡åŒ¹é…] æ‰¾åˆ° {len(search_results)} ä¸ªå€™é€‰è®°å¿†å‘é‡")
        for i, result in enumerate(search_results):
            score = result.get('score', 0.0)
            _log.info(f"  [{i+1}] ç›¸ä¼¼åº¦={score:.4f}")

        temperature = max(1e-5, autorecall_temperature)
        scores = torch.tensor(
            [item['score'] for item in search_results],
            dtype=torch.float32,
            device=query_vec.device
        )
                
        # å¯é€‰ top-p æˆªæ–­
        if 0 < autorecall_top_p < 1.0:
            sorted_scores, sorted_indices = torch.sort(scores, descending=True)
            probs_for_p = torch.softmax(sorted_scores / temperature, dim=-1)
            cumulative = torch.cumsum(probs_for_p, dim=-1)
            cutoff_mask = cumulative <= autorecall_top_p
            cutoff_mask[..., 0] = True  # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ª
            valid_indices = sorted_indices[cutoff_mask]
            if len(valid_indices) > 0:
                scores = scores[valid_indices]
                search_results = [search_results[i.item()] for i in valid_indices]
                _log.info(f"ğŸ” [å‘é‡åŒ¹é…] top_p={autorecall_top_p} æˆªæ–­åä¿ç•™ {len(search_results)} ä¸ªå€™é€‰")
            probs = torch.softmax(scores / temperature, dim=-1)
        else:
            probs = torch.softmax(scores / temperature, dim=-1)

        if autorecall_use_sampling:
            choice_idx = torch.multinomial(probs, num_samples=1).item()
            _log.info(f"ğŸ” [å‘é‡åŒ¹é…] ä½¿ç”¨é‡‡æ ·æ–¹å¼é€‰æ‹©è®°å¿†ï¼Œé€‰æ‹©ç´¢å¼•: {choice_idx}, æ¦‚ç‡: {probs[choice_idx]:.4f}")
        else:
            choice_idx = torch.argmax(scores).item()
            _log.info(f"ğŸ” [å‘é‡åŒ¹é…] ä½¿ç”¨è´ªå©ªæ–¹å¼é€‰æ‹©è®°å¿†ï¼Œé€‰æ‹©ç´¢å¼•: {choice_idx}, æœ€é«˜ç›¸ä¼¼åº¦: {scores[choice_idx]:.4f}")

        selected = search_results[choice_idx]
        embedding_tensor = selected['embedding']
        _log.info(f"âœ… [å‘é‡åŒ¹é…] å·²é€‰æ‹©è®°å¿†å‘é‡ï¼Œç›¸ä¼¼åº¦={selected.get('score', 0.0):.4f}")
        return embedding_tensor, selected

    def _inject_memory_embedding(memory_embedding_tensor):
        """å°†è®°å¿†embeddingæ³¨å…¥æ¨¡å‹ï¼Œè¿”å›æ–°çš„outputs"""
        nonlocal model_kwargs, input_ids, memory_pad_token_id
        if memory_embedding_tensor is None:
            _log.warning("âš ï¸ [å‘é‡æ’å…¥] è®°å¿†å‘é‡ä¸ºNoneï¼Œæ— æ³•æ³¨å…¥")
            return None

        actual_device = next(model.parameters()).device
        memory_dtype = next(model.parameters()).dtype

        _log.info(f"ğŸ’‰ [å‘é‡æ’å…¥] å¼€å§‹æ³¨å…¥è®°å¿†å‘é‡ï¼Œshape: {memory_embedding_tensor.shape}, device: {actual_device}, dtype: {memory_dtype}")

        # åœ¨input_idsæœ«å°¾æ·»åŠ <|memory_pad|> token
        if memory_pad_token_id is not None:
            memory_pad_tensor = torch.tensor([[memory_pad_token_id]], dtype=input_ids.dtype, device=input_ids.device)
            input_ids = torch.cat([input_ids, memory_pad_tensor], dim=-1)
            _log.info(f"ğŸ’‰ [å‘é‡æ’å…¥] åœ¨input_idsä¸­æ’å…¥<|memory_pad|> token ID: {memory_pad_token_id}")
        else:
            _log.warning("âš ï¸ [å‘é‡æ’å…¥] <|memory_pad|> token IDä¸å­˜åœ¨ï¼Œæ— æ³•åœ¨input_idsä¸­æ ‡è®°è®°å¿†å‘é‡ä½ç½®")

        # æ›´æ–°attention_maskä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨
        if 'attention_mask' in model_kwargs and model_kwargs['attention_mask'] is not None:
            old_mask_len = model_kwargs['attention_mask'].shape[1]
            new_attention_mask = torch.ones(1, 1, device=actual_device, dtype=torch.long)
            model_kwargs['attention_mask'] = torch.cat(
                [model_kwargs['attention_mask'], new_attention_mask],
                dim=1
            )
            _log.info(f"ğŸ’‰ [å‘é‡æ’å…¥] æ›´æ–°attention_mask: {old_mask_len} -> {model_kwargs['attention_mask'].shape[1]}")

        # ä½¿ç”¨ç»Ÿä¸€çš„æ³¨å…¥æ–¹æ³•ï¼šå‡†å¤‡inputs_embedså¹¶æ³¨å…¥å‘é‡
        # ç”±äºè¿™æ˜¯æ¨ç†é˜¶æ®µï¼Œæˆ‘ä»¬éœ€è¦è·å–å½“å‰æ¨¡å‹è¾“å…¥å¯¹åº”çš„embeddings
        model_inputs = model.prepare_inputs_for_generation(input_ids, **model_kwargs)

        # è·å–token embeddingsï¼ˆåªå¯¹æ–°æ·»åŠ çš„tokenï¼‰
        embedding_layer = model.get_input_embeddings()
        # åªå¯¹æ–°æ·»åŠ çš„<|memory_pad|> tokenç”Ÿæˆembeddingï¼Œç„¶åæ›¿æ¢ä¸ºè®°å¿†å‘é‡
        memory_position = -1  # æ–°æ·»åŠ çš„tokenåœ¨æœ«å°¾

        # åˆ›å»ºå•tokençš„embeddingsç”¨äºæ³¨å…¥
        single_token_embed = embedding_layer(torch.tensor([[memory_pad_token_id]], device=actual_device))
                
        # ä½¿ç”¨ç»Ÿä¸€çš„æ³¨å…¥æ–¹æ³•æ›¿æ¢ä¸ºè®°å¿†å‘é‡ï¼ˆä¼ å…¥éªŒè¯å‚æ•°ï¼‰
        injected_embed = inject_memory_embedding_to_inputs_embeds(
            single_token_embed, 0, memory_embedding_tensor,
            input_ids=torch.tensor([[memory_pad_token_id]], device=actual_device),
            memory_pad_token_id=memory_pad_token_id
        )

        _log.info(f"ğŸ’‰ [å‘é‡æ’å…¥] ä½¿ç”¨ç»Ÿä¸€æ³¨å…¥æ–¹æ³•ï¼Œæ›¿æ¢ä½ç½®: {memory_position}")
                
        # ä½¿ç”¨æ³¨å…¥åçš„embeddingsè¿›è¡Œå‰å‘ä¼ æ’­
        with torch.no_grad():
            memory_outputs = _forward_with_last_hidden_state({
                "inputs_embeds": injected_embed,
                "attention_mask": torch.ones(1, 1, device=actual_device, dtype=torch.long),
                "past_key_values": model_kwargs.get('past_key_values'),
                "use_cache": True,
            })
                
        _log.info(f"ğŸ’‰ [å‘é‡æ’å…¥] è®°å¿†å‘é‡å‰å‘ä¼ æ’­å®Œæˆï¼Œoutputs.logits.shape: {memory_outputs.logits.shape}")

        _update_model_kwargs_helper(memory_outputs)
        _log.info(f"âœ… [å‘é‡æ’å…¥] è®°å¿†å‘é‡æ³¨å…¥æˆåŠŸï¼Œå·²æ›´æ–°model_kwargs")
        return memory_outputs
    
    # ç”Ÿæˆå¾ªç¯ï¼šå®Œå…¨æŒ‰ç…§transformerså®˜æ–¹å®ç°
    while cur_len < max_new_tokens:
        # æ£€æŸ¥ä¸­æ–­ä¿¡å·
        if interrupt_event and interrupt_event.is_set():
            break
                
        # ä½¿ç”¨å®˜æ–¹æ–¹æ³•å‡†å¤‡æ¨¡å‹è¾“å…¥
        # prepare_inputs_for_generationä¼šè‡ªåŠ¨å¤„ç†ï¼š
        # - KV cacheæ—¶çš„input_idsè£å‰ªï¼ˆåªä¼ å…¥æœªç¼“å­˜çš„tokenï¼‰
        # - attention_maskçš„æ­£ç¡®é•¿åº¦å’Œæ ¼å¼
        # - position_idsçš„å¤„ç†
        # - cache_positionçš„å¤„ç†
        # - å…¶ä»–model_kwargsçš„ä¼ é€’
        model_inputs = model.prepare_inputs_for_generation(
            input_ids,
            **model_kwargs
        )
                
        # ğŸ”„ ç»Ÿä¸€è®°å¿†è§¦å‘æœºåˆ¶ï¼šæ£€æµ‹åˆ°æœ€æ–°è¾“å…¥æ˜¯<recall> tokenæ—¶è§¦å‘å›å¿†
        # æ£€æŸ¥å½“å‰è¦å¤„ç†çš„æœ€åä¸€ä¸ªtokenæ˜¯å¦æ˜¯<recall> token
        current_input_ids = model_inputs.get('input_ids', input_ids)
        if current_input_ids.shape[-1] > 0:
            last_token_id = current_input_ids[0, -1].item()
            if (
                autorecall_enabled
                and recall_token_id is not None
                and last_token_id == recall_token_id
                and not recall_pending  # é¿å…é‡å¤è§¦å‘
            ):
                if memory_db is None or len(memory_db) == 0:
                    _log.info("â„¹ï¸ [è¾“å…¥æ£€æµ‹] è®°å¿†å‘é‡åº“ä¸ºç©ºï¼Œ<recall> tokenæŒ‰æ™®é€štokenå¤„ç†")
                else:
                    _log.info(f"ğŸ¯ [è¾“å…¥æ£€æµ‹] æ£€æµ‹åˆ°æœ€æ–°è¾“å…¥æ˜¯<recall> token (ID: {recall_token_id})ï¼Œè§¦å‘å›å¿†æœºåˆ¶")
                    recall_pending = True
                
        # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨backboneæå–<recall>å‘é‡ï¼‰
        forward_inputs = dict(model_inputs)
        forward_inputs.setdefault("use_cache", model_kwargs.get("use_cache", True))
        outputs = _forward_with_last_hidden_state(forward_inputs)
        last_hidden_state = outputs.last_hidden_state

        if autorecall_enabled and recall_pending:
            recall_pending = False
            _log.info("ğŸ”„ [å›å¿†è§¦å‘] æ£€æµ‹åˆ°recall_pending=Trueï¼Œå¼€å§‹å¤„ç†å›å¿†æœºåˆ¶")
            if last_hidden_state is None:
                _log.warning("âš ï¸ [å›å¿†è§¦å‘] æ— æ³•è·å–<recall>éšè—å‘é‡ï¼Œç»§ç»­æ™®é€šç”Ÿæˆ")
            elif memory_db is None or len(memory_db) == 0:
                _log.info("â„¹ï¸ [å›å¿†è§¦å‘] è®°å¿†å‘é‡åº“ä¸ºç©ºï¼Œ<recall> æŒ‰æ™®é€štokenå¤„ç†")
            else:
                query_vector = last_hidden_state[0, -1, :]
                _log.info(f"ğŸ” [å›å¿†è§¦å‘] æå–<recall> tokençš„hidden stateä½œä¸ºæŸ¥è¯¢å‘é‡ï¼Œshape: {query_vector.shape}")
                memory_embedding, selected_meta = _sample_memory_embedding_from_db(query_vector)
                if memory_embedding is None:
                    _log.info("â„¹ï¸ [å›å¿†è§¦å‘] æœªæ‰¾åˆ°å¯ç”¨è®°å¿†ï¼Œ<recall> æŒ‰æ™®é€štokenå¤„ç†")
                else:
                    memory_score = selected_meta.get("score") if selected_meta else None
                    if memory_score is not None:
                        _log.info(f"ğŸ¯ [å›å¿†è§¦å‘] é‡‡æ ·åˆ°è®°å¿†å‘é‡ï¼Œç›¸ä¼¼åº¦={memory_score:.4f}")
                    memory_outputs = _inject_memory_embedding(memory_embedding)
                    if memory_outputs is None:
                        _log.warning("âš ï¸ [å›å¿†è§¦å‘] è®°å¿†å‘é‡æ³¨å…¥å¤±è´¥ï¼Œç»§ç»­æ™®é€šç”Ÿæˆ")
                    else:
                        outputs = memory_outputs
                        last_hidden_state = outputs.last_hidden_state
                        # è®°å½•è®°å¿†å‘é‡æ’å…¥ä½ç½®ï¼ˆç”¨äºè¿”å›ï¼Œä½†ä¸å†ç”¨äºæ‰“å°æ ‡æ³¨ï¼Œå› ä¸º<|memory_pad|>ä¼šåŸç”Ÿæ˜¾ç¤ºï¼‰
                        memory_score = selected_meta.get("score", 0.0) if selected_meta else 0.0
                        injection_pos = input_ids.shape[-1]  # è®°å¿†å‘é‡æ’å…¥åœ¨å½“å‰ä½ç½®ä¹‹å
                        memory_injection_positions.append((injection_pos, memory_score))
                        _log.info(f"âœ… [å›å¿†è§¦å‘] è®°å¿†å‘é‡æ³¨å…¥æˆåŠŸï¼Œç›¸ä¼¼åº¦={memory_score:.4f}")

        # è·å–logitsï¼ˆåªå–æœ€åä¸€ä¸ªä½ç½®çš„logitsï¼‰
        next_token_logits = outputs.logits[:, -1, :]
        
        # åº”ç”¨LogitsProcessorï¼ˆå¦‚repetition_penaltyï¼‰
        # æ³¨æ„ï¼šLogitsProcessoræ¥æ”¶(input_ids, scores)ä½œä¸ºå‚æ•°
        next_token_scores = logits_processor(input_ids, next_token_logits)

        # åº”ç”¨LogitsWarperï¼ˆå¦‚temperature, top_k, top_pï¼‰
        if do_sample and logits_warper is not None:
            next_token_scores = logits_warper(input_ids, next_token_scores)
                
        # é‡‡æ ·ä¸‹ä¸€ä¸ªtokenï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹å®ç°ï¼‰
        if do_sample:
            # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            probs = torch.nn.functional.softmax(next_token_scores, dim=-1)
            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
        else:
            # è´ªå©ªè§£ç 
            next_tokens = torch.argmax(next_token_scores, dim=-1)
                
        # æ³¨æ„ï¼šè®°å¿†è§¦å‘æœºåˆ¶å·²ç»Ÿä¸€ä¸º"æ£€æµ‹åˆ°æœ€æ–°è¾“å…¥æ˜¯<recall> tokenæ—¶è§¦å‘"
        # å› æ­¤ä¸å†éœ€è¦åœ¨è¿™é‡Œæ£€æµ‹ç”Ÿæˆçš„<recall> token
        # ç”Ÿæˆçš„<recall> tokenä¼šåœ¨ä¸‹ä¸€è½®å¾ªç¯çš„å‰å‘ä¼ æ’­å‰è¢«æ£€æµ‹åˆ°
        # æ³¨æ„ï¼šæ’å…¥å®Œè®°å¿†å‘é‡åå·²ç«‹å³é€€å‡ºå›å¿†æ¨¡å¼ï¼Œä¸éœ€è¦æ£€æµ‹</recall> token
        
        # å¤„ç†EOS tokenï¼šå®Œå…¨æŒ‰ç…§transformerså®˜æ–¹å®ç°
        # å¦‚æœç”Ÿæˆå®Œæˆäº†ï¼Œå°±å°†æ–°ç”Ÿæˆçš„tokenæ›¿æ¢æˆpad_token_id
        if has_eos_stopping_criteria:
            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)
        
        # æ›´æ–°generated idså’Œmodel inputsï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹å®ç°ï¼‰
        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)
        
        # ä½¿ç”¨å®˜æ–¹æ–¹æ³•æ›´æ–°model_kwargsï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹å®ç°ï¼‰
        # _update_model_kwargs_for_generationä¼šè‡ªåŠ¨å¤„ç†ï¼š
        # - past_key_valuesçš„æ›´æ–°
        # - attention_maskçš„æ›´æ–°
        # - cache_positionçš„æ›´æ–°
        # - ç§»é™¤åªåœ¨é¦–æ¬¡å‰å‘ä¼ æ’­æ—¶éœ€è¦çš„å­—æ®µï¼ˆå¦‚pixel_valuesï¼‰
        try:
            model_kwargs = model._update_model_kwargs_for_generation(
                outputs,
                model_kwargs,
                is_encoder_decoder=False,
                standardize_cache_format=True,
            )
        except TypeError:
            # å¦‚æœå‚æ•°ä¸æ”¯æŒï¼Œå°è¯•ä¸å¸¦standardize_cache_format
            try:
                model_kwargs = model._update_model_kwargs_for_generation(
                    outputs,
                    model_kwargs,
                    is_encoder_decoder=False,
                )
            except TypeError:
                # å¦‚æœè¿˜æ˜¯ä¸æ”¯æŒï¼Œå°è¯•åªä¼ å¿…éœ€å‚æ•°
                model_kwargs = model._update_model_kwargs_for_generation(
                    outputs,
                    model_kwargs,
                )
        
        # æ›´æ–°æœªå®Œæˆåºåˆ—æ ‡è®°ï¼ˆå®Œå…¨æŒ‰ç…§transformerså®˜æ–¹å®ç°ï¼‰
        if eos_token_ids is not None:
            # æ£€æŸ¥æ–°ç”Ÿæˆçš„tokenæ˜¯å¦æ˜¯EOS token
            # ä½¿ç”¨å¹¿æ’­æ“ä½œï¼Œæ£€æŸ¥æ¯ä¸ªnext_tokenæ˜¯å¦ç­‰äºä»»ä½•ä¸€ä¸ªeos_token_id
            eos_in_sentence = (next_tokens.unsqueeze(-1) == eos_token_ids.unsqueeze(0)).any(dim=-1)
            unfinished_sequences = unfinished_sequences & ~eos_in_sentence
        
        cur_len += 1
        
        # æ£€æŸ¥StoppingCriteriaï¼ˆåœ¨æ¯ä¸ªtokenç”Ÿæˆåæ£€æŸ¥ï¼‰
        # è¿™æ˜¯transformersæ ‡å‡†å®ç°çš„å…³é”®éƒ¨åˆ†
        # æ³¨æ„ï¼šå®˜æ–¹ä½¿ç”¨ & æ“ä½œç¬¦ï¼Œä¸æ˜¯ mul
        # stopping_criteriaè¿”å›boolæˆ–tensorï¼Œè¡¨ç¤ºæ˜¯å¦åº”è¯¥åœæ­¢
        should_stop = stopping_criteria(input_ids, next_token_scores)
        
        # å¦‚æœstopping_criteriaè¿”å›å•ä¸ªboolå€¼ï¼Œéœ€è¦è½¬æ¢ä¸ºtensor
        if isinstance(should_stop, bool):
            # å¯¹äºå•ä¸ªboolå€¼ï¼Œè½¬æ¢ä¸ºtensorï¼ˆä¸batch_sizeåŒ¹é…ï¼‰
            should_stop_tensor = torch.tensor([should_stop], device=unfinished_sequences.device, dtype=torch.bool)
            # å¦‚æœbatch_size > 1ï¼Œéœ€è¦æ‰©å±•åˆ°æ‰€æœ‰åºåˆ—
            if batch_size > 1:
                should_stop_tensor = should_stop_tensor.expand(batch_size)
        else:
            # å¦‚æœå·²ç»æ˜¯tensorï¼Œç›´æ¥ä½¿ç”¨
            should_stop_tensor = should_stop.bool() if should_stop.dtype != torch.bool else should_stop
        
        # æ›´æ–°unfinished_sequencesï¼šå¦‚æœshould_stopä¸ºTrueï¼Œåˆ™æ ‡è®°ä¸ºå·²å®Œæˆ
        unfinished_sequences = unfinished_sequences & ~should_stop_tensor
        
        # å¦‚æœæ‰€æœ‰åºåˆ—éƒ½å®Œæˆäº†ï¼Œæå‰åœæ­¢
        if unfinished_sequences.max() == 0:
            # è®°å½•åœæ­¢åŸå› ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if interrupt_event and interrupt_event.is_set():
                _log.info("âš ï¸ ç”Ÿæˆå› ä¸­æ–­è€Œåœæ­¢")
            else:
                # StoppingCriteriaåœæ­¢æ˜¯æ­£å¸¸æƒ…å†µï¼ˆå¦‚è¾¾åˆ°æœ€å¤§é•¿åº¦ã€é‡åˆ°åœæ­¢è¯ç­‰ï¼‰ï¼Œä½¿ç”¨debugçº§åˆ«
                _log.debug("ç”Ÿæˆå› StoppingCriteriaè€Œåœæ­¢ï¼ˆæ­£å¸¸åœæ­¢ï¼Œå¦‚è¾¾åˆ°æœ€å¤§é•¿åº¦æˆ–é‡åˆ°åœæ­¢è¯ï¼‰")
            break
    
        # æ—©åœï¼šæ£€æµ‹åˆ°<tool_call>é—­åˆæ ‡ç­¾å³åœæ­¢ï¼ˆé¦–è½®å³å¯è§¦å‘ï¼‰
        if early_stop_on_tool_call:
            try:
                # è§£ç å½“å‰å…¨éƒ¨ï¼ˆåŒ…å«ç‰¹æ®Štokenï¼‰ï¼ŒæŸ¥æ‰¾å·¥å…·è°ƒç”¨é—­åˆ
                decoded_so_far = processor.batch_decode(input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]
                open_idx = decoded_so_far.rfind("<tool_call")
                if open_idx != -1:
                    close_idx = decoded_so_far.rfind("</tool_call>")
                    if close_idx != -1 and close_idx > open_idx:
                        _log.info("ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨é—­åˆæ ‡ç­¾ï¼Œæå‰ç»“æŸé¦–è½®ç”Ÿæˆ")
                        break
            except Exception:
                pass
    
    # è¿”å›ç”Ÿæˆç»“æœå’Œè®°å¿†æ’å…¥ä½ç½®ä¿¡æ¯
    # ä¸ºäº†ä¿æŒå‘åå…¼å®¹ï¼Œå¦‚æœmemory_injection_positionsä¸ºç©ºï¼Œåªè¿”å›input_ids
    # å¦åˆ™è¿”å›å…ƒç»„ (input_ids, memory_injection_positions)
    if memory_injection_positions:
        return input_ids, memory_injection_positions
    else:
        return input_ids




def truncate_history_by_tokens(chat_history: List[Dict[str, Any]], system_prompt: str, 
                                 chat_type: str, chat_id: str, 
                                 max_tokens: int = 35000,
                                 interrupt_event: threading.Event = None) -> List[Dict[str, Any]]:
    """
    æ ¹æ®tokenæ•°é‡æˆªæ–­èŠå¤©å†å²è®°å½•
    
    Args:
        chat_history: èŠå¤©å†å²è®°å½•ï¼ˆåŸå§‹åˆ—è¡¨ï¼Œä¼šè¢«ä¿®æ”¹ï¼‰
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        chat_type: "group" æˆ– "private"
        chat_id: ç¾¤IDæˆ–ç”¨æˆ·ID
        max_tokens: æœ€å¤§tokenæ•°é‡ï¼ˆé»˜è®¤35000ï¼‰
        interrupt_event: ä¸­æ–­äº‹ä»¶ï¼ˆå¦‚æœè¢«è®¾ç½®ï¼Œåˆ™ç«‹å³è¿”å›ï¼‰
    
    Returns:
        æˆªæ–­åçš„èŠå¤©å†å²è®°å½•ï¼ˆå¦‚æœè¢«ä¸­æ–­ï¼Œè¿”å›åŸå§‹å†å²ï¼‰
    """
    global model, processor
    
    # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿chat_historyä¸ä¸ºNone
    if chat_history is None:
        _log.error(f"âŒ chat_historyä¸ºNoneï¼Œæ— æ³•è¿›è¡Œæˆªæ–­ï¼ˆ{chat_type} {chat_id}ï¼‰")
        return []
    
    # åœ¨å¼€å§‹å‰æ£€æŸ¥ä¸­æ–­
    if interrupt_event and interrupt_event.is_set():
        _log.info(f"âš ï¸ æˆªæ–­å†å²æ¶ˆæ¯åœ¨å¼€å§‹å‰è¢«ä¸­æ–­ï¼ˆ{chat_type} {chat_id}ï¼‰")
        return chat_history
    
    if model is None or processor is None:
        _log.warning("âš ï¸ æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ£€æŸ¥tokené•¿åº¦ï¼Œè·³è¿‡æˆªæ–­")
        return chat_history
    
    # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ç”¨äºæ£€æŸ¥
    full_messages = []
    if system_prompt:
        full_messages.append({
            "role": "system",
            "content": [{"type": "text", "text": system_prompt}]
        })
    full_messages.extend(chat_history)
    
    # ç¬¬ä¸€æ¬¡tokenizeæ£€æŸ¥é•¿åº¦
    try:
        # åœ¨apply_chat_templateå‰æ£€æŸ¥ä¸­æ–­ï¼ˆå¤„ç†å›¾ç‰‡å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼‰
        if interrupt_event and interrupt_event.is_set():
            _log.info(f"âš ï¸ æˆªæ–­å†å²æ¶ˆæ¯åœ¨ç¬¬ä¸€æ¬¡tokenizeå‰è¢«ä¸­æ–­ï¼ˆ{chat_type} {chat_id}ï¼‰")
            return chat_history
        
        inputs = processor.apply_chat_template(
            full_messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
            max_length=None,  # ä¸é™åˆ¶é•¿åº¦
            truncation=False,  # ä¸æˆªæ–­
            padding=False
        )
    except Exception as e:
        # å¤„ç†å›¾ç‰‡ç›¸å…³é”™è¯¯çš„æƒ…å†µ - å¯¹äºæˆªæ–­æ£€æŸ¥ï¼Œæˆ‘ä»¬è·³è¿‡å›¾ç‰‡å¤„ç†
        error_msg = str(e)
        error_type = type(e).__name__

        image_errors = [
            "multimedia.nt.qq.com.cn", "Failed to resolve", "NameResolutionError",
            "UnidentifiedImageError", "cannot identify image file",
            "HTTPConnectionPool", "ConnectionError", "Timeout"
        ]

        is_image_error = any(img_err in error_msg for img_err in image_errors) or error_type in [
            "UnidentifiedImageError", "ConnectionError", "Timeout"
        ]

        video_errors = [
            "PyAV is not installed", "torchvision.io.video", "read_video", "video_utils.py",
            "Using `torchvision` for video decoding is deprecated"
        ]
        is_video_error = any(ve in error_msg for ve in video_errors)

        if is_image_error or is_video_error:
            _log.warning(f"âš ï¸ å¤šåª’ä½“å¤„ç†å¤±è´¥ï¼ˆæˆªæ–­æ£€æŸ¥ï¼‰ï¼Œå°è¯•ç§»é™¤å›¾ç‰‡/è§†é¢‘åé‡è¯• (é”™è¯¯ç±»å‹: {error_type}): {error_msg}")
            # ç§»é™¤å›¾ç‰‡/è§†é¢‘é¡¹ï¼Œä»…ç”¨äºé•¿åº¦æ£€æŸ¥
            cleaned_messages = []
            for msg in full_messages:
                if isinstance(msg.get("content"), list):
                    cleaned_content = []
                    for item in msg["content"]:
                        if item.get("type") == "text":
                            cleaned_content.append(item)
                        # å¿½ç•¥ image / video
                    if cleaned_content:
                        cleaned_messages.append({"role": msg["role"], "content": cleaned_content})
                else:
                    cleaned_messages.append(msg)
            try:
                inputs = processor.apply_chat_template(
                    cleaned_messages,
                    tokenize=True,
                    add_generation_prompt=True,
                    return_dict=True,
                    return_tensors="pt",
                    max_length=None,
                    truncation=False,
                    padding=False
                )
            except Exception as e2:
                _log.warning(f"âš ï¸ å³ä½¿ç§»é™¤å¤šåª’ä½“ä¹Ÿæ— æ³•è¿›è¡Œæˆªæ–­æ£€æŸ¥ï¼Œé€€å›åŸå§‹å†å²: {e2}")
                return chat_history
        else:
            # å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨æ–¹å¤„ç†
            _log.error(f"âŒ æˆªæ–­å†å²æ—¶å‘ç”Ÿéå¤šåª’ä½“é”™è¯¯ï¼ˆ{chat_type} {chat_id}ï¼‰: {error_type}: {error_msg}", exc_info=True)
            raise e
    
    # åœ¨apply_chat_templateåæ£€æŸ¥ä¸­æ–­ï¼ˆåªæœ‰åœ¨æˆåŠŸtokenizeåæ‰æ‰§è¡Œåˆ°è¿™é‡Œï¼‰
    if interrupt_event and interrupt_event.is_set():
        _log.info(f"âš ï¸ æˆªæ–­å†å²æ¶ˆæ¯åœ¨ç¬¬ä¸€æ¬¡tokenizeåè¢«ä¸­æ–­ï¼ˆ{chat_type} {chat_id}ï¼‰")
        return chat_history
    
    # æ£€æŸ¥inputsæ˜¯å¦å·²å®šä¹‰ï¼ˆåœ¨å¼‚å¸¸å¤„ç†åå¯èƒ½æœªå®šä¹‰ï¼‰
    if 'inputs' not in locals() or inputs is None:
        _log.warning("âš ï¸ inputsæœªå®šä¹‰ï¼Œè·³è¿‡æˆªæ–­")
        return chat_history
    
    if 'input_ids' not in inputs or not isinstance(inputs['input_ids'], torch.Tensor):
        _log.warning("âš ï¸ æ— æ³•è·å–input_idsï¼Œè·³è¿‡æˆªæ–­")
        return chat_history
    
    input_length = inputs['input_ids'].shape[-1]
    _log.info(f"ğŸ“Š æ£€æŸ¥è¾“å…¥tokené•¿åº¦: {input_length}, æœ€å¤§é™åˆ¶: {max_tokens}")
    
    if input_length <= max_tokens:
        _log.info(f"âœ… è¾“å…¥tokené•¿åº¦åœ¨é™åˆ¶å†…ï¼Œæ— éœ€æˆªæ–­")
        return chat_history
    
    _log.warning(f"âš ï¸ è¾“å…¥tokené•¿åº¦ ({input_length}) è¶…è¿‡æœ€å¤§é™åˆ¶ ({max_tokens})ï¼Œå¼€å§‹æˆªæ–­å†å²æ¶ˆæ¯...")
    
    # é€æ¡åˆ é™¤æœ€æ—©çš„æ¶ˆæ¯ï¼Œç›´åˆ°é•¿åº¦åœ¨é™åˆ¶å†…
    removed_messages = []  # ç”¨äºä¿å­˜è¢«åˆ é™¤çš„æ¶ˆæ¯
    iteration = 0
    max_iterations = 5  # æœ€å¤šè¿­ä»£5æ¬¡
    
    while input_length > max_tokens and len(chat_history) > 0 and iteration < max_iterations:
        # åœ¨æ¯æ¬¡è¿­ä»£å‰æ£€æŸ¥ä¸­æ–­ï¼ˆé‡è¦ï¼šåœ¨åˆ é™¤æ¶ˆæ¯å‰æ£€æŸ¥ï¼Œé¿å…ä¸å¿…è¦çš„ä¿®æ”¹ï¼‰
        if interrupt_event and interrupt_event.is_set():
            _log.info(f"âš ï¸ æˆªæ–­å†å²æ¶ˆæ¯åœ¨è¿­ä»£ {iteration} ä¸­è¢«ä¸­æ–­ï¼ˆ{chat_type} {chat_id}ï¼‰ï¼Œæ¢å¤è¢«åˆ é™¤çš„æ¶ˆæ¯")
            # æ¢å¤è¢«åˆ é™¤çš„æ¶ˆæ¯
            chat_history[:0] = removed_messages
            return chat_history
        
        iteration += 1
        
        # åˆ é™¤æœ€æ—©çš„ä¸€æ¡æ¶ˆæ¯
        removed_msg = chat_history.pop(0)
        removed_messages.append(removed_msg)
        
        # é‡æ–°æ„å»ºæ¶ˆæ¯å¹¶æ£€æŸ¥é•¿åº¦
        test_messages = []
        if system_prompt:
            test_messages.append({
                "role": "system",
                "content": [{"type": "text", "text": system_prompt}]
            })
        test_messages.extend(chat_history)
        
        try:
            # åœ¨é‡æ–°tokenizeå‰æ£€æŸ¥ä¸­æ–­
            if interrupt_event and interrupt_event.is_set():
                _log.info(f"âš ï¸ æˆªæ–­å†å²æ¶ˆæ¯åœ¨é‡æ–°tokenizeå‰è¢«ä¸­æ–­ï¼ˆ{chat_type} {chat_id}ï¼‰ï¼Œæ¢å¤è¢«åˆ é™¤çš„æ¶ˆæ¯")
                chat_history[:0] = removed_messages
                return chat_history
            
            test_inputs = processor.apply_chat_template(
                test_messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt",
                max_length=None,
                truncation=False,
                padding=False
            )
            
            # åœ¨é‡æ–°tokenizeåæ£€æŸ¥ä¸­æ–­
            if interrupt_event and interrupt_event.is_set():
                _log.info(f"âš ï¸ æˆªæ–­å†å²æ¶ˆæ¯åœ¨é‡æ–°tokenizeåè¢«ä¸­æ–­ï¼ˆ{chat_type} {chat_id}ï¼‰ï¼Œæ¢å¤è¢«åˆ é™¤çš„æ¶ˆæ¯")
                chat_history[:0] = removed_messages
                return chat_history
            
            input_length = test_inputs['input_ids'].shape[-1]
            _log.info(f"ğŸ“Š åˆ é™¤ {iteration} æ¡æ¶ˆæ¯åï¼Œè¾“å…¥tokené•¿åº¦: {input_length}")
            
            if input_length <= max_tokens:
                # é•¿åº¦åœ¨é™åˆ¶å†…ï¼Œä¿å­˜è¢«åˆ é™¤çš„æ¶ˆæ¯å¹¶è¿”å›
                if removed_messages:
                    save_chat_history_to_storage(chat_type, chat_id, removed_messages)
                    _log.info(f"âœ… å·²æˆªæ–­å†å²æ¶ˆæ¯: åˆ é™¤ {len(removed_messages)} æ¡ï¼Œå½“å‰é•¿åº¦: {input_length}")
                return chat_history
                
        except Exception as e:
            _log.error(f"âŒ æˆªæ–­å†å²æ¶ˆæ¯æ—¶é‡æ–°tokenizeå¤±è´¥: {e}", exc_info=True)
            # å¦‚æœå‡ºé”™ï¼Œæ¢å¤è¢«åˆ é™¤çš„æ¶ˆæ¯
            chat_history[:0] = removed_messages
            return chat_history
    
    # å¦‚æœè¶…è¿‡5æ¬¡è¿­ä»£è¿˜æ²¡æœ‰è¾¾åˆ°è¦æ±‚ï¼Œæ¸…ç©ºä¸€åŠçš„èŠå¤©è®°å½•
    if iteration >= max_iterations and input_length > max_tokens:
        # åœ¨æ¸…ç©ºä¸€åŠå‰æ£€æŸ¥ä¸­æ–­
        if interrupt_event and interrupt_event.is_set():
            _log.info(f"âš ï¸ æˆªæ–­å†å²æ¶ˆæ¯åœ¨æ¸…ç©ºä¸€åŠå‰è¢«ä¸­æ–­ï¼ˆ{chat_type} {chat_id}ï¼‰ï¼Œæ¢å¤è¢«åˆ é™¤çš„æ¶ˆæ¯")
            chat_history[:0] = removed_messages
            return chat_history
        
        _log.warning(f"âš ï¸ è¶…è¿‡ {max_iterations} æ¬¡è¿­ä»£ä»æœªè¾¾åˆ°è¦æ±‚ï¼Œæ¸…ç©ºä¸€åŠçš„èŠå¤©è®°å½•...")
        
        # ä¿å­˜å°†è¢«æ¸…ç©ºçš„æ¶ˆæ¯
        half_count = len(chat_history) // 2
        if half_count > 0:
            removed_messages.extend(chat_history[:half_count])
            chat_history[:] = chat_history[half_count:]
        
        # é‡æ–°æ£€æŸ¥é•¿åº¦
        test_messages = []
        if system_prompt:
            test_messages.append({
                "role": "system",
                "content": [{"type": "text", "text": system_prompt}]
            })
        test_messages.extend(chat_history)
        
        try:
            # åœ¨æ¸…ç©ºä¸€åŠåtokenizeå‰æ£€æŸ¥ä¸­æ–­
            if interrupt_event and interrupt_event.is_set():
                _log.info(f"âš ï¸ æˆªæ–­å†å²æ¶ˆæ¯åœ¨æ¸…ç©ºä¸€åŠåtokenizeå‰è¢«ä¸­æ–­ï¼ˆ{chat_type} {chat_id}ï¼‰ï¼Œæ¢å¤è¢«åˆ é™¤çš„æ¶ˆæ¯")
                chat_history[:0] = removed_messages
                return chat_history
            
            test_inputs = processor.apply_chat_template(
                test_messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt",
                max_length=None,
                truncation=False,
                padding=False
            )
            
            # åœ¨æ¸…ç©ºä¸€åŠåtokenizeåæ£€æŸ¥ä¸­æ–­
            if interrupt_event and interrupt_event.is_set():
                _log.info(f"âš ï¸ æˆªæ–­å†å²æ¶ˆæ¯åœ¨æ¸…ç©ºä¸€åŠåtokenizeåè¢«ä¸­æ–­ï¼ˆ{chat_type} {chat_id}ï¼‰ï¼Œæ¢å¤è¢«åˆ é™¤çš„æ¶ˆæ¯")
                chat_history[:0] = removed_messages
                return chat_history
            
            input_length = test_inputs['input_ids'].shape[-1]
            _log.info(f"ğŸ“Š æ¸…ç©ºä¸€åŠåï¼Œè¾“å…¥tokené•¿åº¦: {input_length}")
            
            if input_length <= max_tokens:
                # é•¿åº¦åœ¨é™åˆ¶å†…ï¼Œä¿å­˜è¢«åˆ é™¤çš„æ¶ˆæ¯å¹¶è¿”å›
                if removed_messages:
                    save_chat_history_to_storage(chat_type, chat_id, removed_messages)
                    _log.info(f"âœ… å·²æ¸…ç©ºä¸€åŠå†å²æ¶ˆæ¯: åˆ é™¤ {len(removed_messages)} æ¡ï¼Œå½“å‰é•¿åº¦: {input_length}")
                return chat_history
            else:
                # æ¸…ç©ºä¸€åŠåä»ç„¶è¶…è¿‡é™åˆ¶ï¼Œæ¸…ç©ºå…¨éƒ¨èŠå¤©è®°å½•
                _log.error(f"âŒ æ¸…ç©ºä¸€åŠåä»ç„¶è¶…è¿‡é™åˆ¶ ({input_length} > {max_tokens})ï¼Œæ¸…ç©ºå…¨éƒ¨èŠå¤©è®°å½•")
                removed_messages.extend(chat_history)
                chat_history.clear()
                
                # ä¿å­˜æ‰€æœ‰è¢«åˆ é™¤çš„æ¶ˆæ¯
                if removed_messages:
                    save_chat_history_to_storage(chat_type, chat_id, removed_messages)
                    _log.warning(f"âš ï¸ å·²æ¸…ç©ºå…¨éƒ¨å†å²æ¶ˆæ¯: åˆ é™¤ {len(removed_messages)} æ¡")
                return chat_history
                
        except Exception as e:
            _log.error(f"âŒ æ¸…ç©ºä¸€åŠåé‡æ–°tokenizeå¤±è´¥: {e}", exc_info=True)
            # å¦‚æœå‡ºé”™ï¼Œæ¢å¤è¢«åˆ é™¤çš„æ¶ˆæ¯
            chat_history[:0] = removed_messages
            return chat_history
    
    # ä¿å­˜è¢«åˆ é™¤çš„æ¶ˆæ¯
    if removed_messages:
        save_chat_history_to_storage(chat_type, chat_id, removed_messages)
        _log.info(f"âœ… å·²æˆªæ–­å†å²æ¶ˆæ¯: åˆ é™¤ {len(removed_messages)} æ¡ï¼Œå½“å‰é•¿åº¦: {input_length}")
    
    # ç¡®ä¿æ€»æ˜¯è¿”å›chat_historyï¼ˆé˜²å¾¡æ€§æ£€æŸ¥ï¼‰
    if chat_history is None:
        _log.error(f"âŒ chat_historyæ„å¤–å˜ä¸ºNoneï¼ˆ{chat_type} {chat_id}ï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []
    
    return chat_history
        


def generate_reply(chat_history: List[Dict[str, Any]], max_new_tokens: int = None, 
                   temperature: float = None, chat_type: str = None, 
                   chat_context: Dict[str, str] = None, 
                   interrupt_event: threading.Event = None,
                   chat_id: str = None, response_dict: dict = None,
                   log_full_io: bool = True) -> Tuple[Optional[str], bool, bool]:
    """
    ä½¿ç”¨Qwen3-VLæ¨¡å‹ç”Ÿæˆå›å¤
    
    Args:
        chat_history: èŠå¤©å†å²ï¼Œæ ¼å¼ï¼š[{"role": "user", "content": [...]}, ...]
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰
        chat_type: "group" æˆ– "private"ï¼Œè¡¨ç¤ºå¯¹è¯ç±»å‹
        chat_context: å¯¹è¯ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…å«ç¾¤åç§°æˆ–ç”¨æˆ·æ˜µç§°ç­‰
    
    Returns:
        (å›å¤æ–‡æœ¬, æ˜¯å¦éœ€è¦å›å¤, æ˜¯å¦è¢«ä¸­æ–­)
        - å¦‚æœè¢«ä¸­æ–­ï¼Œè¿”å›(None, False, True)
        - å¦‚æœæ¨¡å‹åˆ¤æ–­ä¸éœ€è¦å›å¤ï¼Œè¿”å›("", False, False)
        - å¦‚æœéœ€è¦å›å¤ï¼Œè¿”å›(å›å¤æ–‡æœ¬, True, False)
    """
    global model, processor, device, config, is_training, training_lock
    
    # æ£€æŸ¥æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
    with training_lock:
        if is_training:
            _log.warning("âš ï¸ å½“å‰å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œæ‹’ç»ç”Ÿæˆå›å¤")
            raise RuntimeError("æœåŠ¡å™¨æ­£åœ¨è®­ç»ƒä¸­ï¼Œæš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤")
    
    if model is None or processor is None:
        raise RuntimeError("æ¨¡å‹æœªåˆå§‹åŒ–")
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–ç”Ÿæˆå‚æ•°ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    gen_config = config.get("generation", {})
    if max_new_tokens is None:
        max_new_tokens = gen_config.get("max_new_tokens", 1000)
    if temperature is None:
        temperature = gen_config.get("temperature", 1.0)  # å®˜æ–¹é»˜è®¤1.0
    do_sample = gen_config.get("do_sample", True)
    top_p = gen_config.get("top_p", 0.95)  # å®˜æ–¹é»˜è®¤0.95
    top_k = gen_config.get("top_k", 20)  # å®˜æ–¹é»˜è®¤20
    repetition_penalty = gen_config.get("repetition_penalty", 1.0)  # å®˜æ–¹é»˜è®¤1.0
    presence_penalty = gen_config.get("presence_penalty", 0.0)  # å®˜æ–¹é»˜è®¤0.0
    
    try:
        # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«å¯¹è¯ä¸Šä¸‹æ–‡ï¼‰
        system_prompt = build_system_prompt(chat_type, chat_context)
        
        # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼šç³»ç»Ÿæç¤ºè¯ + èŠå¤©è®°å½•
        full_messages = []
        if system_prompt:
            full_messages.append({
                "role": "system",
                "content": [{"type": "text", "text": system_prompt}]
            })
        
        # æ·»åŠ èŠå¤©è®°å½•
        full_messages.extend(chat_history)
        
        # å‡†å¤‡æ¨ç†è¾“å…¥
        _log.info(f"å‡†å¤‡æ¨ç†è¾“å…¥ï¼Œç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}, å†å²æ¶ˆæ¯æ•°: {len(chat_history)}")
        
        # ä½¿ç”¨processor.apply_chat_templateå¤„ç†æ¶ˆæ¯
        # æ³¨æ„ï¼šå¤„ç†å›¾ç‰‡æ—¶å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œéœ€è¦åœ¨æ­¤å‰åæ£€æŸ¥ä¸­æ–­
        if interrupt_event and interrupt_event.is_set():
            if chat_id and response_dict:
                with queue_lock:
                    current_processing = processing_chats.get(chat_id)
                    if current_processing and current_processing["response_dict"] is not response_dict:
                        _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨apply_chat_templateå‰å·²è¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œé€€å‡ºç”Ÿæˆ")
                        return None, False, True
        
        try:
            inputs = processor.apply_chat_template(
                full_messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt",
                max_length=None,  # ä¸é™åˆ¶é•¿åº¦
                truncation=False,  # ä¸æˆªæ–­
                padding=False
            )
            _log.debug(
                f"âœ… ç¬¬ä¸€æ¬¡apply_chat_templateæˆåŠŸï¼Œinputsç±»å‹: {type(inputs)}ï¼Œkeys: {inputs.keys() if isinstance(inputs, dict) else 'éå­—å…¸'}"
            )
        except Exception as e:
            # å¤„ç†å›¾ç‰‡ç›¸å…³é”™è¯¯çš„æƒ…å†µ
            error_msg = str(e)
            error_type = type(e).__name__

            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡å¤„ç†ç›¸å…³çš„é”™è¯¯
            image_errors = [
                "multimedia.nt.qq.com.cn", "Failed to resolve", "NameResolutionError",
                "UnidentifiedImageError", "cannot identify image file",
                "HTTPConnectionPool", "ConnectionError", "Timeout"
            ]

            is_image_error = any(img_err in error_msg for img_err in image_errors) or error_type in [
                "UnidentifiedImageError", "ConnectionError", "Timeout"
            ]

            # æ£€æŸ¥æ˜¯å¦æ˜¯è§†é¢‘å¤„ç†ç›¸å…³çš„é”™è¯¯ï¼ˆå¦‚ç¼ºå°‘PyAV/torchcodecï¼‰
            video_errors = [
                "PyAV is not installed", "torchvision.io.video", "read_video", "video_utils.py",
                "Using `torchvision` for video decoding is deprecated"
            ]
            is_video_error = any(ve in error_msg for ve in video_errors)

            if is_image_error or is_video_error:
                _log.warning(f"âš ï¸ å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œå¼€å§‹é€ä¸ªæ£€æŸ¥å›¾ç‰‡æœ‰æ•ˆæ€§ (é”™è¯¯ç±»å‹: {error_type}): {error_msg}")

                # é€ä¸ªæ£€æŸ¥å¹¶ç§»é™¤å¤±æ•ˆçš„å›¾ç‰‡ï¼Œä¿ç•™æœ‰æ•ˆçš„å›¾ç‰‡
                cleaned_messages = []
                for msg in full_messages:
                    if isinstance(msg.get("content"), list):
                        # å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œæ£€æŸ¥æ¯å¼ å›¾ç‰‡
                        cleaned_content = []
                        for item in msg["content"]:
                            if item.get("type") == "text":
                                # æ–‡æœ¬ç›´æ¥ä¿ç•™
                                cleaned_content.append(item)
                            elif item.get("type") == "image":
                                # å›¾ç‰‡éœ€è¦æ£€æŸ¥URLæœ‰æ•ˆæ€§
                                img_url = item.get("image", "")
                                if img_url and _is_image_url_valid(img_url):
                                    # å›¾ç‰‡URLæœ‰æ•ˆï¼Œä¿ç•™
                                    cleaned_content.append(item)
                                    _log.debug(f"âœ… ä¿ç•™æœ‰æ•ˆå›¾ç‰‡: {img_url}")
                                else:
                                    # å›¾ç‰‡URLæ— æ•ˆï¼Œç§»é™¤
                                    _log.warning(f"âš ï¸ ç§»é™¤å¤±æ•ˆå›¾ç‰‡: {img_url}")
                            elif item.get("type") == "video":
                                # è§†é¢‘å†…å®¹ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°æœåŠ¡å™¨URLï¼ˆè¿™äº›æ˜¯æ°¸ä¹…æœ‰æ•ˆçš„ï¼‰
                                video_url = item.get("video") or item.get("url", "")
                                if video_url:
                                    # æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°æœåŠ¡å™¨URL
                                    if (
                                        video_url.startswith('http://127.0.0.1:9999/static/videos/')
                                        or video_url.startswith('http://localhost:9999/static/videos/')
                                        or (
                                            server_base_url
                                            and video_url.startswith(f"{server_base_url.rstrip('/')}/static/videos/")
                                        )
                                    ):
                                        # æœ¬åœ°æœåŠ¡å™¨URLï¼Œä¿ç•™ï¼ˆæ°¸ä¹…æœ‰æ•ˆï¼‰
                                        cleaned_content.append(item)
                                        _log.debug(f"âœ… ä¿ç•™æœ¬åœ°è§†é¢‘URL: {video_url}")
                                    else:
                                        # éæœ¬åœ°URLï¼Œç§»é™¤ï¼ˆå¤–éƒ¨URLå¯èƒ½å·²å¤±æ•ˆæˆ–ç¯å¢ƒä¸æ”¯æŒè§£ç ï¼‰
                                        _log.warning(f"âš ï¸ ç§»é™¤éæœ¬åœ°è§†é¢‘URL: {video_url}")
                                else:
                                    _log.warning("âš ï¸ å‘ç°æ— æ•ˆçš„è§†é¢‘é¡¹ï¼ˆæ— URLï¼‰ï¼Œè·³è¿‡")
                        if cleaned_content:  # åªä¿ç•™æœ‰å†…å®¹çš„æ¶ˆæ¯
                            cleaned_messages.append({
                                "role": msg["role"],
                                "content": cleaned_content
                            })
                    else:
                        # çº¯æ–‡æœ¬æ¶ˆæ¯ï¼Œç›´æ¥ä¿ç•™
                        cleaned_messages.append(msg)

                # å¦‚æœæ¸…ç†åæ²¡æœ‰æœ‰æ•ˆæ¶ˆæ¯ï¼Œè¿”å›é”™è¯¯
                if not cleaned_messages:
                    _log.error("âŒ æ¸…ç†å¤±æ•ˆå›¾ç‰‡åæ²¡æœ‰æœ‰æ•ˆæ¶ˆæ¯å†…å®¹")
                    return None, False, False

                # ä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯é‡è¯•
                try:
                    inputs = processor.apply_chat_template(
                        cleaned_messages,
                        tokenize=True,
                        add_generation_prompt=True,
                        return_dict=True,
                        return_tensors="pt",
                        max_length=None,
                        truncation=False,
                        padding=False
                    )
                    _log.info("âœ… æˆåŠŸä½¿ç”¨æ¸…ç†åçš„æ¶ˆæ¯ï¼ˆç§»é™¤å¤±æ•ˆå›¾ç‰‡ï¼‰ç»§ç»­å¤„ç†")
                    _log.debug(
                        f"âœ… é‡è¯•apply_chat_templateæˆåŠŸï¼Œinputsç±»å‹: {type(inputs)}ï¼Œkeys: {inputs.keys() if isinstance(inputs, dict) else 'éå­—å…¸'}"
                    )
                except Exception as retry_error:
                    _log.error(f"âŒ å³ä½¿ç§»é™¤å¤±æ•ˆå›¾ç‰‡ä¹Ÿå¤±è´¥: {retry_error}")
                    return None, False, False
            else:
                # å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                raise e
        
        # åœ¨apply_chat_templateåç«‹å³æ£€æŸ¥ä¸­æ–­ï¼ˆå¤„ç†å›¾ç‰‡å¯èƒ½è€—æ—¶å¾ˆé•¿ï¼‰
        if interrupt_event and interrupt_event.is_set():
            if chat_id and response_dict:
                with queue_lock:
                    current_processing = processing_chats.get(chat_id)
                    if current_processing and current_processing["response_dict"] is not response_dict:
                        _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨apply_chat_templateåè¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œé€€å‡ºç”Ÿæˆ")
                        return None, False, True
                    elif current_processing and current_processing["response_dict"] is response_dict:
                        # å¦‚æœä»»åŠ¡ä»ç„¶æ˜¯æœ€æ–°çš„ï¼Œä½†interrupt_eventè¢«è®¾ç½®äº†ï¼Œå¯èƒ½æ˜¯è¯¯è®¾ç½®ï¼Œæ¸…é™¤å®ƒ
                        _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨apply_chat_templateåæ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†ä»»åŠ¡ä»æ˜¯æœ€æ–°çš„ï¼Œæ¸…é™¤ä¸­æ–­ä¿¡å·")
                        interrupt_event.clear()
            else:
                # å¦‚æœæ²¡æœ‰chat_idå’Œresponse_dictï¼Œæ— æ³•éªŒè¯ï¼Œä¸ºäº†å®‰å…¨ç›´æ¥é€€å‡º
                _log.warning("âš ï¸ ç”Ÿæˆä»»åŠ¡åœ¨apply_chat_templateåæ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†æ— æ³•éªŒè¯ä»»åŠ¡çŠ¶æ€ï¼Œé€€å‡º")
                return None, False, True
        
        # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡

        # å…ˆéªŒè¯input_idsçš„æœ‰æ•ˆæ€§ï¼ˆé˜²æ­¢ç´¢å¼•è¶Šç•Œï¼‰
        vocab_size = model.config.vocab_size if hasattr(model, 'config') and hasattr(model.config, 'vocab_size') else None
        if vocab_size is not None and 'input_ids' in inputs:
            input_ids_check = inputs['input_ids']
            if isinstance(input_ids_check, torch.Tensor):
                invalid_tokens = input_ids_check[input_ids_check >= vocab_size]
                if len(invalid_tokens) > 0:
                    _log.error(f"âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆtoken ID: {invalid_tokens}, vocab_size={vocab_size}")
                    # å°†æ— æ•ˆçš„token IDé™åˆ¶åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    inputs['input_ids'] = torch.clamp(input_ids_check, 0, vocab_size - 1)
        
        inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                 for k, v in inputs.items()}
        
        # åœ¨ç§»åŠ¨åˆ°è®¾å¤‡åæ£€æŸ¥ä¸­æ–­
        if interrupt_event and interrupt_event.is_set():
            if chat_id and response_dict:
                with queue_lock:
                    current_processing = processing_chats.get(chat_id)
                    if current_processing and current_processing["response_dict"] is not response_dict:
                        _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡åè¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œé€€å‡ºç”Ÿæˆ")
                        return None, False, True
                    elif current_processing and current_processing["response_dict"] is response_dict:
                        # å¦‚æœä»»åŠ¡ä»ç„¶æ˜¯æœ€æ–°çš„ï¼Œä½†interrupt_eventè¢«è®¾ç½®äº†ï¼Œå¯èƒ½æ˜¯è¯¯è®¾ç½®ï¼Œæ¸…é™¤å®ƒ
                        _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡åæ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†ä»»åŠ¡ä»æ˜¯æœ€æ–°çš„ï¼Œæ¸…é™¤ä¸­æ–­ä¿¡å·")
                        interrupt_event.clear()
        
        # æ‰“å°å®Œæ•´çš„è¾“å…¥ï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰
        input_ids_text = processor.tokenizer.batch_decode(
            inputs['input_ids'],
            skip_special_tokens=False,
            clean_up_tokenization_spaces=False
        )
        if log_full_io:
            _log.info("=" * 80)
            _log.info("ğŸ”¤ æ¨¡å‹å®Œæ•´è¾“å…¥ï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰ï¼š")
            _log.info(input_ids_text[0])
            _log.info("=" * 80)

        # åœ¨æ‰“å°è¾“å…¥åæ£€æŸ¥ä¸­æ–­
        if interrupt_event and interrupt_event.is_set():
            if chat_id and response_dict:
                with queue_lock:
                    current_processing = processing_chats.get(chat_id)
                    if current_processing and current_processing["response_dict"] is not response_dict:
                        _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ‰“å°è¾“å…¥åè¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œé€€å‡ºç”Ÿæˆ")
                        return None, False, True
                    elif current_processing and current_processing["response_dict"] is response_dict:
                        # å¦‚æœä»»åŠ¡ä»ç„¶æ˜¯æœ€æ–°çš„ï¼Œä½†interrupt_eventè¢«è®¾ç½®äº†ï¼Œå¯èƒ½æ˜¯è¯¯è®¾ç½®ï¼Œæ¸…é™¤å®ƒ
                        _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨æ‰“å°è¾“å…¥åæ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†ä»»åŠ¡ä»æ˜¯æœ€æ–°çš„ï¼Œæ¸…é™¤ä¸­æ–­ä¿¡å·")
                        interrupt_event.clear()
        
        # åœ¨ç”Ÿæˆå‰æ£€æŸ¥æ˜¯å¦å·²ç»è¢«ä¸­æ–­ï¼Œå¹¶éªŒè¯ä»»åŠ¡æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„
        if interrupt_event and interrupt_event.is_set():
            if chat_id and response_dict:
                # æ£€æŸ¥processing_chatsï¼Œç¡®è®¤å½“å‰ä»»åŠ¡æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°ä»»åŠ¡
                with queue_lock:
                    current_processing = processing_chats.get(chat_id)
                    if current_processing and current_processing["response_dict"] is response_dict:
                        # å½“å‰ä»»åŠ¡ä»ç„¶æ˜¯æœ€æ–°ä»»åŠ¡ï¼Œä½†interrupt_eventè¢«è®¾ç½®äº†
                        # è¿™å¯èƒ½æ˜¯è¯¯è®¾ç½®ï¼ˆæ¯”å¦‚ä¸åŒèŠå¤©ä¹‹é—´åˆ‡æ¢æ—¶ï¼‰ï¼Œæ¸…é™¤å®ƒå¹¶ç»§ç»­
                        _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå¼€å§‹å‰æ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†ä»»åŠ¡ä»æ˜¯æœ€æ–°çš„ï¼Œæ¸…é™¤ä¸­æ–­ä¿¡å·å¹¶ç»§ç»­")
                        interrupt_event.clear()
                    else:
                        # å½“å‰ä»»åŠ¡å·²ç»ä¸æ˜¯æœ€æ–°ä»»åŠ¡ï¼Œåº”è¯¥é€€å‡º
                        _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨ç”Ÿæˆå¼€å§‹å‰å·²è¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œé€€å‡ºç”Ÿæˆ")
                        return None, False, True
            else:
                # å¦‚æœæ²¡æœ‰chat_idå’Œresponse_dictï¼Œæ— æ³•éªŒè¯ï¼Œä¸ºäº†å®‰å…¨ç›´æ¥é€€å‡º
                _log.warning("âš ï¸ ç”Ÿæˆä»»åŠ¡åœ¨å¼€å§‹å‰æ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†æ— æ³•éªŒè¯ä»»åŠ¡çŠ¶æ€ï¼Œé€€å‡º")
                return None, False, True
        
        _log.info("å¼€å§‹ç”Ÿæˆå›å¤...")
        
        # å‡†å¤‡LogitsProcessorï¼ˆä½¿ç”¨transformersæ ‡å‡†å®ç°ï¼‰
        logits_processor = LogitsProcessorList()
        if repetition_penalty != 1.0:
            logits_processor.append(RepetitionPenaltyLogitsProcessor(penalty=repetition_penalty))
        
        # å‡†å¤‡StoppingCriteriaï¼ˆæ”¯æŒä¸­æ–­ï¼‰
        stopping_criteria = StoppingCriteriaList()
        if interrupt_event:
            stopping_criteria.append(InterruptStoppingCriteria(interrupt_event))
        
        # ä½¿ç”¨å®Œå…¨å¤åˆ»transformerså®˜æ–¹æºç çš„è‡ªå®šä¹‰generateæ–¹æ³•
        # è¿™ä¸ªå®ç°å®Œå…¨æŒ‰ç…§transformersçš„é€»è¾‘ï¼Œæ–¹ä¾¿åç»­é­”æ”¹
        # ä½¿ç”¨æ¨¡å‹é”ç¡®ä¿åŒä¸€æ—¶åˆ»åªæœ‰ä¸€ä¸ªçº¿ç¨‹ä½¿ç”¨æ¨¡å‹ï¼ˆä¸²è¡Œæ¨ç†ï¼‰
        # æ³¨æ„ï¼šè·å–é”åï¼Œå¦‚æœå·²è¢«ä¸­æ–­ï¼Œåœ¨ç”Ÿæˆå¾ªç¯ä¸­ä¼šæ£€æµ‹åˆ°
        with model_lock:
            # åœ¨è·å–é”ä¹‹åã€å¼€å§‹ç”Ÿæˆä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦å·²è¢«ä¸­æ–­
            # è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå¯èƒ½åœ¨ç­‰å¾…è·å–é”æœŸé—´æœ‰æ–°æ¶ˆæ¯åˆ°è¾¾
            # å…³é”®ä¿®å¤ï¼šæ£€æŸ¥processing_chatsï¼Œç¡®è®¤å½“å‰ä»»åŠ¡æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°ä»»åŠ¡
            # å¦‚æœinterrupt_eventè¢«è®¾ç½®ï¼Œä½†å½“å‰ä»»åŠ¡ä»ç„¶æ˜¯æœ€æ–°çš„ï¼Œè¯´æ˜æ˜¯è¯¯è®¾ç½®ï¼Œåº”è¯¥æ¸…é™¤
            if interrupt_event and interrupt_event.is_set():
                if chat_id and response_dict:
                    # æ£€æŸ¥processing_chatsï¼Œç¡®è®¤å½“å‰ä»»åŠ¡æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°ä»»åŠ¡
                    with queue_lock:
                        current_processing = processing_chats.get(chat_id)
                        if current_processing and current_processing["response_dict"] is response_dict:
                            # å½“å‰ä»»åŠ¡ä»ç„¶æ˜¯æœ€æ–°ä»»åŠ¡ï¼Œä½†interrupt_eventè¢«è®¾ç½®äº†
                            # è¿™å¯èƒ½æ˜¯è¯¯è®¾ç½®ï¼ˆæ¯”å¦‚ä¸åŒèŠå¤©ä¹‹é—´åˆ‡æ¢æ—¶ï¼‰ï¼Œæ¸…é™¤å®ƒå¹¶ç»§ç»­
                            _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨è·å–æ¨¡å‹é”åæ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†ä»»åŠ¡ä»æ˜¯æœ€æ–°çš„ï¼Œæ¸…é™¤ä¸­æ–­ä¿¡å·å¹¶ç»§ç»­")
                            interrupt_event.clear()
                        else:
                            # å½“å‰ä»»åŠ¡å·²ç»ä¸æ˜¯æœ€æ–°ä»»åŠ¡ï¼Œåº”è¯¥é€€å‡º
                            _log.warning(f"âš ï¸ èŠå¤© {chat_id} çš„ä»»åŠ¡åœ¨è·å–æ¨¡å‹é”åå·²è¢«æ–°ä»»åŠ¡æ›¿æ¢ï¼Œé€€å‡ºç”Ÿæˆ")
                            return None, False, True
                else:
                    # å¦‚æœæ²¡æœ‰chat_idå’Œresponse_dictï¼Œæ— æ³•éªŒè¯ï¼Œä¸ºäº†å®‰å…¨ç›´æ¥é€€å‡º
                    _log.warning("âš ï¸ ç”Ÿæˆä»»åŠ¡åœ¨è·å–æ¨¡å‹é”åæ£€æµ‹åˆ°interrupt_eventè¢«è®¾ç½®ï¼Œä½†æ— æ³•éªŒè¯ä»»åŠ¡çŠ¶æ€ï¼Œé€€å‡º")
                    return None, False, True
            with torch.no_grad():
                try:
                    result = custom_generate(
                        model=model,
                        inputs=inputs,
                        max_new_tokens=max_new_tokens,
                        stopping_criteria=stopping_criteria,
                        logits_processor=logits_processor,
                        temperature=temperature,
                        top_k=top_k if top_k and top_k > 0 else None,
                        top_p=top_p if top_p and top_p < 1.0 else None,
                        do_sample=do_sample,
                        pad_token_id=processor.tokenizer.eos_token_id,
                        eos_token_id=processor.tokenizer.eos_token_id,
                        interrupt_event=interrupt_event,
                        early_stop_on_tool_call=False,
                    )
                    # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯ (input_ids, memory_injection_positions) æˆ– input_ids
                    if isinstance(result, tuple):
                        generated_ids, memory_injection_positions = result
                    else:
                        generated_ids = result
                        memory_injection_positions = []
                except Exception as e:
                    # æ£€æŸ¥æ˜¯å¦å› ä¸ºä¸­æ–­è€Œåœæ­¢
                    if interrupt_event and interrupt_event.is_set():
                        _log.warning("âš ï¸ ç”Ÿæˆè¿‡ç¨‹è¢«ä¸­æ–­")
                        return None, False, True
                    raise e
            
            # æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­
            if interrupt_event and interrupt_event.is_set():
                _log.warning("âš ï¸ ç”Ÿæˆè¿‡ç¨‹è¢«ä¸­æ–­")
                return None, False, True
        
        # ç”Ÿæˆå®Œæˆåç«‹å³æ£€æŸ¥æ˜¯å¦è¢«ä¸­æ–­ï¼ˆåœ¨é‡Šæ”¾é”ä¹‹å‰æ£€æŸ¥ï¼‰
        # è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå¯èƒ½åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰æ–°æ¶ˆæ¯åˆ°è¾¾
        if interrupt_event and interrupt_event.is_set():
            _log.warning("âš ï¸ ç”Ÿæˆè¿‡ç¨‹åœ¨å®Œæˆåè¢«ä¸­æ–­ï¼Œä¸¢å¼ƒç»“æœ")
            return None, False, True
        
        # åœ¨é‡Šæ”¾æ¨¡å‹é”ä¹‹å‰ï¼Œå†æ¬¡æ£€æŸ¥æ˜¯å¦ä»ç„¶æ˜¯æœ€æ–°çš„ä»»åŠ¡
        # é€šè¿‡æ£€æŸ¥interrupt_eventæ¥åˆ¤æ–­ï¼ˆå¦‚æœè¢«ä¸­æ–­ï¼Œè¯´æ˜æœ‰æ›´æ–°çš„ä»»åŠ¡ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½ä½¿ç”¨processing_chatsæ£€æŸ¥ï¼Œå› ä¸ºè¿˜åœ¨æŒæœ‰model_lock
        if interrupt_event and interrupt_event.is_set():
            _log.warning("âš ï¸ ç”Ÿæˆè¿‡ç¨‹åœ¨é‡Šæ”¾é”å‰è¢«ä¸­æ–­ï¼Œä¸¢å¼ƒç»“æœ")
            return None, False, True
        
        # æå–ç”Ÿæˆçš„tokenï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
        generated_ids_trimmed = [
            out_ids[len(in_ids):] 
            for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
        ]
        
        # è§£ç ç”Ÿæˆç»“æœï¼ˆåŒ…å«ç‰¹æ®Štokençš„ç‰ˆæœ¬ï¼‰
        output_text_with_special = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=False,  # ä¸è·³è¿‡ç‰¹æ®Štoken
            clean_up_tokenization_spaces=False
        )
        
        # æ‰“å°å®Œæ•´çš„è¾“å‡ºï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰
        # æ³¨æ„ï¼šè®°å¿†å‘é‡æ’å…¥ä½ç½®ç°åœ¨é€šè¿‡<|memory_pad|> tokenåŸç”Ÿæ˜¾ç¤ºï¼Œæ— éœ€é¢å¤–æ ‡æ³¨
        if log_full_io:
            _log.info("=" * 80)
            _log.info("ğŸ”¤ æ¨¡å‹å®Œæ•´è¾“å‡ºï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰ï¼š")
            _log.info(output_text_with_special[0])
            _log.info("=" * 80)
        
        # è§£ç ç”Ÿæˆç»“æœï¼ˆæ­£å¸¸ç‰ˆæœ¬ï¼Œè·³è¿‡ç‰¹æ®Štokenï¼‰
        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )
        
        full_output = output_text[0] if output_text else ""
        
        # æå–thinkingæ¨¡å‹çš„æ­£å¼å›å¤ä¸åŠ¨ä½œæŒ‡ä»¤ï¼ˆ</think>æ ‡ç­¾åçš„å†…å®¹ï¼‰
        reply, should_reply, action_cmds = extract_final_reply(full_output)
        
        if should_reply:
            _log.info(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå®Œæ•´è¾“å‡ºé•¿åº¦: {len(full_output)}, æ­£å¼å›å¤é•¿åº¦: {len(reply)}")
        else:
            _log.info(f"âœ… ç”Ÿæˆå®Œæˆï¼Œæ¨¡å‹åˆ¤æ–­ä¸éœ€è¦å›å¤")
        
        # é™„å¸¦è¿”å›è§£æåˆ°çš„åŠ¨ä½œæŒ‡ä»¤
        try:
            if action_cmds:
                _log.info(f"ğŸ¯ è§£æåˆ°åŠ¨ä½œæŒ‡ä»¤ {len(action_cmds)} æ¡")
        except Exception:
            pass
        return reply, should_reply, False, (action_cmds or [])
        
    except Exception as e:
        _log.error(f"ç”Ÿæˆå›å¤å¤±è´¥: {e}", exc_info=True)
        raise
    finally:
        # æ¨ç†åå°½é‡å›æ”¶æ˜¾å­˜
        try:
            import torch as _torch
            if _torch.cuda.is_available():
                _torch.cuda.empty_cache()
        except Exception:
            pass
        # è®°å½•æ—¶å»¶
        try:
            import time as __t
            _metrics_add_latency((__t.time() - _t0) * 1000.0)
        except Exception:
            pass


# å¥åº·æ£€æŸ¥ä¸æŒ‡æ ‡è·¯ç”±å·²è¿ç§»è‡³ routes/health.py

# æŒ‡æ ‡è·¯ç”±å·²è¿ç§»è‡³ routes/health.py


def trigger_training():
    """
    æ‰‹åŠ¨è§¦å‘è®°å¿†è®­ç»ƒï¼ˆç”¨äºè°ƒè¯•ï¼‰
    
    Returns:
        JSONå“åº”ï¼ŒåŒ…å«è®­ç»ƒçŠ¶æ€å’Œè¯¦ç»†ä¿¡æ¯
    """
    global training_scheduler
    
    _log.info("æ”¶åˆ°æ‰‹åŠ¨è®­ç»ƒè§¦å‘è¯·æ±‚")
    
    if training_scheduler is None:
        # å°è¯•é‡æ–°åˆå§‹åŒ–è®­ç»ƒè°ƒåº¦å™¨ï¼ˆå¤šçº¿ç¨‹ç¯å¢ƒä¸‹å¯èƒ½ä¸¢å¤±ï¼‰
        try:
            _log.warning("æ£€æµ‹åˆ°training_schedulerä¸ºNoneï¼Œå°è¯•é‡æ–°åˆå§‹åŒ–...")
            from memory_training_scheduler import MemoryTrainingScheduler

            # ä¿å­˜è„šæœ¬è·¯å¾„å’Œå‚æ•°ï¼Œç”¨äºè®­ç»ƒå®Œæˆåé‡å¯
            script_path = os.path.abspath(__file__)
            script_args = sys.argv[1:]  # ä¿å­˜å‘½ä»¤è¡Œå‚æ•°ï¼ˆé™¤äº†è„šæœ¬åï¼‰

            _log.info("æ­£åœ¨é‡æ–°åˆ›å»º MemoryTrainingScheduler å®ä¾‹...")
            training_scheduler = MemoryTrainingScheduler(config, script_path, script_args)
            _log.info("âœ… é‡æ–°åˆå§‹åŒ–è®­ç»ƒè°ƒåº¦å™¨æˆåŠŸ")

            # å°è¯•å¯åŠ¨ï¼ˆå¦‚æœæ²¡æœ‰å¯åŠ¨çš„è¯ï¼‰
            if not hasattr(training_scheduler, 'scheduler') or not training_scheduler.scheduler.running:
                training_scheduler.start()
                _log.info("âœ… é‡æ–°å¯åŠ¨è®­ç»ƒè°ƒåº¦å™¨æˆåŠŸ")

        except Exception as init_error:
            _log.error(f"âŒ é‡æ–°åˆå§‹åŒ–è®­ç»ƒè°ƒåº¦å™¨å¤±è´¥: {init_error}")
            return jsonify({
                "success": False,
                "error": "è®­ç»ƒè°ƒåº¦å™¨é‡æ–°åˆå§‹åŒ–å¤±è´¥",
                "message": f"æ— æ³•é‡æ–°åˆå§‹åŒ–è®­ç»ƒè°ƒåº¦å™¨: {str(init_error)}"
            }), 500
    
    try:
        _log.info("=" * 60)
        _log.info("æ‰‹åŠ¨è§¦å‘è®­ç»ƒä»»åŠ¡")
        _log.info("=" * 60)
        
        # æ£€æŸ¥æ˜¯å¦æ­£åœ¨è®­ç»ƒ
        if training_scheduler.is_running:
            return jsonify({
                "success": False,
                "error": "è®­ç»ƒä»»åŠ¡æ­£åœ¨è¿è¡Œ",
                "message": "è¯·ç­‰å¾…å½“å‰è®­ç»ƒä»»åŠ¡å®Œæˆ"
            }), 409
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œè®­ç»ƒï¼ˆé¿å…é˜»å¡HTTPè¯·æ±‚ï¼‰
        import threading
        training_result = {"status": "running", "details": {}}
        
        def run_training_async():
            global group_chat_histories, private_chat_histories, CHAT_HISTORY_STORAGE_DIR, training_scheduler, is_training, model, processor

            # åœ¨ä½¿ç”¨å…¨å±€å˜é‡å‰å…ˆå£°æ˜ï¼ˆé¿å…SyntaxErrorï¼‰
            global group_chat_histories, private_chat_histories
            
            # ç¡®ä¿torchåœ¨å‡½æ•°ä½œç”¨åŸŸå†…å¯ç”¨ï¼ˆå·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ï¼‰
            import torch
            
            try:
                # è®¾ç½®è®­ç»ƒæ¨¡å¼æ ‡å¿—ï¼Œé˜»æ­¢APIè¯·æ±‚å’Œæ¨¡å‹ç”Ÿæˆ
                with training_lock:
                    is_training = True
                _log.info("ğŸ”’ å·²è¿›å…¥è®­ç»ƒæ¨¡å¼ï¼ŒAPIæ¥æ”¶ä¿¡æ¯å’Œæ¨¡å‹ç”Ÿæˆå›å¤åŠŸèƒ½å·²åœæ­¢")
                
                training_result["status"] = "running"
                training_result["details"]["started_at"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                # æ­¥éª¤0: å¼ºåˆ¶ä¿å­˜å†…å­˜ä¸­çš„èŠå¤©è®°å½•åˆ°JSONæ–‡ä»¶ï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­ï¼Œå¯ä»¥è®¿é—®å…¨å±€å˜é‡ï¼‰
                _log.info("=" * 60)
                _log.info("æ­¥éª¤0: å¼ºåˆ¶ä¿å­˜å†…å­˜ä¸­çš„èŠå¤©è®°å½•ï¼ˆåœ¨è®­ç»ƒçº¿ç¨‹ä¸­ï¼‰")
                _log.info("=" * 60)
                
                # ç»Ÿè®¡å½“å‰å†…å­˜ä¸­çš„èŠå¤©è®°å½•ï¼ˆç›´æ¥ä½¿ç”¨å…¨å±€å˜é‡ï¼‰
                group_count = len(group_chat_histories)
                private_count = len(private_chat_histories)
                total_group_messages = sum(len(history) for history in group_chat_histories.values())
                total_private_messages = sum(len(history) for history in private_chat_histories.values())
                
                _log.info(f"ğŸ“Š å†…å­˜ä¸­çš„èŠå¤©è®°å½•ç»Ÿè®¡:")
                _log.info(f"   ç¾¤èŠæ•°é‡: {group_count}")
                _log.info(f"   ç§èŠæ•°é‡: {private_count}")
                _log.info(f"   ç¾¤èŠæ¶ˆæ¯æ€»æ•°: {total_group_messages}")
                _log.info(f"   ç§èŠæ¶ˆæ¯æ€»æ•°: {total_private_messages}")
                
                # è¯¦ç»†è¾“å‡ºæ¯ä¸ªèŠå¤©çš„æ¶ˆæ¯æ•°
                for chat_id, history in group_chat_histories.items():
                    _log.info(f"   ç¾¤èŠ {chat_id}: {len(history)} æ¡æ¶ˆæ¯")
                for chat_id, history in private_chat_histories.items():
                    _log.info(f"   ç§èŠ {chat_id}: {len(history)} æ¡æ¶ˆæ¯")
                
                # ä¿å­˜èŠå¤©è®°å½•ï¼ˆåŒæ­¥é˜»å¡ï¼Œç­‰å¾…æ¯ä¸ªä¿å­˜å®Œæˆï¼‰
                saved_count = 0
                for chat_id, history in group_chat_histories.items():
                    if history:
                        try:
                            _log.info(f"æ­£åœ¨ä¿å­˜ç¾¤èŠ {chat_id} çš„ {len(history)} æ¡æ¶ˆæ¯...")
                            save_chat_history_to_storage("group", chat_id, history)  # åŒæ­¥é˜»å¡ï¼Œç­‰å¾…å®Œæˆ
                            saved_count += len(history)
                            _log.info(f"âœ… ç¾¤èŠ {chat_id} ä¿å­˜å®Œæˆ")
                        except Exception as e:
                            _log.error(f"ä¿å­˜ç¾¤èŠ {chat_id} å¤±è´¥: {e}", exc_info=True)
                
                for chat_id, history in private_chat_histories.items():
                    if history:
                        try:
                            _log.info(f"æ­£åœ¨ä¿å­˜ç§èŠ {chat_id} çš„ {len(history)} æ¡æ¶ˆæ¯...")
                            save_chat_history_to_storage("private", chat_id, history)  # åŒæ­¥é˜»å¡ï¼Œç­‰å¾…å®Œæˆ
                            saved_count += len(history)
                            _log.info(f"âœ… ç§èŠ {chat_id} ä¿å­˜å®Œæˆ")
                        except Exception as e:
                            _log.error(f"ä¿å­˜ç§èŠ {chat_id} å¤±è´¥: {e}", exc_info=True)
                
                _log.info(f"âœ… æ­¥éª¤0å®Œæˆï¼šå…±ä¿å­˜ {saved_count} æ¡å†…å­˜ä¸­çš„èŠå¤©è®°å½•åˆ°å­˜å‚¨ï¼ˆæ‰€æœ‰ä¿å­˜æ“ä½œå·²å®Œæˆï¼‰")
                _log.info("=" * 60)
                
                # æ­¥éª¤0.5: å¸è½½ä¸»æ¨¡å‹ä»¥é‡Šæ”¾æ˜¾å­˜ï¼ˆè®­ç»ƒæ—¶ä¼šåŠ è½½æ–°çš„æ¨¡å‹å®ä¾‹ï¼‰
                _log.info("=" * 60)
                _log.info("æ­¥éª¤0.5: å½»åº•æ¸…ç†æ‰€æœ‰æ¨¡å‹å’Œæ˜¾å­˜")
                _log.info("=" * 60)

                # æ¸…ç†ä¸»æ¨¡å‹
                if model is not None:
                    _log.info("æ­£åœ¨å¸è½½ä¸»æ¨¡å‹...")
                    try:
                        model = model.cpu()
                    except:
                        pass
                    del model
                    model = None

                if processor is not None:
                    del processor
                    processor = None

                # æ¸…ç†å…¨å±€å˜é‡ï¼ˆå·²åœ¨å‡½æ•°å¼€å§‹æ—¶å£°æ˜globalï¼‰
                group_chat_histories.clear()
                private_chat_histories.clear()

                # å¤šé‡åƒåœ¾å›æ”¶å’Œæ˜¾å­˜æ¸…ç†
                import gc
                for _ in range(3):  # å¤šæ¬¡GCç¡®ä¿æ¸…ç†å½»åº•
                    gc.collect()

                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                    torch.cuda.reset_peak_memory_stats()
                    torch.cuda.empty_cache()

                _log.info("âœ… æ‰€æœ‰æ¨¡å‹å’Œæ˜¾å­˜å·²å½»åº•æ¸…ç†")
                _log.info("=" * 60)
                
                # ç¡®ä¿è®­ç»ƒæœåŠ¡å·²åˆå§‹åŒ–
                if training_scheduler is None:
                    _log.error("âŒ training_scheduler ä¸º Noneï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
                    raise RuntimeError("training_scheduler æœªåˆå§‹åŒ–")
                if not training_scheduler.training_service:
                    training_scheduler._setup_training_service()
                
                # æ‰§è¡Œè®­ç»ƒï¼ˆè®­ç»ƒæœåŠ¡ä¼šä»JSONæ–‡ä»¶åŠ è½½èŠå¤©è®°å½•ï¼‰
                model_path = training_scheduler.training_service.run_training(skip_memory_dump=True)
                
                # è®­ç»ƒå®Œæˆåï¼Œé‡æ–°åŠ è½½ä¸»æ¨¡å‹ï¼ˆæ— è®ºæ˜¯å¦æå–åˆ°è®°å¿†æ¡ç›®ï¼‰
                # å› ä¸ºè®­ç»ƒå¼€å§‹æ—¶å¸è½½äº†ä¸»æ¨¡å‹ï¼Œæ‰€ä»¥å¿…é¡»é‡æ–°åŠ è½½
                memory_config = config.get("memory", {}).get("training", {})
                auto_restart = memory_config.get("auto_restart_after_training", False)
                restart_mode = memory_config.get("restart_mode", "reload_model")
                
                if model_path:
                    training_result["status"] = "completed"
                    training_result["details"]["model_path"] = model_path
                    training_result["details"]["completed_at"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    # è®­ç»ƒå®Œæˆåï¼Œæ€»æ˜¯é‡æ–°å¯åŠ¨æ•´ä¸ªæœåŠ¡å™¨è¿›ç¨‹ï¼Œé¿å…å æ®ç«¯å£
                    _log.info("è®­ç»ƒå®Œæˆï¼Œé‡æ–°å¯åŠ¨æœåŠ¡å™¨è¿›ç¨‹...")
                    training_result["details"]["restart_mode"] = "restart_server"
                    training_result["details"]["restart_scheduled"] = True
                    training_scheduler.restart_server()  # é‡æ–°å¯åŠ¨æ•´ä¸ªè¿›ç¨‹
                else:
                    training_result["status"] = "skipped"
                    training_result["details"]["reason"] = "æ²¡æœ‰æ•°æ®æˆ–æ²¡æœ‰æå–åˆ°è®°å¿†æ¡ç›®"
                    training_result["details"]["completed_at"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    
                    # è®­ç»ƒè·³è¿‡åï¼Œä¹Ÿé‡æ–°å¯åŠ¨æœåŠ¡å™¨è¿›ç¨‹
                    _log.info("è®­ç»ƒè·³è¿‡ï¼Œé‡æ–°å¯åŠ¨æœåŠ¡å™¨è¿›ç¨‹...")
                    training_result["details"]["restart_mode"] = "restart_server"
                    training_result["details"]["restart_scheduled"] = True
                    training_scheduler.restart_server()  # é‡æ–°å¯åŠ¨æ•´ä¸ªè¿›ç¨‹
                    
            except Exception as e:
                training_result["status"] = "failed"
                training_result["error"] = str(e)
                training_result["details"]["failed_at"] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                _log.error(f"æ‰‹åŠ¨è§¦å‘è®­ç»ƒå¤±è´¥: {e}", exc_info=True)
                
                # è®­ç»ƒå¤±è´¥æ—¶ï¼Œæ‰§è¡Œå½»åº•æ¸…ç†å¹¶é€€å‡ºè¿›ç¨‹
                _log.info("=" * 60)
                _log.info("è®­ç»ƒå¤±è´¥ï¼Œæ‰§è¡Œå½»åº•æ˜¾å­˜æ¸…ç†å¹¶é€€å‡ºè¿›ç¨‹...")
                _log.info("=" * 60)

                import sys
                
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿TrainingModelContextçš„__exit__å®Œå…¨æ‰§è¡Œ
                import time
                time.sleep(2)
                
                # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰GPUçš„æ˜¾å­˜
                import gc
                import torch
                
                # å¤šæ¬¡åƒåœ¾å›æ”¶
                for _ in range(5):
                    gc.collect()
                
                # æ¸…ç†æ‰€æœ‰GPUçš„æ˜¾å­˜
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        with torch.cuda.device(i):
                            torch.cuda.synchronize()
                            torch.cuda.empty_cache()
                            torch.cuda.reset_peak_memory_stats()
                    
                    # å†æ¬¡æ¸…ç†
                    for i in range(torch.cuda.device_count()):
                        with torch.cuda.device(i):
                            torch.cuda.empty_cache()
                    
                    _log.info(f"âœ… å·²æ¸…ç†æ‰€æœ‰ {torch.cuda.device_count()} å¼ GPUçš„æ˜¾å­˜")
                
                # è®­ç»ƒå¤±è´¥åç›´æ¥é€€å‡ºè¿›ç¨‹ï¼Œé¿å…å æ®æ˜¾å­˜å’Œç«¯å£
                _log.info("è®­ç»ƒå¤±è´¥ï¼Œè¿›ç¨‹å³å°†é€€å‡º...")
                # åœ¨é€€å‡ºå‰ç¡®ä¿è®­ç»ƒæ¨¡å¼å·²è§£é™¤
                with training_lock:
                    is_training = False
                sys.exit(1)
            finally:
                # è®­ç»ƒå®Œæˆæˆ–å¤±è´¥åï¼Œè§£é™¤è®­ç»ƒæ¨¡å¼ï¼ˆé™¤éè¿›ç¨‹å·²é€€å‡ºï¼‰
                try:
                    with training_lock:
                        is_training = False
                    _log.info("ğŸ”“ å·²é€€å‡ºè®­ç»ƒæ¨¡å¼ï¼ŒAPIæ¥æ”¶ä¿¡æ¯å’Œæ¨¡å‹ç”Ÿæˆå›å¤åŠŸèƒ½å·²æ¢å¤")
                except Exception:
                    # å¦‚æœè¿›ç¨‹æ­£åœ¨é€€å‡ºï¼Œå¿½ç•¥è¿™ä¸ªé”™è¯¯
                    pass
        
        # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
        training_thread = threading.Thread(target=run_training_async, daemon=True)
        training_thread.start()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œæ£€æŸ¥è®­ç»ƒæ˜¯å¦ç«‹å³å¤±è´¥ï¼ˆä¾‹å¦‚æ²¡æœ‰æ•°æ®ï¼‰
        import time
        time.sleep(0.5)
        
        if training_result["status"] in ["skipped", "failed"]:
            return jsonify({
                "success": training_result["status"] == "skipped",
                "status": training_result["status"],
                "details": training_result.get("details", {}),
                "error": training_result.get("error")
            }), 200 if training_result["status"] == "skipped" else 500
        else:
            # è®­ç»ƒå·²å¼€å§‹ï¼Œè¿”å›å¯åŠ¨ä¿¡æ¯
            return jsonify({
                "success": True,
                "status": "started",
                "message": "è®­ç»ƒä»»åŠ¡å·²åœ¨åå°å¯åŠ¨",
                "details": training_result.get("details", {})
            }), 200
            
    except Exception as e:
        _log.error(f"è§¦å‘è®­ç»ƒæ—¶å‡ºé”™: {e}", exc_info=True)
        return jsonify({
            "success": False,
            "error": str(e),
            "message": "è§¦å‘è®­ç»ƒå¤±è´¥"
        }), 500


def get_training_status():
    """
    è·å–è®­ç»ƒçŠ¶æ€
    
    Returns:
        JSONå“åº”ï¼ŒåŒ…å«å½“å‰è®­ç»ƒçŠ¶æ€
    """
    global training_scheduler
    
    if training_scheduler is None:
        return jsonify({
            "training_enabled": False,
            "message": "è®­ç»ƒè°ƒåº¦å™¨æœªåˆå§‹åŒ–"
        }), 200
    
    memory_config = config.get("memory", {}).get("training", {})
    training_enabled = memory_config.get("enabled", False)
    schedule = memory_config.get("schedule", "3-7")
    
    return jsonify({
        "training_enabled": training_enabled,
        "schedule": schedule,
        "is_running": training_scheduler.is_running,
        "scheduler_running": training_scheduler.scheduler.running if hasattr(training_scheduler, 'scheduler') else False
    }), 200


def debug_training_scheduler():
    """
    è°ƒè¯•è®­ç»ƒè°ƒåº¦å™¨çŠ¶æ€ï¼ˆç”¨äºæ’æŸ¥é—®é¢˜ï¼‰
    
    Returns:
        JSONå“åº”ï¼ŒåŒ…å«è®­ç»ƒè°ƒåº¦å™¨çš„è¯¦ç»†çŠ¶æ€ä¿¡æ¯
    """
    global training_scheduler
    
    # æ£€æŸ¥æ¨¡å—çº§åˆ«çš„ training_scheduler
    import api_server_qwen3vl as api_module
    module_training_scheduler = api_module.training_scheduler
    
    debug_info = {
        "global_training_scheduler": {
            "is_none": training_scheduler is None,
            "type": str(type(training_scheduler)),
            "id": id(training_scheduler) if training_scheduler is not None else None,
            "value": str(training_scheduler) if training_scheduler is not None else None
        },
        "module_training_scheduler": {
            "is_none": module_training_scheduler is None,
            "type": str(type(module_training_scheduler)),
            "id": id(module_training_scheduler) if module_training_scheduler is not None else None,
            "value": str(module_training_scheduler) if module_training_scheduler is not None else None
        },
        "are_same_object": training_scheduler is module_training_scheduler,
        "config": {
            "memory_training_enabled": config.get("memory", {}).get("training", {}).get("enabled", False),
            "auto_restart": config.get("memory", {}).get("training", {}).get("auto_restart_after_training", False),
            "restart_mode": config.get("memory", {}).get("training", {}).get("restart_mode", "reload_model")
        }
    }
    
    if training_scheduler is not None:
        try:
            debug_info["training_scheduler_details"] = {
                "is_running": training_scheduler.is_running,
                "has_training_service": training_scheduler.training_service is not None,
                "scheduler_running": training_scheduler.scheduler.running if hasattr(training_scheduler, 'scheduler') else False
            }
        except Exception as e:
            debug_info["training_scheduler_details"] = {
                "error": str(e)
            }
    
    return jsonify(debug_info), 200


def save_chat_history_manually():
    """
    æ‰‹åŠ¨ä¿å­˜å½“å‰å†…å­˜ä¸­çš„èŠå¤©è®°å½•åˆ°å­˜å‚¨ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    
    Returns:
        JSONå“åº”ï¼ŒåŒ…å«ä¿å­˜çŠ¶æ€å’Œç»Ÿè®¡ä¿¡æ¯
    """
    global training_scheduler, group_chat_histories, private_chat_histories
    
    try:
        _log.info("=" * 60)
        _log.info("æ‰‹åŠ¨è§¦å‘ä¿å­˜èŠå¤©è®°å½•")
        _log.info("=" * 60)
        
        # ç»Ÿè®¡å½“å‰å†…å­˜ä¸­çš„èŠå¤©è®°å½•
        group_count = len(group_chat_histories)
        private_count = len(private_chat_histories)
        total_group_messages = sum(len(history) for history in group_chat_histories.values())
        total_private_messages = sum(len(history) for history in private_chat_histories.values())
        
        _log.info(f"ğŸ“Š å½“å‰å†…å­˜ä¸­çš„èŠå¤©è®°å½•ç»Ÿè®¡:")
        _log.info(f"   ç¾¤èŠæ•°é‡: {group_count}")
        _log.info(f"   ç§èŠæ•°é‡: {private_count}")
        _log.info(f"   ç¾¤èŠæ¶ˆæ¯æ€»æ•°: {total_group_messages}")
        _log.info(f"   ç§èŠæ¶ˆæ¯æ€»æ•°: {total_private_messages}")
        
        # è¯¦ç»†è¾“å‡ºæ¯ä¸ªèŠå¤©çš„æ¶ˆæ¯æ•°
        for chat_id, history in group_chat_histories.items():
            _log.info(f"   ç¾¤èŠ {chat_id}: {len(history)} æ¡æ¶ˆæ¯")
        for chat_id, history in private_chat_histories.items():
            _log.info(f"   ç§èŠ {chat_id}: {len(history)} æ¡æ¶ˆæ¯")
        
        # ç›´æ¥ä½¿ç”¨api_serverçš„ä¿å­˜å‡½æ•°ï¼ˆå¯ä»¥è®¿é—®è¿è¡Œæ—¶çš„å…¨å±€å˜é‡ï¼‰
        # ä¸è¦ä½¿ç”¨training_serviceçš„ä¿å­˜å‡½æ•°ï¼Œå› ä¸ºå®ƒé€šè¿‡æ¨¡å—å¯¼å…¥è·å–ä¸åˆ°è¿è¡Œæ—¶çš„å…¨å±€å˜é‡
        saved_count = 0
        for chat_id, history in group_chat_histories.items():
            if history:
                try:
                    save_chat_history_to_storage("group", chat_id, history)
                    saved_count += len(history)
                    _log.info(f"âœ… ä¿å­˜ç¾¤èŠ {chat_id} çš„ {len(history)} æ¡æ¶ˆæ¯åˆ° {CHAT_HISTORY_STORAGE_DIR}")
                except Exception as e:
                    _log.error(f"ä¿å­˜ç¾¤èŠ {chat_id} å¤±è´¥: {e}", exc_info=True)
        
        for chat_id, history in private_chat_histories.items():
            if history:
                try:
                    save_chat_history_to_storage("private", chat_id, history)
                    saved_count += len(history)
                    _log.info(f"âœ… ä¿å­˜ç§èŠ {chat_id} çš„ {len(history)} æ¡æ¶ˆæ¯åˆ° {CHAT_HISTORY_STORAGE_DIR}")
                except Exception as e:
                    _log.error(f"ä¿å­˜ç§èŠ {chat_id} å¤±è´¥: {e}", exc_info=True)
        
        return jsonify({
            "success": True,
            "message": "èŠå¤©è®°å½•å·²ä¿å­˜",
            "storage_dir": CHAT_HISTORY_STORAGE_DIR,
            "stats": {
                "group_chats": group_count,
                "private_chats": private_count,
                "total_group_messages": total_group_messages,
                "total_private_messages": total_private_messages,
                "saved_messages": saved_count
            }
        }), 200
            
    except Exception as e:
        _log.error(f"ä¿å­˜èŠå¤©è®°å½•å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            "success": False,
            "error": str(e),
            "message": "ä¿å­˜èŠå¤©è®°å½•å¤±è´¥"
        }), 500


def upload_image():
    """
    æ¥æ”¶å®¢æˆ·ç«¯ä¸Šä¼ çš„å›¾ç‰‡ï¼ˆbase64ï¼‰å¹¶ä¿å­˜åˆ°æœ¬åœ°ï¼Œè¿”å›å¯è®¿é—®çš„URL
    """
    try:
        payload = request.get_json(force=True) or {}
        image_data = payload.get("data")
        image_format = str(payload.get("format", "jpeg")).lower().strip()

        if not image_data:
            return jsonify({"status": "error", "message": "ç¼ºå°‘å›¾ç‰‡æ•°æ®"}), 400

        # å…è®¸çš„æ ¼å¼æ˜ å°„
        format_map = {
            "jpg": "jpg",
            "jpeg": "jpg",
            "png": "png",
            "webp": "webp",
            "gif": "gif",
        }
        file_ext = format_map.get(image_format, "jpg")

        try:
            image_bytes = base64.b64decode(image_data, validate=True)
        except Exception as decode_err:
            _log.warning(f"å›¾ç‰‡Base64è§£ç å¤±è´¥: {decode_err}")
            return jsonify({"status": "error", "message": "å›¾ç‰‡æ•°æ®æ— æ•ˆ"}), 400

        # æ–‡ä»¶åä½¿ç”¨æ—¶é—´æˆ³+uuidï¼Œé¿å…å†²çª
        timestamp = datetime.utcnow().strftime("%Y%m%d%H%M%S%f")
        filename = f"{timestamp}_{uuid4().hex}.{file_ext}"
        file_path = os.path.join(IMAGE_UPLOAD_DIR, filename)

        with open(file_path, "wb") as f:
            f.write(image_bytes)

        file_url = url_for('serve_uploaded_image', filename=filename, _external=True)
        _log.info(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {file_path} -> {file_url}")

        return jsonify({
            "status": "success",
            "url": file_url,
            "filename": filename
        }), 200

    except Exception as e:
        _log.error(f"å›¾ç‰‡ä¸Šä¼ å¤±è´¥: {e}", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500


def handle_group_message():
    """
    å¤„ç†ç¾¤æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼šæ–‡å­—+å›¾ç‰‡ï¼‰
    å°†æ¶ˆæ¯æ”¾å…¥é˜Ÿåˆ—ï¼Œç”±å·¥ä½œçº¿ç¨‹å¤„ç†
    """
    global worker_thread_started
    
    try:
        data = request.json
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        group_id = str(data.get("group_id", ""))
        content = data.get("content", "")
        
        # ä»contentä¸­æå–CQå›¾ç‰‡ç ä¸­çš„URLï¼ˆç”¨äºéªŒè¯ï¼‰
        cleaned_content, image_urls = extract_cq_image_urls(content)
        
        if not group_id or (not cleaned_content and not image_urls):
            return jsonify({"status": "error", "message": "ç¼ºå°‘å¿…è¦å‚æ•°"}), 400
        
        # ç¡®ä¿å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨
        if not worker_thread_started:
            with queue_lock:
                if not worker_thread_started:
                    worker_thread = threading.Thread(target=message_queue_worker, daemon=True)
                    worker_thread.start()
                    worker_thread_started = True
                    _log.info("âœ… æ¶ˆæ¯é˜Ÿåˆ—å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨")
        
        # åˆ›å»ºå“åº”å­—å…¸ï¼ˆç”¨äºçº¿ç¨‹é—´é€šä¿¡ï¼‰
        response_dict = {}
        
        # åˆ›å»ºæ¶ˆæ¯ä»»åŠ¡
        task = MessageTask(
            chat_type="group",
            chat_id=group_id,
            data=data,
            response_dict=response_dict
        )
        
        # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
        message_queue.put(task)
        
        # ç­‰å¾…å¤„ç†å®Œæˆï¼ˆæœ€å¤šç­‰å¾…120ç§’ï¼Œä¸å®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´ä¸€è‡´ï¼‰
        # æ³¨æ„ï¼šå¦‚æœå¤šæ¡æ¶ˆæ¯æ’é˜Ÿï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
        timeout = 120
        start_time = time.time()
        while time.time() - start_time < timeout:
            if "status" in response_dict:
                # å¤„ç†å®Œæˆ
                status_code = response_dict.pop("status_code", 200)
                return jsonify(response_dict), status_code
            time.sleep(0.1)  # ç­‰å¾…100ms
        
        # è¶…æ—¶
        return jsonify({
            "status": "error",
            "message": "å¤„ç†è¶…æ—¶"
        }), 500
            
    except Exception as e:
        _log.error(f"å¤„ç†ç¾¤æ¶ˆæ¯å‡ºé”™: {e}", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500


def handle_private_message():
    """
    å¤„ç†ç§èŠæ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼šæ–‡å­—+å›¾ç‰‡ï¼‰
    å°†æ¶ˆæ¯æ”¾å…¥é˜Ÿåˆ—ï¼Œç”±å·¥ä½œçº¿ç¨‹å¤„ç†
    """
    global worker_thread_started
    
    try:
        data = request.json
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        user_id = str(data.get("user_id", ""))
        content = data.get("content", "")
        
        # ä»contentä¸­æå–CQå›¾ç‰‡ç ä¸­çš„URLï¼ˆç”¨äºéªŒè¯ï¼‰
        cleaned_content, image_urls = extract_cq_image_urls(content)
        
        if not user_id or (not cleaned_content and not image_urls):
            return jsonify({"status": "error", "message": "ç¼ºå°‘å¿…è¦å‚æ•°"}), 400
        
        # ç¡®ä¿å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨
        if not worker_thread_started:
            with queue_lock:
                if not worker_thread_started:
                    worker_thread = threading.Thread(target=message_queue_worker, daemon=True)
                    worker_thread.start()
                    worker_thread_started = True
                    _log.info("âœ… æ¶ˆæ¯é˜Ÿåˆ—å·¥ä½œçº¿ç¨‹å·²å¯åŠ¨")
        
        # åˆ›å»ºå“åº”å­—å…¸ï¼ˆç”¨äºçº¿ç¨‹é—´é€šä¿¡ï¼‰
        response_dict = {}
        
        # åˆ›å»ºæ¶ˆæ¯ä»»åŠ¡
        task = MessageTask(
            chat_type="private",
            chat_id=user_id,
            data=data,
            response_dict=response_dict
        )
        
        # å°†ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
        message_queue.put(task)
        
        # ç­‰å¾…å¤„ç†å®Œæˆï¼ˆæœ€å¤šç­‰å¾…120ç§’ï¼Œä¸å®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´ä¸€è‡´ï¼‰
        # æ³¨æ„ï¼šå¦‚æœå¤šæ¡æ¶ˆæ¯æ’é˜Ÿï¼Œå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
        timeout = 120
        start_time = time.time()
        while time.time() - start_time < timeout:
            if "status" in response_dict:
                # å¤„ç†å®Œæˆ
                status_code = response_dict.pop("status_code", 200)
                return jsonify(response_dict), status_code
            time.sleep(0.1)  # ç­‰å¾…100ms
        
        # è¶…æ—¶
        return jsonify({
            "status": "error",
            "message": "å¤„ç†è¶…æ—¶"
        }), 500
            
    except Exception as e:
        _log.error(f"å¤„ç†ç§èŠæ¶ˆæ¯å‡ºé”™: {e}", exc_info=True)
        return jsonify({"status": "error", "message": str(e)}), 500


if __name__ == "__main__":
    print("æœ¬æ–‡ä»¶ä¸å†ä½œä¸ºè¿è¡Œå…¥å£ã€‚è¯·ä½¿ç”¨ç»Ÿä¸€å…¥å£ï¼špython server/app.py")

