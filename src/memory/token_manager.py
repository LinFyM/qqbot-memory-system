# -*- coding: utf-8 -*-
"""
è®°å¿†ç›¸å…³ç‰¹æ®Štokenç®¡ç†å™¨
ç”¨äºæ·»åŠ å’Œç®¡ç†<recall>å’Œ</recall>ç‰¹æ®Štoken
"""

import torch
import logging
from transformers import AutoModelForCausalLM, AutoTokenizer

_log = logging.getLogger(__name__)


class MemoryTokenManager:
    """è®°å¿†ç›¸å…³ç‰¹æ®Štokenç®¡ç†å™¨"""
    
    def __init__(self, model, tokenizer):
        """
        åˆå§‹åŒ–tokenç®¡ç†å™¨
        
        Args:
            model: å·²åŠ è½½çš„æ¨¡å‹
            tokenizer: å·²åŠ è½½çš„åˆ†è¯å™¨ï¼ˆå¯èƒ½æ˜¯processoræˆ–tokenizerï¼‰
        """
        self.model = model
        self.tokenizer = tokenizer
        
        # è·å–çœŸæ­£çš„tokenizerï¼ˆå¦‚æœä¼ å…¥çš„æ˜¯processorï¼‰
        if hasattr(tokenizer, 'tokenizer'):
            self._actual_tokenizer = tokenizer.tokenizer
        else:
            self._actual_tokenizer = tokenizer
        
        # è¦æ·»åŠ çš„ç‰¹æ®Štokenï¼ˆåŒ…å«<recall>ã€</recall>å’Œ<|memory_pad|>ï¼‰
        self.special_tokens = ["<recall>", "</recall>", "<|memory_pad|>"]
        
        # å‚è€ƒtokenæ˜ å°„ï¼ˆç”¨äºåˆå§‹åŒ–æƒé‡ï¼‰
        # æ¯ä¸ªtokenå¯ä»¥æœ‰å¤šä¸ªå‚è€ƒtokenï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•ï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªå­˜åœ¨çš„
        # <recall>: ä¼˜å…ˆä½¿ç”¨"å›å¿†"ï¼Œå¤‡é€‰"æ€»ç»“"ã€"å›æƒ³"ã€"è®°å¿†"ç­‰
        # </recall>: ä¼˜å…ˆä½¿ç”¨"ç»“æŸ"ï¼Œå¤‡é€‰"å®Œæˆ"ã€"ç»ˆæ­¢"ã€"å®Œæ¯•"ç­‰
        # <|memory_pad|>: ä¸ä½¿ç”¨å‚è€ƒtokenï¼Œä½¿ç”¨å¹³å‡åˆå§‹åŒ–å¹¶ç¼©å°èŒƒæ•°ï¼ˆåªæ˜¯å ä½ç¬¦ï¼Œä¸éœ€è¦è®­ç»ƒï¼‰
        self.reference_tokens = {
            "<recall>": ["å›å¿†", "æ€»ç»“", "å›æƒ³", "è®°å¿†", "å›é¡¾", "æƒ³èµ·"],
            "</recall>": ["ç»“æŸ", "å®Œæˆ", "ç»ˆæ­¢", "å®Œæ¯•", "å®Œç»“", "åœæ­¢"],
            "<|memory_pad|>": []  # ç©ºåˆ—è¡¨è¡¨ç¤ºä¸ä½¿ç”¨å‚è€ƒtokenï¼Œä½¿ç”¨ç‰¹æ®Šåˆå§‹åŒ–
        }
    
    def check_and_add_tokens(self, perturbation_std=0.1):
        """
        æ£€æŸ¥å¹¶æ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        
        Args:
            perturbation_std: åˆå§‹åŒ–æƒé‡æ—¶çš„æ‰°åŠ¨æ ‡å‡†å·®ï¼ˆé»˜è®¤0.1ï¼Œè¾ƒå¤§çš„æ‰°åŠ¨ï¼‰
        
        Returns:
            dict: token_idæ˜ å°„ï¼Œå¦‚ {"<recall>": 123456, "</recall>": 123457}
        """
        _log.info("æ£€æŸ¥ç‰¹æ®Štoken...")
        
        # æ£€æŸ¥å“ªäº›tokenå·²å­˜åœ¨ï¼Œå“ªäº›éœ€è¦æ·»åŠ 
        tokens_to_add = []
        existing_token_ids = {}
        
        for token in self.special_tokens:
            token_id = self._actual_tokenizer.convert_tokens_to_ids(token)
            if token_id is None or token_id == self._actual_tokenizer.unk_token_id:
                tokens_to_add.append(token)
                _log.info(f"  {token} ä¸å­˜åœ¨ï¼Œéœ€è¦æ·»åŠ ")
            else:
                existing_token_ids[token] = token_id
                _log.info(f"  {token} å·²å­˜åœ¨ (ID: {token_id})")
        
        # å¦‚æœæ²¡æœ‰éœ€è¦æ·»åŠ çš„tokenï¼Œç›´æ¥è¿”å›
        if not tokens_to_add:
            _log.info("æ‰€æœ‰ç‰¹æ®Štokenå·²å­˜åœ¨ï¼Œæ— éœ€æ·»åŠ ")
            return existing_token_ids
        
        # æ·»åŠ æ–°token
        _log.info(f"æ·»åŠ  {len(tokens_to_add)} ä¸ªæ–°token...")
        original_vocab_size = len(self._actual_tokenizer)
        
        # å°†æ–°tokenæ³¨å†Œä¸ºçœŸæ­£çš„ç‰¹æ®Štokenï¼Œç¡®ä¿ä¿å­˜åå¯è¢«é‡æ–°åŠ è½½
        additional_specials = list(self._actual_tokenizer.special_tokens_map.get("additional_special_tokens", [])) if hasattr(self._actual_tokenizer, "special_tokens_map") else []
        updated_specials = []
        for token in self.special_tokens:
            if token in tokens_to_add or token in additional_specials:
                if token not in updated_specials:
                    updated_specials.append(token)
        if updated_specials:
            self._actual_tokenizer.add_special_tokens({"additional_special_tokens": updated_specials}, replace_additional_special_tokens=True)
        
        new_vocab_size = len(self._actual_tokenizer)
        _log.info(f"è¯è¡¨å¤§å°: {original_vocab_size} -> {new_vocab_size} (+{new_vocab_size - original_vocab_size})")
        
        # è°ƒæ•´æ¨¡å‹embeddingå±‚
        _log.info("è°ƒæ•´æ¨¡å‹embeddingå±‚...")
        self.model.resize_token_embeddings(len(self._actual_tokenizer))
        
        # éªŒè¯embeddingå±‚å’Œè¾“å‡ºå±‚çš„å¤§å°æ˜¯å¦æ­£ç¡®è°ƒæ•´
        input_embeddings = self.model.get_input_embeddings()
        input_emb_size = input_embeddings.weight.shape[0]
        _log.info(f"âœ… Input embeddingså¤§å°: {input_emb_size} (æœŸæœ›: {len(self._actual_tokenizer)})")
        
        # æ£€æŸ¥è¾“å‡ºå±‚ï¼ˆlm_headï¼‰
        output_embeddings = None
        if hasattr(self.model, 'lm_head'):
            output_embeddings = self.model.lm_head
        elif hasattr(self.model, 'get_output_embeddings'):
            output_embeddings = self.model.get_output_embeddings()
        
        if output_embeddings is not None:
            output_emb_size = output_embeddings.weight.shape[0]
            _log.info(f"âœ… Output embeddings (lm_head)å¤§å°: {output_emb_size} (æœŸæœ›: {len(self._actual_tokenizer)})")
            
            # æ£€æŸ¥inputå’Œoutput embeddingsæ˜¯å¦ç»‘å®šï¼ˆtiedï¼‰
            if input_embeddings.weight.data_ptr() == output_embeddings.weight.data_ptr():
                _log.info("â„¹ï¸ Inputå’ŒOutput embeddingsæ˜¯ç»‘å®šçš„ï¼ˆtiedï¼‰ï¼Œåªéœ€è°ƒæ•´ä¸€ä¸ªå³å¯")
            else:
                _log.info("â„¹ï¸ Inputå’ŒOutput embeddingsæ˜¯ç‹¬ç«‹çš„ï¼Œä¸¤è€…éƒ½å·²è°ƒæ•´")
        else:
            _log.warning("âš ï¸ æ¨¡å‹æ²¡æœ‰è¾“å‡ºå±‚ï¼ˆlm_headï¼‰ï¼Œå¯èƒ½æ— æ³•ç”Ÿæˆæ–°token")
        
        # è·å–æ‰€æœ‰tokençš„IDï¼ˆåŒ…æ‹¬æ–°æ·»åŠ çš„ï¼‰
        token_ids = {}
        for token in self.special_tokens:
            token_ids[token] = self._actual_tokenizer.convert_tokens_to_ids(token)
        
        _log.info(f"ç‰¹æ®Štoken IDs: {token_ids}")
        
        # éªŒè¯æ–°tokençš„IDæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        for token, token_id in token_ids.items():
            if token_id is None or token_id == self._actual_tokenizer.unk_token_id:
                _log.error(f"âŒ é”™è¯¯ï¼štoken '{token}' çš„IDæ— æ•ˆ: {token_id}")
            elif token_id >= input_emb_size:
                _log.error(f"âŒ é”™è¯¯ï¼štoken '{token}' çš„ID ({token_id}) è¶…å‡ºembeddingèŒƒå›´ ({input_emb_size})")
            else:
                _log.info(f"âœ… Token '{token}' IDéªŒè¯é€šè¿‡: {token_id} (åœ¨èŒƒå›´å†…: 0-{input_emb_size-1})")
        
        # åˆå§‹åŒ–æ–°æ·»åŠ tokençš„æƒé‡
        if tokens_to_add:
            self._initialize_token_weights(token_ids, tokens_to_add, perturbation_std)
            # å¦‚æœæ·»åŠ äº†tokenï¼Œä¿å­˜æ¨¡å‹
            self._save_model_with_tokens()
        
        return token_ids
    
    def _save_model_with_tokens(self):
        """
        ä¿å­˜æ·»åŠ äº†tokençš„æ¨¡å‹åˆ°æŒ‡å®šç›®å½•
        """
        import os
        import shutil
        from datetime import datetime
        
        # è·å–é…ç½®ä¸­çš„token_added_model_dir
        try:
            import yaml
            config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "config_qwen3vl.yaml")
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                token_added_model_dir = config.get("memory", {}).get("training", {}).get("token_added_model_dir", "./models/token_added")
            else:
                token_added_model_dir = "./models/token_added"
        except Exception as e:
            _log.warning(f"è¯»å–é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {e}")
            token_added_model_dir = "./models/token_added"
        
        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        if not os.path.isabs(token_added_model_dir):
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(script_dir))
            token_added_model_dir = os.path.abspath(os.path.join(project_root, token_added_model_dir))
        
        # åˆ›å»ºç›®å½•
        os.makedirs(token_added_model_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ·»åŠ äº†tokençš„æ¨¡å‹
        existing_models = [d for d in os.listdir(token_added_model_dir) 
                          if os.path.isdir(os.path.join(token_added_model_dir, d)) and d.startswith("model_")]
        
        if existing_models:
            # å¦‚æœå·²å­˜åœ¨ï¼Œä½¿ç”¨ç°æœ‰çš„æ¨¡å‹è·¯å¾„
            existing_models.sort(reverse=True)
            existing_model_path = os.path.join(token_added_model_dir, existing_models[0])
            _log.info(f"âœ… å·²å­˜åœ¨æ·»åŠ äº†tokençš„æ¨¡å‹: {existing_model_path}")
            return existing_model_path
        
        # åˆ›å»ºæ–°çš„æ¨¡å‹ç›®å½•ï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_dir_name = f"model_{timestamp}"
        save_path = os.path.join(token_added_model_dir, model_dir_name)
        
        _log.info(f"ğŸ’¾ ä¿å­˜æ·»åŠ äº†tokençš„æ¨¡å‹åˆ°: {save_path}")
        
        try:
            # ä¿å­˜æ¨¡å‹
            self.model.save_pretrained(save_path)
            # ä¿å­˜tokenizer
            self._actual_tokenizer.save_pretrained(save_path)
            
            # ä¿å­˜å®Œæ•´çš„processoré…ç½®ï¼ˆåŒ…å«image_processorã€video_processorç­‰æ‰€æœ‰ç»„ä»¶ï¼‰
            # ä»åŸºç¡€æ¨¡å‹åŠ è½½å®Œæ•´çš„processorï¼Œç„¶åæ›´æ–°tokenizerä¸ºæ·»åŠ äº†ç‰¹æ®Štokençš„ç‰ˆæœ¬
            try:
                from transformers import AutoProcessor
                # è·å–åŸºç¡€æ¨¡å‹è·¯å¾„
                base_model_path = getattr(self.model.config, "_name_or_path", None)
                if not base_model_path:
                    _log.warning("æ— æ³•ç¡®å®šåŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡processorä¿å­˜")
                else:
                    if not os.path.isabs(base_model_path):
                        script_dir = os.path.dirname(os.path.abspath(__file__))
                        project_root = os.path.dirname(os.path.dirname(script_dir))
                        base_model_path = os.path.abspath(os.path.join(project_root, base_model_path))
                    
                    if os.path.isdir(base_model_path):
                        # ä»åŸºç¡€æ¨¡å‹åŠ è½½å®Œæ•´çš„processor
                        base_processor = AutoProcessor.from_pretrained(
                            base_model_path,
                            trust_remote_code=True,
                            local_files_only=True
                        )
                        # æ›´æ–°processorçš„tokenizerä¸ºæ·»åŠ äº†ç‰¹æ®Štokençš„ç‰ˆæœ¬
                        base_processor.tokenizer = self._actual_tokenizer

                        # ä¿å­˜å®Œæ•´çš„processoré…ç½®
                        base_processor.save_pretrained(save_path)
                        _log.info(f"âœ… å·²ä¿å­˜å®Œæ•´Processoré…ç½®åˆ°: {save_path}")

                        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„é…ç½®æ–‡ä»¶éƒ½è¢«æ­£ç¡®ä¿å­˜ï¼ˆåœ¨save_pretrainedä¹‹åï¼Œç¡®ä¿ä¸è¢«è¦†ç›–ï¼‰
                        # è¿™äº›æ–‡ä»¶å¯¹äºQwen3VLProcessorçš„æ­£ç¡®å·¥ä½œè‡³å…³é‡è¦
                        import shutil
                        essential_files = [
                            "chat_template.json",
                            "preprocessor_config.json",
                            "video_preprocessor_config.json"
                        ]
                        for file_name in essential_files:
                            source_file = os.path.join(base_model_path, file_name)
                            target_file = os.path.join(save_path, file_name)
                            if os.path.exists(source_file):
                                try:
                                    shutil.copy2(source_file, target_file)
                                    _log.info(f"âœ… å·²å¤åˆ¶{file_name}åˆ°: {save_path}")
                                except Exception as e:
                                    _log.warning(f"âš ï¸ å¤åˆ¶{file_name}å¤±è´¥: {e}")
                            else:
                                _log.warning(f"âš ï¸ åŸºç¡€æ¨¡å‹ä¸­ä¸å­˜åœ¨{file_name}ï¼Œè·³è¿‡å¤åˆ¶")
                    else:
                        _log.warning(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}ï¼Œè·³è¿‡processorä¿å­˜")
            except Exception as proc_e:
                _log.warning(f"âš ï¸ ä¿å­˜Processoré…ç½®å¤±è´¥: {proc_e}ï¼Œå°†å°è¯•å¤åˆ¶æ–‡ä»¶")
                # å¦‚æœä¿å­˜processorå¤±è´¥ï¼Œè‡³å°‘å¤åˆ¶é¢å¤–çš„æ–‡ä»¶
                self._copy_additional_files(save_path)
            
            _log.info(f"âœ… æ¨¡å‹å’Œtokenizerå·²ä¿å­˜åˆ°: {save_path}")
            return save_path
        except Exception as e:
            _log.error(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}", exc_info=True)
            return None
    
    def _copy_additional_files(self, target_path: str):
        """
        å°†åŸºç¡€æ¨¡å‹ä¸­çš„é¢å¤–æ–‡ä»¶ï¼ˆchat_template.json ç­‰ï¼‰å¤åˆ¶åˆ°æ–°ç›®å½•
        """
        import shutil
        import os
        
        source_path = getattr(self.model.config, "_name_or_path", None)
        if not source_path:
            _log.warning("æ— æ³•ç¡®å®šåŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œè·³è¿‡é¢å¤–æ–‡ä»¶å¤åˆ¶")
            return
        
        if not os.path.isabs(source_path):
            # å°è¯•å°†ç›¸å¯¹è·¯å¾„è½¬ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(os.path.dirname(script_dir))
            candidate = os.path.abspath(os.path.join(project_root, source_path))
            if os.path.isdir(candidate):
                source_path = candidate
        if not os.path.isdir(source_path):
            _log.warning(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡é¢å¤–æ–‡ä»¶å¤åˆ¶: {source_path}")
            return
        
        extra_files = [
            "chat_template.json",
            "preprocessor_config.json",
            "video_preprocessor_config.json",
            "README.md"
        ]
        
        for filename in extra_files:
            src_file = os.path.join(source_path, filename)
            dst_file = os.path.join(target_path, filename)
            if os.path.exists(src_file):
                try:
                    shutil.copy2(src_file, dst_file)
                    _log.info(f"  âœ… å·²å¤åˆ¶ {filename} åˆ° {target_path}")
                except Exception as e:
                    _log.warning(f"  âš ï¸ å¤åˆ¶ {filename} å¤±è´¥: {e}")
    
    def _initialize_token_weights(self, token_ids, tokens_to_add, perturbation_std):
        """
        ä½¿ç”¨å‚è€ƒtokençš„embeddingåˆå§‹åŒ–æƒé‡ï¼Œå¹¶æ·»åŠ è¾ƒå¤§æ‰°åŠ¨
        
        ç­–ç•¥ï¼šä½¿ç”¨è¯­ä¹‰ç›¸è¿‘çš„token embeddingä½œä¸ºåŸºç¡€ï¼Œç„¶åæ·»åŠ éšæœºæ‰°åŠ¨
        - <recall>: ä½¿ç”¨"æ€»ç»“"tokençš„embedding + æ‰°åŠ¨
        - </recall>: ä½¿ç”¨"ç»“æŸ"tokençš„embedding + æ‰°åŠ¨
        
        Args:
            token_ids: æ‰€æœ‰ç‰¹æ®Štokençš„IDæ˜ å°„
            tokens_to_add: éœ€è¦åˆå§‹åŒ–æƒé‡çš„tokenåˆ—è¡¨
            perturbation_std: æ‰°åŠ¨æ ‡å‡†å·®ï¼ˆé»˜è®¤0.1ï¼Œè¾ƒå¤§çš„æ‰°åŠ¨ï¼‰
        """
        _log.info(f"åˆå§‹åŒ–tokenæƒé‡ï¼ˆä½¿ç”¨å‚è€ƒtoken embedding + æ‰°åŠ¨ï¼Œæ‰°åŠ¨æ ‡å‡†å·®={perturbation_std}ï¼‰...")
        
        # è·å–embeddingå±‚å’Œè¾“å‡ºå±‚
        embedding_layer = self.model.get_input_embeddings()
        
        # å°è¯•è·å–è¾“å‡ºå±‚ï¼ˆå¯èƒ½æ˜¯lm_headæˆ–é€šè¿‡get_output_embeddingsï¼‰
        output_layer = None
        if hasattr(self.model, 'lm_head'):
            output_layer = self.model.lm_head
        elif hasattr(self.model, 'get_output_embeddings'):
            output_layer = self.model.get_output_embeddings()
        
        if output_layer is None:
            _log.warning("æ¨¡å‹æ²¡æœ‰è¾“å‡ºå±‚ï¼ˆlm_headï¼‰ï¼Œåªåˆå§‹åŒ–embeddingå±‚")
        
        # è·å–æ¨¡å‹è®¾å¤‡
        model_device = next(self.model.parameters()).device
        
        # è®¡ç®—æ‰€æœ‰ç°æœ‰token embeddingçš„å¹³å‡å€¼ï¼ˆç”¨äºå‚è€ƒtokenä¸å­˜åœ¨æ—¶çš„åˆå§‹åŒ–ï¼‰
        old_vocab_size = len(self._actual_tokenizer) - len(tokens_to_add)
        avg_embedding = None
        avg_output = None
        
        try:
            # è®¡ç®—æ—§è¯æ±‡è¡¨çš„å¹³å‡embedding
            avg_embedding = embedding_layer.weight.data[:old_vocab_size].mean(dim=0, keepdim=False)
            _log.info(f"è®¡ç®—å¾—åˆ°å¹³å‡embeddingï¼ŒèŒƒæ•°={avg_embedding.norm().item():.4f}")
            
            # è®¡ç®—æ—§è¯æ±‡è¡¨çš„å¹³å‡output embedding
            if output_layer is not None:
                avg_output = output_layer.weight.data[:old_vocab_size].mean(dim=0, keepdim=False)
                _log.info(f"è®¡ç®—å¾—åˆ°å¹³å‡output embeddingï¼ŒèŒƒæ•°={avg_output.norm().item():.4f}")
        except Exception as e:
            _log.warning(f"è®¡ç®—å¹³å‡embeddingå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–")
        
        with torch.no_grad():
            for target_token in tokens_to_add:
                target_id = token_ids[target_token]
                
                # è·å–å‚è€ƒtokenåˆ—è¡¨
                ref_tokens = self.reference_tokens.get(target_token, [])
                
                # ç‰¹æ®Šå¤„ç†ï¼š<|memory_pad|>ä½¿ç”¨å¹³å‡åˆå§‹åŒ–å¹¶ç¼©å°èŒƒæ•°ï¼ˆåªæ˜¯å ä½ç¬¦ï¼Œä¸éœ€è¦è®­ç»ƒï¼‰
                if target_token == "<|memory_pad|>":
                    _log.info(f"  ğŸ”§ {target_token} ä½¿ç”¨å ä½ç¬¦åˆå§‹åŒ–ï¼ˆå¹³å‡åˆå§‹åŒ– + ç¼©å°èŒƒæ•°ï¼‰")
                    if avg_embedding is not None:
                        embedding_dim = avg_embedding.size(0)
                        # ä½¿ç”¨å¹³å‡embeddingï¼Œä½†ç¼©å°èŒƒæ•°åˆ°åŸæ¥çš„0.1å€ï¼ˆå¾ˆå°çš„èŒƒæ•°ï¼‰
                        embedding_vec = avg_embedding.clone() * 0.1
                        embedding_layer.weight.data[target_id] = embedding_vec
                        _log.info(f"    âœ… Input embeddingåˆå§‹åŒ–å®Œæˆï¼ŒèŒƒæ•°={embedding_vec.norm().item():.4f} (åŸå§‹å¹³å‡èŒƒæ•°çš„10%)")
                    else:
                        # å¦‚æœå¹³å‡embeddingè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å¾ˆå°çš„éšæœºåˆå§‹åŒ–
                        embedding_dim = embedding_layer.weight.size(1)
                        embedding_vec = torch.randn(embedding_dim, device=model_device) * 0.01  # å¾ˆå°çš„åˆå§‹åŒ–
                        embedding_layer.weight.data[target_id] = embedding_vec
                        _log.info(f"    âœ… Input embeddingéšæœºåˆå§‹åŒ–å®Œæˆï¼ŒèŒƒæ•°={embedding_vec.norm().item():.4f}")
                    
                    if output_layer is not None:
                        if avg_output is not None:
                            out_dim = avg_output.size(0)
                            # åŒæ ·ç¼©å°èŒƒæ•°
                            output_vec = avg_output.clone() * 0.1
                            output_layer.weight.data[target_id] = output_vec
                            _log.info(f"    âœ… Output embeddingåˆå§‹åŒ–å®Œæˆï¼ŒèŒƒæ•°={output_vec.norm().item():.4f} (åŸå§‹å¹³å‡èŒƒæ•°çš„10%)")
                        else:
                            out_dim = output_layer.weight.shape[1]
                            output_vec = torch.randn(out_dim, device=model_device) * 0.01
                            output_layer.weight.data[target_id] = output_vec
                            _log.info(f"    âœ… Output embeddingéšæœºåˆå§‹åŒ–å®Œæˆï¼ŒèŒƒæ•°={output_vec.norm().item():.4f}")
                    
                    _log.info(f"  âœ… {target_token} (ID: {target_id}) å ä½ç¬¦åˆå§‹åŒ–å®Œæˆï¼ˆä¸éœ€è¦è®­ç»ƒï¼‰")
                    continue
                
                if not ref_tokens:
                    _log.warning(f"  âš ï¸ {target_token} æ²¡æœ‰å‚è€ƒtokenï¼Œä½¿ç”¨å¹³å‡åˆå§‹åŒ–")
                    # ä½¿ç”¨å¹³å‡åˆå§‹åŒ–
                    if avg_embedding is not None:
                        embedding_dim = avg_embedding.size(0)
                        perturbation = torch.randn(embedding_dim, device=model_device) * perturbation_std
                        embedding_vec = avg_embedding.clone() + perturbation
                        embedding_layer.weight.data[target_id] = embedding_vec
                    else:
                        # å¦‚æœå¹³å‡embeddingè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–
                        embedding_dim = embedding_layer.weight.size(1)
                        init_std = getattr(getattr(self.model, "config", None), "initializer_range", 0.02)
                        embedding_vec = torch.randn(embedding_dim, device=model_device) * init_std
                        embedding_layer.weight.data[target_id] = embedding_vec
                    
                    if output_layer is not None:
                        if avg_output is not None:
                            out_dim = avg_output.size(0)
                            output_perturbation = torch.randn(out_dim, device=model_device) * perturbation_std
                            output_vec = avg_output.clone() + output_perturbation
                            output_layer.weight.data[target_id] = output_vec
                        else:
                            out_dim = output_layer.weight.shape[1]
                            init_std = getattr(getattr(self.model, "config", None), "initializer_range", 0.02)
                            output_vec = torch.randn(out_dim, device=model_device) * init_std
                            output_layer.weight.data[target_id] = output_vec
                    
                    init_method = "å¹³å‡åˆå§‹åŒ–" if avg_embedding is not None else "éšæœºåˆå§‹åŒ–"
                    _log.info(f"  âœ… {target_token} (ID: {target_id}) {init_method}å®Œæˆ")
                    continue
                
                # å°è¯•å¤šä¸ªå‚è€ƒtokenï¼Œæ”¶é›†æ‰€æœ‰æ‰¾åˆ°çš„token ID
                found_token_ids = []
                found_tokens = []
                
                for candidate_token in ref_tokens:
                    candidate_id = None
                    
                    # æ–¹æ³•1: å°è¯•ç›´æ¥convert_tokens_to_ids
                    try:
                        candidate_id = self._actual_tokenizer.convert_tokens_to_ids(candidate_token)
                        if candidate_id is not None and candidate_id != self._actual_tokenizer.unk_token_id:
                            found_token_ids.append(candidate_id)
                            found_tokens.append(candidate_token)
                            _log.info(f"  âœ… æ‰¾åˆ°å‚è€ƒtoken: '{candidate_token}' (ID: {candidate_id}) [æ–¹æ³•: convert_tokens_to_ids]")
                            continue
                    except Exception as e:
                        _log.debug(f"  convert_tokens_to_idså¤±è´¥: {e}")
                    
                    # æ–¹æ³•2: å°è¯•encodeç„¶åå–ç¬¬ä¸€ä¸ªtokenï¼ˆå¯¹äºä¸­æ–‡ï¼Œå¯èƒ½è¢«tokenizeæˆå¤šä¸ªtokenï¼‰
                    if candidate_id is None or candidate_id == self._actual_tokenizer.unk_token_id:
                        try:
                            encoded = self._actual_tokenizer.encode(candidate_token, add_special_tokens=False)
                            if encoded and len(encoded) > 0:
                                # å¯¹äºä¸­æ–‡tokenï¼Œå¯èƒ½è¢«tokenizeæˆå¤šä¸ªtokenï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¬ä¸€ä¸ª
                                candidate_id = encoded[0]
                                # éªŒè¯è¿™ä¸ªIDä¸æ˜¯unk_token_id
                                if candidate_id != self._actual_tokenizer.unk_token_id:
                                    found_token_ids.append(candidate_id)
                                    found_tokens.append(candidate_token)
                                    decoded = self._actual_tokenizer.decode([candidate_id])
                                    _log.info(f"  âœ… æ‰¾åˆ°å‚è€ƒtoken: '{candidate_token}' (ID: {candidate_id}, è§£ç : '{decoded}') [æ–¹æ³•: encode]")
                                    continue
                        except Exception as e:
                            _log.debug(f"  encodeå¤±è´¥: {e}")
                    
                    # æ–¹æ³•3: å°è¯•é€šè¿‡vocabç›´æ¥æŸ¥æ‰¾
                    if candidate_id is None or candidate_id == self._actual_tokenizer.unk_token_id:
                        try:
                            vocab = self._actual_tokenizer.get_vocab()
                            if candidate_token in vocab:
                                candidate_id = vocab[candidate_token]
                                if candidate_id != self._actual_tokenizer.unk_token_id:
                                    found_token_ids.append(candidate_id)
                                    found_tokens.append(candidate_token)
                                    _log.info(f"  âœ… æ‰¾åˆ°å‚è€ƒtoken: '{candidate_token}' (ID: {candidate_id}) [æ–¹æ³•: vocab]")
                                    continue
                        except Exception as e:
                            _log.debug(f"  vocabæŸ¥æ‰¾å¤±è´¥: {e}")
                    
                    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
                    if candidate_id is None or candidate_id == self._actual_tokenizer.unk_token_id:
                        try:
                            # å°è¯•tokenizeçœ‹çœ‹å®é™…ç»“æœ
                            tokenized = self._actual_tokenizer.tokenize(candidate_token)
                            _log.debug(f"  âš ï¸ å‚è€ƒtoken '{candidate_token}' tokenizeç»“æœ: {tokenized}")
                        except Exception:
                            pass
                
                if len(found_token_ids) == 0:
                    # æ‰€æœ‰å‚è€ƒtokenéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨å¹³å‡åˆå§‹åŒ–
                    ref_tokens_str = "ã€".join(ref_tokens)
                    _log.warning(f"  âš ï¸ æ‰€æœ‰å‚è€ƒtokenéƒ½ä¸å­˜åœ¨ï¼ˆ{ref_tokens_str}ï¼‰ï¼Œ{target_token} ä½¿ç”¨å¹³å‡åˆå§‹åŒ–")
                    # ä½¿ç”¨å¹³å‡åˆå§‹åŒ–
                    if avg_embedding is not None:
                        embedding_dim = avg_embedding.size(0)
                        perturbation = torch.randn(embedding_dim, device=model_device) * perturbation_std
                        embedding_vec = avg_embedding.clone() + perturbation
                        embedding_layer.weight.data[target_id] = embedding_vec
                    else:
                        # å¦‚æœå¹³å‡embeddingè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–
                        embedding_dim = embedding_layer.weight.size(1)
                        init_std = getattr(getattr(self.model, "config", None), "initializer_range", 0.02)
                        embedding_vec = torch.randn(embedding_dim, device=model_device) * init_std
                        embedding_layer.weight.data[target_id] = embedding_vec
                    
                    if output_layer is not None:
                        if avg_output is not None:
                            out_dim = avg_output.size(0)
                            output_perturbation = torch.randn(out_dim, device=model_device) * perturbation_std
                            output_vec = avg_output.clone() + output_perturbation
                            output_layer.weight.data[target_id] = output_vec
                        else:
                            out_dim = output_layer.weight.shape[1]
                            init_std = getattr(getattr(self.model, "config", None), "initializer_range", 0.02)
                            output_vec = torch.randn(out_dim, device=model_device) * init_std
                            output_layer.weight.data[target_id] = output_vec
                    
                    init_method = "å¹³å‡åˆå§‹åŒ–" if avg_embedding is not None else "éšæœºåˆå§‹åŒ–"
                    _log.info(f"  âœ… {target_token} (ID: {target_id}) {init_method}å®Œæˆï¼ˆæ‰€æœ‰å‚è€ƒtokenä¸å­˜åœ¨ï¼‰")
                    continue
                
                # ä½¿ç”¨æ‰€æœ‰æ‰¾åˆ°çš„å‚è€ƒtokençš„embeddingçš„å¹³å‡å€¼ä½œä¸ºåŸºç¡€
                ref_embeddings = []
                ref_outputs = []
                
                for ref_id in found_token_ids:
                    ref_embeddings.append(embedding_layer.weight.data[ref_id].clone())
                    if output_layer is not None:
                        ref_outputs.append(output_layer.weight.data[ref_id].clone())
                
                # è®¡ç®—å¹³å‡embedding
                base_embedding = torch.stack(ref_embeddings).mean(dim=0)
                embedding_dim = base_embedding.size(0)
                
                # æ·»åŠ è¾ƒå¤§æ‰°åŠ¨
                perturbation = torch.randn(embedding_dim, device=model_device) * perturbation_std
                embedding_vec = base_embedding + perturbation
                
                # å½’ä¸€åŒ–åˆ°ä¸å¹³å‡embeddingç›¸ä¼¼çš„èŒƒæ•°ï¼ˆå¯é€‰ï¼Œä¿æŒembeddingçš„å°ºåº¦ï¼‰
                base_norm = base_embedding.norm().item()
                new_norm = embedding_vec.norm().item()
                if base_norm > 0 and new_norm > 0:
                    embedding_vec = embedding_vec / new_norm * base_norm
                
                embedding_layer.weight.data[target_id] = embedding_vec
                
                # åŒæ ·å¤„ç†è¾“å‡ºå±‚
                if output_layer is not None and len(ref_outputs) > 0:
                    base_output = torch.stack(ref_outputs).mean(dim=0)
                    out_dim = base_output.size(0)
                    
                    # æ·»åŠ è¾ƒå¤§æ‰°åŠ¨
                    output_perturbation = torch.randn(out_dim, device=model_device) * perturbation_std
                    output_vec = base_output + output_perturbation
                    
                    # å½’ä¸€åŒ–åˆ°ä¸å¹³å‡outputç›¸ä¼¼çš„èŒƒæ•°
                    base_out_norm = base_output.norm().item()
                    new_out_norm = output_vec.norm().item()
                    if base_out_norm > 0 and new_out_norm > 0:
                        output_vec = output_vec / new_out_norm * base_out_norm
                    
                    output_layer.weight.data[target_id] = output_vec
                
                found_tokens_str = "ã€".join([f"'{t}'" for t in found_tokens])
                _log.info(f"  âœ… {target_token} (ID: {target_id}) åˆå§‹åŒ–å®Œæˆï¼ˆå‚è€ƒ: {found_tokens_str}ï¼Œå…±{len(found_token_ids)}ä¸ªtokençš„å¹³å‡å€¼ï¼Œæ‰°åŠ¨æ ‡å‡†å·®={perturbation_std}ï¼‰")
        
        _log.info("tokenæƒé‡åˆå§‹åŒ–å®Œæˆ")


