# -*- coding: utf-8 -*-
"""
è®°å¿†æ¡ç›®/å‘é‡æå–ç›¸å…³çš„è¾…åŠ©å‡½æ•°ã€‚
ä»åŸ MemoryTrainingService ä¸­æ‹†åˆ†ï¼Œæ–¹ä¾¿ç‹¬ç«‹ç»´æŠ¤ä¸æµ‹è¯•ã€‚
"""

from __future__ import annotations

import logging
import os
import json
import random
import re
from typing import List, Dict, Any, Tuple, Optional

import requests
from PIL import UnidentifiedImageError
import torch
from training.model_utils import forward_backbone, ensure_last_hidden_state

_log = logging.getLogger(__name__)


def _strip_formal_reply(generated_text: str) -> Tuple[str, bool, Optional[str]]:
    """
    ä»æ¨¡å‹è¾“å‡ºä¸­æˆªå–æœ€åä¸€ä¸ª </think>/<thinking> æ ‡ç­¾ä¹‹åçš„æ­£å¼å›ç­”ã€‚
    è¿”å› (trimmed_text, trimmed_flag, matched_tag)ã€‚
    """
    if not generated_text:
        return generated_text, False, None

    thinking_patterns = [
        r"</think\s*>",
        r"</thinking\s*>",
    ]
    last_match = None
    for pattern in thinking_patterns:
        matches = list(re.finditer(pattern, generated_text, flags=re.IGNORECASE))
        if matches:
            candidate = matches[-1]
            if last_match is None or candidate.end() > last_match.end():
                last_match = candidate

    if last_match:
        trimmed = generated_text[last_match.end():].strip()
        return trimmed, True, last_match.group(0).strip()

    return generated_text, False, None


def extract_memory_entries(
    service,
    chat_messages: List[Dict[str, Any]],
    model=None,
    processor=None,
) -> Optional[str]:
    """
    æå–è®°å¿†æ¡ç›®å¹¶ç”Ÿæˆç›‘ç£å‘é‡ï¼Œç›´æ¥ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶

    Args:
        service: MemoryTrainingService å®ä¾‹
        chat_messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨

    Returns:
        ä¸´æ—¶è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
    """
    self = service
    _log.info("å¼€å§‹æå–è®°å¿†æ¡ç›®...")

    # æŒ‰èŠå¤©åˆ†ç»„
    chat_groups = {}
    for msg_data in chat_messages:
        chat_type = msg_data.get("chat_type", "unknown")
        chat_id = msg_data.get("chat_id", "unknown")
        message = msg_data.get("message", {})

        key = f"{chat_type}_{chat_id}"
        if key not in chat_groups:
            chat_groups[key] = []
        chat_groups[key].append(message)

    _log.info(f"å…± {len(chat_groups)} ä¸ªèŠå¤©ç»„")

    # æ£€æŸ¥æ˜¯å¦æä¾›äº†æ¨¡å‹å’Œprocessor
    if model is None or processor is None:
        _log.error("âŒ extract_memory_entrieséœ€è¦æä¾›modelå’Œprocessorå‚æ•°")
        return None

    _log.info("ä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ¨¡å‹è¿›è¡Œè®°å¿†æå–")

    # ä»é…ç½®ä¸­è·å–æœ€å¤§tokené™åˆ¶ï¼ˆç”¨äºæ‰¹é‡æå–å‘é‡æ—¶çš„æˆªæ–­ï¼‰
    # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤å€¼ 35000
    max_tokens = self.training_config.get("max_tokens_for_embedding", 35000)
    _log.debug(f"ä½¿ç”¨æœ€å¤§tokené™åˆ¶: {max_tokens}ï¼ˆç”¨äºæ‰¹é‡æå–å‘é‡æ—¶çš„æˆªæ–­ï¼‰")

    # è§’è‰²è®¾å®šï¼ˆç”¨äºè®°å¿†æå–æ—¶æé†’æ¨¡å‹è‡ªå·±çš„èº«ä»½ï¼‰
    role_playing_prompt = ""
    extraction_prompts = getattr(self, "memory_extraction_prompts", {}) or {}
    try:
        role_playing_prompt = self.config.get("prompt", {}).get("role_playing", "")
        if role_playing_prompt:
            role_playing_prompt = role_playing_prompt.strip()
    except Exception:
        role_playing_prompt = ""

    # ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼ˆåªåŒ…å«è®°å¿†æ¡ç›®æ–‡æœ¬ï¼‰
    temp_texts_path = os.path.join(self.memory_db_dir, "temp_memory_texts.pt")
    # æ³¨æ„ï¼šä¿ç•™å·²æœ‰çš„ä¸´æ—¶æ–‡ä»¶ï¼Œæ–°çš„è®°å¿†æ¡ç›®å°†è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
    # è¿™å…è®¸åˆ†æ‰¹å¤„ç†èŠå¤©è®°å½•è€Œä¸ä¸¢å¤±ä¹‹å‰çš„ç»“æœ
    _log.debug(f"è®°å¿†æ¡ç›®å°†ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_texts_path}")

    try:

        # å¯¹æ¯ä¸ªèŠå¤©ç»„è¿›è¡Œæ€»ç»“ï¼ˆé€’å½’å¤„ç†ï¼Œæ”¯æŒå¯¹åŠåˆ†ï¼‰
        def process_chat_group(messages: List[Dict[str, Any]], chat_key: str, depth: int = 0):
            """
            å¤„ç†å•ä¸ªèŠå¤©ç»„ï¼ˆé€’å½’å‡½æ•°ï¼Œæ”¯æŒå¯¹åŠåˆ†ï¼‰
            """
            if not messages:
                return

            # æ„å»ºæ ‡å‡†æ ¼å¼çš„èŠå¤©å†å²ï¼ˆä¿ç•™å¤šæ¨¡æ€ä¿¡æ¯ï¼‰
            chat_messages_for_extraction = []
            for msg in messages:
                role = msg.get("role", "user")
                content = msg.get("content", "")  # é»˜è®¤å€¼ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä¸æ—§ç‰ˆæœ¬ä¿æŒä¸€è‡´

                # ä¿æŒåŸå§‹contentæ ¼å¼ï¼ˆå¯èƒ½æ˜¯listï¼ŒåŒ…å«å›¾ç‰‡ä¿¡æ¯ï¼‰
                if isinstance(content, list):
                    _log.debug(f"ğŸ” èŠå¤©ç»„ {chat_key} æ¶ˆæ¯ {role} çš„contentæ˜¯åˆ—è¡¨ï¼ŒåŒ…å« {len(content)} é¡¹")
                    # å¤šæ¨¡æ€å†…å®¹ï¼Œéœ€è¦éªŒè¯å›¾ç‰‡URLæ˜¯å¦æœ‰æ•ˆ
                    filtered_content = []
                    image_count = 0
                    valid_image_count = 0
                    for item in content:
                        if item.get("type") == "text":
                            filtered_content.append(item)
                        elif item.get("type") == "image":
                            image_url = item.get("image", "")
                            image_count += 1
                            if image_url:
                                if image_url.startswith('http://') or image_url.startswith('https://'):
                                    filtered_content.append(item)
                                    valid_image_count += 1
                                else:
                                    _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å›¾ç‰‡URLæ ¼å¼æ— æ•ˆï¼Œè·³è¿‡")
                            else:
                                _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å‘ç°æ— æ•ˆçš„å›¾ç‰‡é¡¹ï¼ˆæ— URLï¼‰ï¼Œè·³è¿‡")
                        elif item.get("type") == "video":
                            video_url = item.get("video") or item.get("url")
                            if not video_url:
                                _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å‘ç°æ— æ•ˆçš„è§†é¢‘é¡¹ï¼ˆæ— URLï¼‰ï¼Œè·³è¿‡")
                                continue

                            is_local_server_url = (
                                video_url.startswith('http://127.0.0.1:9999/static/videos/') or
                                video_url.startswith('http://localhost:9999/static/videos/') or
                                (self.server_base_url and video_url.startswith(f"{self.server_base_url.rstrip('/')}/static/videos/"))
                            )
                            is_local_file = os.path.exists(video_url) and os.path.isfile(video_url)
                            is_file_url = video_url.startswith('file://') and os.path.exists(video_url[7:])

                            _log.debug(f"ğŸ” è§†é¢‘URLæ£€æŸ¥: {video_url}")
                            _log.debug(f"  is_local_server_url: {is_local_server_url}")
                            _log.debug(f"  is_local_file: {is_local_file} (æ–‡ä»¶å­˜åœ¨: {os.path.exists(video_url) if video_url else False})")
                            _log.debug(f"  is_file_url: {is_file_url}")
                            _log.debug(f"  is_http: {video_url.startswith('http://') or video_url.startswith('https://') if video_url else False}")

                            if is_local_server_url or is_local_file or is_file_url or video_url.startswith('http://') or video_url.startswith('https://'):
                                filtered_content.append({
                                    "type": "video",
                                    "video": video_url
                                })
                                _log.info(f"âœ… ä¿ç•™è§†é¢‘: {video_url}")
                            else:
                                _log.warning(f"âš ï¸ ç§»é™¤æ— æ•ˆè§†é¢‘URL: {video_url}")

                    if filtered_content:
                        img_count = sum(1 for item in filtered_content if item.get("type") == "image")
                        vid_count = sum(1 for item in filtered_content if item.get("type") == "video")
                        if img_count > 0 or vid_count > 0:
                            _log.info(f"ğŸ“Š èŠå¤©ç»„ {chat_key} æ¶ˆæ¯åŒ…å« {img_count} å¼ å›¾ç‰‡å’Œ {vid_count} ä¸ªè§†é¢‘")
                            for item in filtered_content:
                                if item.get("type") == "image":
                                    _log.info(f"   ğŸ“· å›¾ç‰‡URL: {item.get('image', '')}")
                                elif item.get("type") == "video":
                                    _log.info(f"   ğŸ¥ è§†é¢‘URL: {item.get('video', '')}")
                        chat_messages_for_extraction.append({
                            "role": role,
                            "content": filtered_content
                        })
                    else:
                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} æ¶ˆæ¯è¿‡æ»¤åæ— å†…å®¹ï¼Œè·³è¿‡è¯¥æ¶ˆæ¯")
                elif isinstance(content, str):
                    chat_messages_for_extraction.append({
                        "role": role,
                        "content": [{"type": "text", "text": content}]
                    })
                else:
                    _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} æ¶ˆæ¯contentæ ¼å¼æœªçŸ¥: {type(content)}ï¼Œè·³è¿‡")

            extraction_system_prompt = _build_extraction_prompt(role_playing_prompt, extraction_prompts)

            if not chat_messages_for_extraction:
                _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å¤„ç†åæ— æœ‰æ•ˆå†…å®¹ï¼Œè·³è¿‡")
                return

            if len(chat_messages_for_extraction) > self.training_config.get("max_messages_per_group", 80):
                mid = len(messages) // 2
                _log.debug(f"âœ‚ï¸ èŠå¤©ç»„ {chat_key} æ¶ˆæ¯è¿‡é•¿ï¼Œæ‹†åˆ†ä¸ºä¸¤ä¸ªå­ç»„ï¼ˆæ·±åº¦ {depth + 1}ï¼‰")
                process_chat_group(messages[:mid], f"{chat_key}_part1", depth + 1)
                process_chat_group(messages[mid:], f"{chat_key}_part2", depth + 1)
                return

            try:
                _log.info(f"ğŸ§  å¼€å§‹å¤„ç†èŠå¤©ç»„ {chat_key}ï¼ˆæ·±åº¦ {depth}ï¼‰...")
                _log.info(f"   æ¶ˆæ¯æ¡æ•°: {len(chat_messages_for_extraction)}")

                if depth > 0:
                    child_prompt = self.training_config.get("child_depth_prompt")
                    if not child_prompt:
                        child_prompt = extraction_prompts.get("child_depth_prompt")
                    if child_prompt:
                        extraction_system_prompt += "\n\n" + child_prompt
                
                media_instruction = extraction_prompts.get("media_instruction")
                if media_instruction:
                    extraction_system_prompt += f"\n\n{media_instruction}"
                
                activation_instruction = self.training_config.get("memory_activation_prompt")
                if not activation_instruction:
                    activation_instruction = extraction_prompts.get("memory_activation_prompt")
                if activation_instruction:
                    extraction_system_prompt += f"\n\nè®°å¿†æ¿€æ´»æç¤ºï¼š{activation_instruction}"
                
                user_prompt = extraction_prompts.get("user_prompt", "è¯·å¼€å§‹æå–è®°å¿†æ¡ç›®ã€‚")

                full_messages = [
                    {"role": "system", "content": [{"type": "text", "text": extraction_system_prompt}]}
                ]
                full_messages.extend(chat_messages_for_extraction)
                full_messages.append({
                    "role": "user",
                    "content": [{"type": "text", "text": user_prompt}]
                })

                try:
                    inputs = processor.apply_chat_template(
                        full_messages,
                        tokenize=True,
                        add_generation_prompt=True,
                        return_dict=True,
                        return_tensors="pt"
                    )

                    input_ids_text = processor.batch_decode(
                        inputs["input_ids"],
                        skip_special_tokens=False,
                        clean_up_tokenization_spaces=False
                    )
                    _log.info("=" * 80)
                    _log.info("ğŸ”¤ æ¨¡å‹å®Œæ•´è¾“å…¥ï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰ï¼š")
                    _log.info(input_ids_text[0])
                    _log.info("=" * 80)
                except (UnidentifiedImageError, OSError, requests.RequestException, Exception) as media_error:
                    _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} å›¾ç‰‡/è§†é¢‘å¤„ç†å¤±è´¥: {media_error}")
                    _log.warning(f"   é”™è¯¯ç±»å‹: {type(media_error).__name__}", exc_info=True)
                    _log.info("   ğŸ”„ è‡ªåŠ¨é™çº§ï¼šç§»é™¤æ‰€æœ‰å›¾ç‰‡å’Œè§†é¢‘ï¼Œåªä½¿ç”¨æ–‡æœ¬å†…å®¹è¿›è¡Œè®°å¿†æå–...")

                    text_only_messages = []
                    for msg in full_messages:
                        msg_content = msg.get("content", [])
                        if isinstance(msg_content, list):
                            text_items = [item for item in msg_content if item.get("type") == "text"]
                            if text_items:
                                text_only_messages.append({
                                    "role": msg.get("role", "user"),
                                    "content": text_items
                                })
                        else:
                            text_only_messages.append(msg)

                    if not text_only_messages:
                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} ç§»é™¤å¤šæ¨¡æ€å†…å®¹åæ— æœ‰æ•ˆæ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†")
                        return

                    inputs = processor.apply_chat_template(
                        text_only_messages,
                        tokenize=True,
                        add_generation_prompt=True,
                        return_dict=True,
                        return_tensors="pt"
                    )

                input_length = inputs["input_ids"].shape[-1]
                _log.info(f"ğŸ“Š èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) è¾“å…¥tokené•¿åº¦: {input_length}, æœ€å¤§é™åˆ¶: {max_tokens}")

                if input_length > max_tokens:
                    if len(messages) <= 1:
                        _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} ä»…ä¸€æ¡æ¶ˆæ¯ä½†tokené•¿åº¦ {input_length} è¶…è¿‡é™åˆ¶ {max_tokens}ï¼Œè·³è¿‡å¤„ç†")
                        return

                    _log.warning(f"âš ï¸ èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) è¾“å…¥tokené•¿åº¦è¶…é™ï¼Œæ‹†åˆ†åŸå§‹æ¶ˆæ¯é‡æ–°å¤„ç†")
                    half_point = len(messages) // 2
                    process_chat_group(messages[:half_point], f"{chat_key}_part1", depth + 1)
                    process_chat_group(messages[half_point:], f"{chat_key}_part2", depth + 1)
                    return

                inputs = {k: v.to(model.device) if hasattr(v, "to") else v for k, v in inputs.items()}

                gen_config = self.config.get("generation", {})
                max_new_tokens = gen_config.get("max_new_tokens", 1000)
                temperature = gen_config.get("temperature", 1.0)
                top_p = gen_config.get("top_p", 0.95)
                top_k = gen_config.get("top_k", 20)
                repetition_penalty = gen_config.get("repetition_penalty", 1.0)

                _log.info(
                    "ğŸ¯ è®°å¿†æå–ç”Ÿæˆå‚æ•°: max_new_tokens=%s, temperature=%s, top_p=%s, top_k=%s, repetition_penalty=%s",
                    max_new_tokens,
                    temperature,
                    top_p,
                    top_k,
                    repetition_penalty,
                )

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=top_p,
                        top_k=top_k,
                        repetition_penalty=repetition_penalty,
                        do_sample=True,
                    )

                generated_text = processor.batch_decode(
                    outputs[:, inputs["input_ids"].shape[1]:],
                    skip_special_tokens=False,
                    clean_up_tokenization_spaces=False
                )[0]
                generated_text = generated_text.strip()

                generated_text, trimmed, matched_tag = _strip_formal_reply(generated_text)
                if trimmed:
                    tag_text = matched_tag or "</think>"
                    _log.info(f"âœ… ä»æ¨¡å‹è¾“å‡ºä¸­æå–åˆ°æ­£å¼å›ç­”ï¼ˆæˆªå–æœ€åä¸€ä¸ª{tag_text}ä¹‹åçš„å†…å®¹ï¼‰")
                else:
                    _log.warning("âš ï¸ æœªæ‰¾åˆ°</think>æˆ–</thinking>æ ‡ç­¾ï¼Œä½¿ç”¨å®Œæ•´è¾“å‡º")

                _log.info(f"ğŸ“ èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) æ¨¡å‹æ­£å¼å›ç­”ï¼ˆé•¿åº¦ {len(generated_text)}ï¼‰:")
                _log.info(generated_text)

                memory_texts = _parse_memory_entries(self, generated_text)

                _log.info(f"ğŸ“Š è§£æåæå–åˆ° {len(memory_texts)} ä¸ªè®°å¿†æ¡ç›®")
                if memory_texts:
                    for i, mem_text in enumerate(memory_texts, 1):
                        _log.info(f"   è®°å¿†æ¡ç›® {i}: {mem_text[:100]}...")

                for memory_text in memory_texts:
                    _append_memory_text_to_file(self, memory_text, temp_texts_path)
                    _log.info(f"âœ… æå–è®°å¿†æ¡ç›®æ–‡æœ¬ (æ·±åº¦ {depth}): {memory_text[:80]}...")

            except Exception as e:
                _log.warning(f"å¤„ç†èŠå¤©ç»„ {chat_key} (æ·±åº¦ {depth}) æ—¶å‡ºé”™: {e}", exc_info=True)
                return

        for chat_key, messages in chat_groups.items():
            process_chat_group(messages, chat_key)

        if not os.path.exists(temp_texts_path):
            _log.warning("âš ï¸ æ²¡æœ‰æå–åˆ°ä»»ä½•è®°å¿†æ¡ç›®")
            return None

        all_memory_texts = _load_memory_texts_from_file(self, temp_texts_path)
        if not all_memory_texts:
            _log.warning("âš ï¸ ä¸´æ—¶æ–‡ä»¶ä¸­æ²¡æœ‰è®°å¿†æ¡ç›®")
            return None

        _log.info(f"ğŸ“Š ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼šå…±æå– {len(all_memory_texts)} ä¸ªè®°å¿†æ¡ç›®æ–‡æœ¬")
        _log.info("=" * 60)
        _log.info("å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡æå–è®°å¿†æ¡ç›®å‘é‡")
        _log.info("=" * 60)

        all_texts, all_embeddings = _batch_extract_embeddings(self, all_memory_texts, model, processor, max_tokens)

        if all_texts and all_embeddings:
            _save_training_data_batch(self, all_texts, all_embeddings)
            _log.info(f"âœ… æˆåŠŸä¿å­˜ {len(all_texts)} ä¸ªè®°å¿†æ¡ç›®åŠå…¶å‘é‡åˆ°ä¸´æ—¶æ–‡ä»¶")

            try:
                if os.path.exists(temp_texts_path):
                    os.remove(temp_texts_path)
                    _log.info(f"âœ… å·²åˆ é™¤ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶: temp_memory_texts.pt")
            except Exception as e:
                _log.warning(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
        else:
            _log.warning("âŒ æ²¡æœ‰æˆåŠŸæå–åˆ°å‘é‡")
            return None

        temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")
        if os.path.exists(temp_data_path):
            data = torch.load(temp_data_path, map_location='cpu')
            total_entries = len(data.get('texts', []))
            _log.info(f"âœ… æˆåŠŸæå–å¹¶ä¿å­˜ {total_entries} ä¸ªè®°å¿†æ¡ç›®åˆ°ä¸´æ—¶æ–‡ä»¶")
        else:
            _log.warning("âŒ æ²¡æœ‰ç”Ÿæˆè®­ç»ƒæ•°æ®æ–‡ä»¶")
            return None

        return temp_data_path

    finally:
        if 'all_texts' in locals():
            del all_texts
        if 'all_embeddings' in locals():
            del all_embeddings

        temp_texts_path = os.path.join(self.memory_db_dir, "temp_memory_texts.pt")
        if os.path.exists(temp_texts_path):
            try:
                temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")
                if os.path.exists(temp_data_path):
                    os.remove(temp_texts_path)
                    _log.debug(f"æ¸…ç†ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶: temp_memory_texts.pt")
            except Exception as e:
                _log.debug(f"æ¸…ç†ä¸´æ—¶æ–‡æœ¬æ–‡ä»¶å¤±è´¥ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰: {e}")

        _log.info("âœ… è®°å¿†æå–å®Œæˆï¼ˆä½¿ç”¨ç»Ÿä¸€çš„è®­ç»ƒæ¨¡å‹ï¼‰")


def extract_sft_vectors_for_recall_training(
    service,
    num_memory_entries: int,
    model,
    processor
) -> Optional[str]:
    """
    æå–ç­‰é‡çš„SFTå‘é‡ç”¨äºç¬¬ä¸€æ­¥è®­ç»ƒ
    """
    self = service
    try:
        if not self.sft_enabled or not self.sft_path:
            _log.info("â„¹ï¸ SFTæœªå¯ç”¨æˆ–æœªé…ç½®ï¼Œè·³è¿‡SFTå‘é‡æå–")
            return None

        required_sft_count = int(num_memory_entries * 1.5)
        _log.info(f"ğŸ§ª å¼€å§‹æå– {required_sft_count} ä¸ªSFTå‘é‡ç”¨äºç¬¬ä¸€æ­¥è®­ç»ƒï¼ˆè®°å¿†æ¡ç›®æ•°: {num_memory_entries}ï¼‰")

        sft_samples = _load_sft_dataset(self)
        if not sft_samples:
            _log.warning("âš ï¸ æ— æ³•åŠ è½½SFTæ•°æ®é›†ï¼Œè·³è¿‡SFTå‘é‡æå–")
            return None

        max_tokens = int(service.training_config.get("sft_max_tokens") or 0)
        tokenizer = service._get_base_tokenizer(processor)
        sft_thinking_texts = []
        random.shuffle(sft_samples)
        processed = 0
        for sample in sft_samples:
            messages = _standardize_sft_messages(self, sample)
            if not messages:
                continue
            processed += 1
            try:
                full_text = processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False
                )

                start_tag = "<think>"
                end_tag = "</think>"
                start_idx = full_text.find(start_tag)
                end_idx = full_text.find(end_tag)

                if start_idx != -1 and end_idx != -1:
                    thinking_content = full_text[start_idx + len(start_tag):end_idx].strip()
                    if not thinking_content:
                        continue
                    if max_tokens:
                        encoded = tokenizer(
                            thinking_content,
                            return_tensors="pt",
                            add_special_tokens=True,
                            padding=False,
                            truncation=False
                        )
                        if encoded["input_ids"].shape[1] > max_tokens:
                            continue
                    sft_thinking_texts.append(thinking_content)
            except Exception as e:
                _log.debug(f"å¤„ç†SFTæ ·æœ¬å¤±è´¥: {e}")
                continue
            if len(sft_thinking_texts) >= required_sft_count:
                break

        if not sft_thinking_texts:
            _log.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„SFTæ€è€ƒå†…å®¹ï¼Œè·³è¿‡SFTå‘é‡æå–")
            return None

        if len(sft_thinking_texts) < required_sft_count:
            raise ValueError(
                f"SFTæ€è€ƒæ ·æœ¬ä¸è¶³ï¼šéœ€è¦ {required_sft_count} æ¡æ»¡è¶³tokené™åˆ¶çš„thinkingæ®µï¼Œ"
                f"ä½†ä»…æ”¶é›†åˆ° {len(sft_thinking_texts)} æ¡ã€‚"
            )

        _log.info(f"âœ… æå–åˆ° {len(sft_thinking_texts)} ä¸ªSFTæ€è€ƒå†…å®¹")

        sft_texts, sft_embeddings = _batch_extract_embeddings(
            self,
            sft_thinking_texts,
            model,
            processor,
            self.training_config.get("max_tokens_for_embedding", 35000)
        )

        if not sft_embeddings or len(sft_embeddings) < required_sft_count:
            _log.warning("âš ï¸ SFTå‘é‡æå–å¤±è´¥")
            return None

        sft_vectors_path = os.path.join(self.memory_db_dir, "temp_sft_vectors.pt")
        torch.save({
            "texts": sft_texts,
            "embeddings": torch.stack(sft_embeddings)
        }, sft_vectors_path)

        _log.info(f"âœ… å·²ä¿å­˜ {len(sft_embeddings)} ä¸ªSFTå‘é‡åˆ°ä¸´æ—¶æ–‡ä»¶: {sft_vectors_path}")
        return sft_vectors_path

    except Exception as e:
        _log.error(f"âŒ SFTå‘é‡æå–å¤±è´¥: {e}", exc_info=True)
        return None


# ----------------------- Helper Functions -----------------------

def _batch_extract_embeddings(self, memory_texts, model, processor, max_tokens):
    all_texts = []
    all_embeddings = []

    if isinstance(self.device, list):
        model_device = next(model.parameters()).device
    else:
        cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
        if cuda_visible and cuda_visible.strip():
            model_device = "cuda:0"
            _log.debug(f"ğŸ”§ æ‰¹é‡å‘é‡æå–: CUDA_VISIBLE_DEVICES={cuda_visible}ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„è®¾å¤‡ {model_device}ï¼ˆå¯¹åº”ç‰©ç†GPU {self.device}ï¼‰")
        else:
            model_device = self.device
            _log.debug(f"ğŸ”§ æ‰¹é‡å‘é‡æå–: ä½¿ç”¨è®¾å¤‡ {model_device}")

    batch_size = self.training_config.get("embedding_batch_size", 8)
    _log.info(f"ğŸ“¦ ä½¿ç”¨batch_size={batch_size}è¿›è¡Œæ‰¹é‡å‘é‡æå–")

    vectorization_prompts = getattr(self, "memory_vectorization_prompts", {}) or {}
    prompt_template = vectorization_prompts.get("summary_prompt_template")
    if not prompt_template:
        raise ValueError("memory_vectorization.summary_prompt_template æœªé…ç½®ï¼Œè¯·åœ¨ prompts.yaml ä¸­è®¾ç½®")

    prompts = []
    valid_indices = []
    for idx, memory_text in enumerate(memory_texts):
        if "{memory_text}" in prompt_template:
            prompt = prompt_template.replace("{memory_text}", memory_text)
        else:
            prompt = f"{prompt_template}{memory_text}"
        prompts.append(prompt)
        valid_indices.append(idx)

    total_batches = (len(prompts) + batch_size - 1) // batch_size
    _log.info(f"ğŸ“Š å…± {len(prompts)} ä¸ªè®°å¿†æ¡ç›®ï¼Œåˆ†ä¸º {total_batches} ä¸ªbatchå¤„ç†")
    # ç«‹å³åˆ·æ–°æ—¥å¿—ï¼Œç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°è¿›åº¦ä¿¡æ¯
    for handler in _log.handlers:
        if hasattr(handler, 'flush'):
            handler.flush()

    for batch_idx in range(0, len(prompts), batch_size):
        batch_num = batch_idx // batch_size + 1
        batch_prompts = prompts[batch_idx:batch_idx + batch_size]
        batch_texts = [memory_texts[valid_indices[batch_idx + i]] for i in range(len(batch_prompts))]

        _log.info(f"ğŸ”„ å¼€å§‹å¤„ç† Batch {batch_num}/{total_batches} (æ¡ç›® {batch_idx + 1}-{min(batch_idx + batch_size, len(prompts))}/{len(prompts)})")
        # ç«‹å³åˆ·æ–°æ—¥å¿—
        for handler in _log.handlers:
            if hasattr(handler, 'flush'):
                handler.flush()

        try:
            batch_inputs = processor.tokenizer(
                batch_prompts,
                truncation=True,
                max_length=max_tokens,
                padding=True,
                return_tensors="pt"
            )
            batch_inputs = {
                k: v.to(model_device) if isinstance(v, torch.Tensor) else v
                for k, v in batch_inputs.items()
            }

            with torch.no_grad():
                backbone_outputs = forward_backbone(
                    model,
                    input_ids=batch_inputs["input_ids"],
                    attention_mask=batch_inputs["attention_mask"],
                    use_cache=False,
                    output_hidden_states=False,
                    return_dict=True,
                )

            last_hidden_states = ensure_last_hidden_state(backbone_outputs)
            attention_mask = batch_inputs["attention_mask"]

            for i in range(len(batch_prompts)):
                last_token_idx = attention_mask[i].sum().item() - 1
                if last_token_idx < 0:
                    _log.warning(f"âš ï¸ Batch {batch_num} æ ·æœ¬ {i} çš„attention_maskæ— æ•ˆï¼Œè·³è¿‡")
                    continue

                embedding = last_hidden_states[i, last_token_idx, :].detach().cpu()
                all_texts.append(batch_texts[i])
                all_embeddings.append(embedding)

            processed = min(batch_idx + batch_size, len(prompts))
            _log.info(f"âœ… Batch {batch_num}/{total_batches} å®Œæˆ: å·²å¤„ç† {processed}/{len(prompts)} ä¸ªæ¡ç›®")
            # ç«‹å³åˆ·æ–°æ—¥å¿—
            for handler in _log.handlers:
                if hasattr(handler, 'flush'):
                    handler.flush()

            if (batch_idx // batch_size + 1) % 10 == 0 and torch.cuda.is_available():
                torch.cuda.empty_cache()
                _log.debug(f"ğŸ§¹ å·²æ¸…ç†GPUæ˜¾å­˜ï¼ˆå¤„ç†äº† {processed} ä¸ªæ¡ç›®ï¼‰")

        except Exception as e:
            _log.error(f"âŒ Batch {batch_idx//batch_size + 1} å¤„ç†å¤±è´¥: {e}", exc_info=True)
            _log.warning(f"ğŸ”„ å°è¯•é€ä¸ªå¤„ç†è¯¥batchä¸­çš„æ¡ç›®...")
            for i, memory_text in enumerate(batch_texts):
                try:
                    prompt = batch_prompts[i]
                    inputs = processor.tokenizer(
                        prompt,
                        truncation=True,
                        max_length=max_tokens,
                        return_tensors="pt"
                    )
                    inputs = {
                        k: v.to(model_device) if isinstance(v, torch.Tensor) else v
                        for k, v in inputs.items()
                    }

                    with torch.no_grad():
                        backbone_outputs = forward_backbone(
                            model,
                            input_ids=inputs["input_ids"],
                            attention_mask=inputs["attention_mask"],
                            use_cache=False,
                            output_hidden_states=False,
                            return_dict=True,
                        )

                    last_token_idx = inputs["attention_mask"].sum().item() - 1
                    if last_token_idx >= 0:
                        last_hidden = ensure_last_hidden_state(backbone_outputs)
                        embedding = last_hidden[0, last_token_idx, :].detach().cpu()
                        all_texts.append(memory_text)
                        all_embeddings.append(embedding)
                except Exception as single_e:
                    _log.warning(f"âš ï¸ å•ä¸ªæ¡ç›®å¤„ç†ä¹Ÿå¤±è´¥: {memory_text[:50]}... é”™è¯¯: {single_e}")
                    continue

    _log.info(f"âœ… æ‰¹é‡å‘é‡æå–å®Œæˆï¼šæˆåŠŸæå– {len(all_embeddings)}/{len(memory_texts)} ä¸ªå‘é‡")
    return all_texts, all_embeddings


def _append_memory_text_to_file(self, memory_text: str, file_path: str):
    try:
        if os.path.exists(file_path):
            existing_data = torch.load(file_path, map_location='cpu')
            existing_texts = existing_data.get('texts', [])
            existing_texts.append(memory_text)
        else:
            existing_texts = [memory_text]

        torch.save({"texts": existing_texts}, file_path)
    except Exception as e:
        _log.warning(f"è¿½åŠ è®°å¿†æ¡ç›®æ–‡æœ¬åˆ°æ–‡ä»¶å¤±è´¥: {e}")


def _load_memory_texts_from_file(self, file_path: str) -> List[str]:
    try:
        if not os.path.exists(file_path):
            return []

        data = torch.load(file_path, map_location='cpu')
        texts = data.get('texts', [])
        return texts
    except Exception as e:
        _log.error(f"ä»æ–‡ä»¶åŠ è½½è®°å¿†æ¡ç›®æ–‡æœ¬å¤±è´¥: {e}")
        return []


def _save_training_data_batch(self, texts: List[str], embeddings: List[torch.Tensor]):
    temp_data_path = os.path.join(self.memory_db_dir, "temp_training_data.pt")

    if not texts:
        return

    try:
        embeddings_tensor = torch.stack(embeddings)

        if os.path.exists(temp_data_path):
            existing_data = torch.load(temp_data_path, map_location='cpu')
            existing_texts = existing_data.get('texts', [])
            existing_embeddings = existing_data.get('embeddings')
            all_texts = existing_texts + texts
            all_embeddings = torch.cat([existing_embeddings, embeddings_tensor], dim=0)
        else:
            all_texts = texts
            all_embeddings = embeddings_tensor

        torch.save({
            "texts": all_texts,
            "embeddings": all_embeddings
        }, temp_data_path)

        _log.info(f"ä¿å­˜äº† {len(texts)} ä¸ªæ¡ç›®çš„è®­ç»ƒæ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆæ€»è®¡ {len(all_texts)} ä¸ªæ¡ç›®ï¼‰")

    except Exception as e:
        _log.error(f"ä¿å­˜è®­ç»ƒæ•°æ®æ‰¹æ¬¡å¤±è´¥: {e}")
        raise


def _load_sft_dataset(self) -> List[Dict[str, Any]]:
    dataset_path = self.sft_path
    if not dataset_path:
        _log.warning("âš ï¸ æœªé…ç½®SFTæ•°æ®é›†è·¯å¾„")
        return []
    if not os.path.isabs(dataset_path):
        dataset_path = os.path.abspath(os.path.join(self._project_root, dataset_path))
    if not os.path.exists(dataset_path):
        _log.warning(f"âš ï¸ SFTæ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        return []
    samples = []
    try:
        with open(dataset_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    samples.append(obj)
                except Exception:
                    continue
    except Exception as e:
        _log.warning(f"åŠ è½½SFTæ•°æ®é›†å¤±è´¥: {e}")
        return []
    _log.info(f"âœ… åŠ è½½SFTæ ·æœ¬: {len(samples)}")
    return samples


def _standardize_sft_messages(self, sample: Dict[str, Any]) -> List[Dict[str, Any]]:
    msgs = sample.get("messages")
    if isinstance(msgs, list) and msgs:
        std = []
        for m in msgs:
            role = m.get("role", "user")
            content = m.get("content") or m.get("text") or ""
            if isinstance(content, str):
                std.append({"role": role, "content": [{"type": "text", "text": content}]})
            elif isinstance(content, list):
                text_join = ""
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_join += item.get("text", "")
                    elif isinstance(item, str):
                        text_join += item
                std.append({"role": role, "content": [{"type": "text", "text": text_join}]})
        if std:
            return std
    inst = sample.get("instruction") or sample.get("input")
    out = sample.get("output") or sample.get("answer")
    if isinstance(inst, str) and isinstance(out, str):
        return [
            {"role": "user", "content": [{"type": "text", "text": inst}]},
            {"role": "assistant", "content": [{"type": "text", "text": out}]},
        ]
    q = sample.get("query") or sample.get("question")
    a = sample.get("response") or sample.get("answer")
    if isinstance(q, str) and isinstance(a, str):
        return [
            {"role": "user", "content": [{"type": "text", "text": q}]},
            {"role": "assistant", "content": [{"type": "text", "text": a}]},
        ]
    return []


def _parse_memory_entries(self, generated_text: str) -> List[str]:
    """
    è§£ææ¨¡å‹è¾“å‡ºï¼Œæå–æ ¼å¼åŒ–çš„è®°å¿†æ¡ç›®ã€‚ä¸¥æ ¼æŒ‰ç…§ v1.2 çš„è§£æé€»è¾‘ï¼Œé¿å…æŠŠæ€è€ƒå†…å®¹æˆ–æŒ‡ä»¤åŸæ–‡å½“æˆè®°å¿†ã€‚
    """
    if not generated_text or not generated_text.strip():
        return []

    text = generated_text.strip()
    text = re.sub(r"<\|[^>]+?\|>", "", text)
    text = re.sub(r"</?think>", "", text, flags=re.IGNORECASE)
    text = re.sub(r"</?thinking>", "", text, flags=re.IGNORECASE)
    text = re.sub(r"</?analysis>", "", text, flags=re.IGNORECASE)
    text = re.sub(r"</?reflect>", "", text, flags=re.IGNORECASE)
    text = text.replace("<no_reply>", "").replace("</no_reply>", "")
    # æ˜¾å¼æ‹’ç»â€œæ— è®°å¿†â€æç¤º
    if "æ— è®°å¿†æ¡ç›®" in text or "æ— è®°å¿†" in text[:20]:
        return []

    entries: List[str] = []

    # æ–¹æ³•1ï¼šé€è¡ŒåŒ¹é…â€œæ¡ç›®/ç¼–å·â€æ ¼å¼
    for line in text.splitlines():
        stripped = line.strip()
        if not stripped:
            continue
        match = re.match(r'^(?:æ¡ç›®\s*\d+|[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[\.ã€]|[-*])\s*[:ï¼š]?\s*(.+)', stripped)
        if match:
            candidate = match.group(1).strip()
            if candidate and len(candidate) > 3:
                entries.append(candidate)
        elif len(stripped) > 10:
            # å¤‡ç”¨ï¼šæ²¡æœ‰æ˜æ˜¾ç¼–å·ä½†çœ‹èµ·æ¥åƒäº‹å®é™ˆè¿°
            if any(keyword in stripped for keyword in ["å–œæ¬¢", "æ˜¯", "åœ¨", "æœ‰", "çš„", "äº†", "ä¼š", "è¦", "å»", "æ¥"]):
                entries.append(stripped)

    # æ–¹æ³•2ï¼šè‹¥ä»æ— æ¡ç›®ï¼ŒæŒ‰å¥å­åˆ‡åˆ†å¹¶è¿‡æ»¤æ‰æŒ‡ä»¤/æ€è€ƒæè¿°
    if not entries:
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) <= 5:
                continue
            if any(keyword in sentence for keyword in ["è¯·", "åˆ†æ", "å¯¹è¯", "å†…å®¹", "è®°å¿†æ¡ç›®", "ä»¥ä¸‹", "æ€è€ƒ", "æ­¥éª¤"]):
                continue
            entries.append(sentence)

    # å»é‡ã€å»å™ª
    seen = set()
    cleaned: List[str] = []
    for entry in entries:
        normalized = " ".join(entry.split())
        if len(normalized) < 3:
            continue
        if normalized in seen:
            continue
        if any(token in normalized for token in ["å¯¹è¯å†…å®¹", "æå–", "è¯·å¼€å§‹", "æ€è€ƒå†…å®¹"]):
            continue
        seen.add(normalized)
        cleaned.append(normalized)

    _log.debug(f"è§£æåçš„è®°å¿†æ¡ç›®: {cleaned}")
    return cleaned


def _build_extraction_prompt(role_prompt: str, extraction_prompts: Dict[str, Any]) -> str:
    prompts_cfg = extraction_prompts or {}
    base_prompt = prompts_cfg.get("system_prompt")
    if not base_prompt:
        raise ValueError("memory_extraction.system_prompt æœªé…ç½®ï¼Œè¯·åœ¨ prompts.yaml ä¸­è®¾ç½®")
    wrapper = prompts_cfg.get("role_prompt_wrapper")
    if role_prompt:
        if wrapper and "{role_prompt}" in wrapper:
            base_prompt += "\n\n" + wrapper.replace("{role_prompt}", role_prompt)
        else:
            base_prompt += f"\n\n{role_prompt}"
    return base_prompt.strip()

