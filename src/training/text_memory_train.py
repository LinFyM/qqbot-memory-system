import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.utils.data.distributed import DistributedSampler
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
import os
import shutil
import random
from tqdm import tqdm
import json
from datetime import datetime, timedelta
from typing import Tuple, Optional, Dict, List
from peft import LoraConfig, get_peft_model, TaskType
from modelscope import AutoModelForCausalLM, AutoTokenizer
from accelerate import Accelerator
from memory.utils import inject_memory_embedding_to_inputs_embeds


def _ensure_prompt_list(prompts, prompt_name: str):
    if isinstance(prompts, list) and len(prompts) > 0:
        return prompts
    raise ValueError(f"{prompt_name}ä¸èƒ½ä¸ºç©ºï¼Œè¯·åœ¨prompts.yamlä¸­é…ç½®")

def enhanced_collate_fn(batch):
    """ç®€åŒ–ç‰ˆcollateå‡½æ•° - æ”¯æŒæ–°çš„è®°å¿†è§£ç è®­ç»ƒæ ¼å¼å’ŒSFTæ•°æ®"""

    batch_size = len(batch)
    sample_types = [item.get('sample_type', 'unknown') for item in batch]

    # æ£€æŸ¥æ˜¯å¦æœ‰SFTæ ·æœ¬
    has_sft = any(item.get('is_sft', False) for item in batch)
    has_memory = any(not item.get('is_sft', False) for item in batch)

    # è®¡ç®—æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¾“å…¥+ç›®æ ‡ï¼‰
    max_input_len = max(len(item['sequence_tokens']) for item in batch)
    max_target_len = max(len(item['labels']) - len(item['sequence_tokens']) for item in batch)
    max_total_len = max_input_len + max_target_len

    # åˆå§‹åŒ–æ‰¹æ¬¡å¼ é‡
    input_ids = torch.zeros(batch_size, max_total_len, dtype=torch.long)
    attention_mask = torch.zeros(batch_size, max_total_len, dtype=torch.long)
    labels = torch.full((batch_size, max_total_len), -100, dtype=torch.long)

    # è®°å½•embeddingä¿¡æ¯ï¼ˆä»…ç”¨äºè®°å¿†æ¡ç›®ï¼‰
    embeddings_to_insert = []
    embedding_positions = []

    for i, item in enumerate(batch):
        input_tokens = item['sequence_tokens']
        item_labels = item['labels']
        input_len = len(input_tokens)
        is_sft = item.get('is_sft', False)

        if is_sft:
            # SFTæ ·æœ¬ï¼šç›´æ¥ä½¿ç”¨input_idså’Œlabelsï¼Œä¸éœ€è¦embeddingæ’å…¥
            total_len = len(input_tokens)
            input_ids[i, :total_len] = input_tokens
            attention_mask[i, :total_len] = 1
            labels[i, :len(item_labels)] = item_labels
            
            # SFTæ ·æœ¬ä½¿ç”¨å ä½ç¬¦embeddingï¼Œpositionè®¾ä¸º-1è¡¨ç¤ºä¸éœ€è¦æ’å…¥
            embeddings_to_insert.append(torch.zeros(1, 4096))  # å ä½ç¬¦
            embedding_positions.append(-1)  # -1è¡¨ç¤ºSFTæ ·æœ¬ï¼Œä¸éœ€è¦æ’å…¥
        else:
            # è®°å¿†æ¡ç›®æ ·æœ¬ï¼šåŸæœ‰é€»è¾‘
            target_labels = item_labels[len(input_tokens):]
            total_tokens = torch.cat([input_tokens, target_labels])
            total_len = len(total_tokens)

            # å¡«å……input_idså’Œattention_mask
            input_ids[i, :total_len] = total_tokens
            attention_mask[i, :total_len] = 1

            # ç›´æ¥ä½¿ç”¨é¢„å…ˆè®¡ç®—å¥½çš„labelsï¼ˆitem_labelså·²ç»æ˜¯tensorï¼‰
            labels[i, :len(item_labels)] = item_labels

            # å¤„ç†embeddingæ’å…¥ï¼ˆä½ç½®éœ€è¦è°ƒæ•´ï¼ŒåŠ ä¸Šè¾“å…¥é•¿åº¦çš„åç§»ï¼‰
            embeddings_to_insert.append(item['embedding_to_insert'])
            embedding_positions.append(item['embedding_position'])

    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'labels': labels,
        'embeddings_to_insert': torch.stack(embeddings_to_insert),
        'embedding_positions': torch.tensor(embedding_positions),
        'batch_info': {
            'batch_size': batch_size,
            'max_length': max_total_len,
            'has_sft': has_sft,
            'has_memory': has_memory,
            'sample_types': sample_types,
        }
    }

class EnhancedTextMemoryDataset(Dataset):
    """å¢å¼ºçš„æ–‡æœ¬è®°å¿†æ•°æ®é›† - æ¯ä¸ªembeddingå¯¹åº”å…¶è‡ªå·±çš„è®°å¿†æ–‡æœ¬"""
    
    def _get_tokenizer(self):
        """è·å–å®é™…çš„tokenizerå¯¹è±¡ï¼ˆå¤„ç†Qwen3VLProcessorçš„æƒ…å†µï¼‰"""
        if hasattr(self.tokenizer, 'tokenizer'):
            return self.tokenizer.tokenizer
        else:
            return self.tokenizer

    def __init__(
        self,
        texts,
        embeddings,
        tokenizer,
        base_model,
        max_length=3000,
        noise_std=0.01,
        is_main_process_fn=None,
        sft_full_texts=None,
        activation_prompts=None,
        end_prompts=None,
        guide_text=None,
    ):
        self.texts = texts
        self.embeddings = embeddings
        self.tokenizer = tokenizer
        self.base_model = base_model
        self.max_length = max_length
        self.noise_std = noise_std
        self._is_main_process_fn = is_main_process_fn
        # SFTå®Œæ•´æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«å®Œæ•´æ–‡æœ¬å’Œæ€è€ƒéƒ¨åˆ†çš„èµ·æ­¢ä½ç½®
        # æ ¼å¼: [{"full_text": "...", "thinking_start": int, "thinking_end": int}, ...]
        self.sft_full_texts = sft_full_texts if sft_full_texts is not None else []
        self.activation_prompts = _ensure_prompt_list(activation_prompts, "activation_prompts")
        self.end_prompts = _ensure_prompt_list(end_prompts, "end_prompts")
        self.guide_text = guide_text or ""
        
        # è·å–æ¨¡å‹å‚æ•°
        first_param = next(base_model.parameters())
        self.model_dtype = first_param.dtype
        self.model_device = first_param.device
        print(f"ğŸ”§ æ£€æµ‹åˆ°æ¨¡å‹æ•°æ®ç±»å‹: {self.model_dtype}, è®¾å¤‡: {self.model_device}")

        # æ³¨æ„ï¼šä¸åœ¨__init__ä¸­é¢„å…ˆç§»åŠ¨æ‰€æœ‰embeddingsåˆ°GPUï¼Œé¿å…æ˜¾å­˜ç´¯ç§¯
        # åªåœ¨__getitem__ä¸­æŒ‰éœ€ç§»åŠ¨å•ä¸ªembedding
        print(f"ğŸ“Š embeddingsä¿æŒåœ¨CPUä¸Šï¼Œè®­ç»ƒæ—¶æŒ‰éœ€ç§»åŠ¨: {self.embeddings.shape}")
        
        # è·å–ç‰¹æ®Štoken ID
        self.recall_start_token = '<recall>'
        self.recall_end_token = '</recall>'
        self.im_start_token = '<|im_start|>'
        # å¼•å¯¼æ–‡å­—ï¼ˆåœ¨</recall>ä¹‹åï¼‰
        self.guide_text = self.guide_text or ""
        
        # è·å–å®é™…çš„tokenizerï¼ˆå¤„ç†Qwen3VLProcessorçš„æƒ…å†µï¼‰
        actual_tokenizer = self._get_tokenizer()

        self.recall_start_id = actual_tokenizer.convert_tokens_to_ids(self.recall_start_token)
        self.recall_end_id = actual_tokenizer.convert_tokens_to_ids(self.recall_end_token)
        self.im_start_id = actual_tokenizer.convert_tokens_to_ids(self.im_start_token)
        self.memory_pad_id = actual_tokenizer.convert_tokens_to_ids("<|memory_pad|>")
        
        # éªŒè¯ç‰¹æ®Štokenï¼ˆç§»é™¤<|recall|>çš„æ£€æŸ¥ï¼‰
        if any(token_id == actual_tokenizer.unk_token_id for token_id in
               [self.recall_start_id, self.recall_end_id, self.memory_pad_id]):
            raise ValueError("ç‰¹æ®Štokenä¸å­˜åœ¨ï¼")
        
        # åˆå§‹åŒ–æ•°æ®é…å¯¹ - ä¼šåœ¨æ¯ä¸ªepochå¼€å§‹æ—¶åˆ·æ–°
        self.refresh_epoch_data()
        
        print(f"âœ… å¢å¼ºæ•°æ®é›†åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç‰¹æ®Štoken IDs: start={self.recall_start_id}, end={self.recall_end_id}")
        print(f"   å¼•å¯¼æ–‡å­—: {self.guide_text}")
        print(f"   åŸå§‹æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"   æ€»è®­ç»ƒæ ·æœ¬æ•°: {self.total_samples}")
    
    def is_main_process(self):
        """åˆ¤æ–­å½“å‰æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼Œç”¨äºå¤šGPU/åˆ†å¸ƒå¼åœºæ™¯ä¸‹çš„æ—¥å¿—æ§åˆ¶"""
        # ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„å‡½æ•°ï¼ˆä¾‹å¦‚Trainerçš„is_main_processï¼‰
        if callable(self._is_main_process_fn):
            try:
                return bool(self._is_main_process_fn())
            except Exception:
                pass
        # å¦‚æœä½¿ç”¨torch.distributedï¼Œåˆ¤æ–­rank
        try:
            if dist.is_available() and dist.is_initialized():
                return dist.get_rank() == 0
        except Exception:
            pass
        # é»˜è®¤è®¤ä¸ºæ˜¯ä¸»è¿›ç¨‹
        return True

    def refresh_epoch_data(self):
        """æ¯ä¸ªepochå¼€å§‹æ—¶åˆ·æ–°æ•°æ® - ç®€åŒ–ç‰ˆ"""
        num_texts = len(self.texts)

        # æ¯ä¸ªembeddingåªå¯¹åº”è‡ªå·±çš„è®°å¿†æ–‡æœ¬
        self.text_indices = list(range(num_texts))
        self.total_samples = num_texts

        if self.is_main_process():
            print(f"âœ… æ•°æ®åˆ·æ–°å®Œæˆ: {self.total_samples} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    def __len__(self):
        return self.total_samples
    
    def __getitem__(self, idx):
        """è·å–è®­ç»ƒæ ·æœ¬ - æ¯ä¸ªembeddingå¯¹åº”å…¶è‡ªå·±çš„è®°å¿†æ–‡æœ¬"""
        text_idx = self.text_indices[idx]
        return self._get_memory_decode_sample(text_idx)
    
    def _split_sft_at_thinking(self, sft_data: dict) -> Tuple[str, str]:
        """
        åœ¨SFTå®Œæ•´æ–‡æœ¬çš„æ€è€ƒéƒ¨åˆ†å†…éƒ¨éšæœºæˆªæ–­ï¼Œä¼˜å…ˆåœ¨å¥å·åé¢æˆªæ–­
        è¿”å›(æˆªæ–­å‰æ–‡æœ¬, æˆªæ–­åæ–‡æœ¬)
        """
        import random
        
        full_text = sft_data["full_text"]
        thinking_start = sft_data["thinking_start"]
        thinking_end = sft_data["thinking_end"]
        
        start_tag = "<think>"
        end_tag = "</think>"
        thinking_content = full_text[thinking_start + len(start_tag):thinking_end - len(end_tag)]
        
        if not thinking_content.strip():
            prefix_text = full_text[:thinking_start]
            suffix_text = full_text[len(prefix_text):]
            return prefix_text.strip(), suffix_text.strip()
        
        actual_tokenizer = self._get_tokenizer()
        thinking_tokens = actual_tokenizer(thinking_content, add_special_tokens=False)['input_ids']
        
        if len(thinking_tokens) <= 1:
            prefix_text = full_text[:thinking_start]
            suffix_text = full_text[len(prefix_text):]
            return prefix_text.strip(), suffix_text.strip()
        
        max_truncate_pos = len(thinking_tokens) - 1
        if max_truncate_pos <= 0:
            prefix_text = full_text[:thinking_start]
            suffix_text = full_text[len(prefix_text):]
            return prefix_text.strip(), suffix_text.strip()
        
        sentence_end_tokens = []
        for i, token_id in enumerate(thinking_tokens):
            try:
                token_text = actual_tokenizer.decode([token_id], skip_special_tokens=True)
                if any(punct in token_text for punct in ['ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?', 'ï¼›', ';']):
                    sentence_end_tokens.append(i + 1)
            except Exception:
                pass
        
        if sentence_end_tokens:
            truncate_pos = random.choice(sentence_end_tokens)
            truncate_pos = min(truncate_pos, max_truncate_pos)
        else:
            truncate_pos = random.randint(1, max_truncate_pos)
        
        truncated_thinking_tokens = thinking_tokens[:truncate_pos]
        truncated_thinking_text = actual_tokenizer.decode(truncated_thinking_tokens, skip_special_tokens=True)
        
        truncated_text_raw = (
            full_text[:thinking_start + len(start_tag)] +
            truncated_thinking_text
        )
        prefix_text = truncated_text_raw.strip()
        suffix_start = len(truncated_text_raw)
        if suffix_start > len(full_text):
            suffix_start = len(full_text)
        suffix_text = full_text[suffix_start:]
        return prefix_text, suffix_text.strip()
    
    def _get_memory_decode_sample(self, text_idx, context_override: Optional[Dict[str, str]] = None):
        """æ„é€ è®°å¿†è§£ç è®­ç»ƒæ ·æœ¬

        æ­£ç¡®æ ¼å¼ï¼š
        è¾“å…¥ï¼šéšæœºä¸Šä¸‹æ–‡ + "<recall>" + [embeddingå‘é‡]
        ç›®æ ‡ï¼š-100æ ‡ç­¾ * ä¸Šä¸‹æ–‡é•¿åº¦ + è®°å¿†æ–‡æœ¬å†…å®¹ + "</recall>" + å¼•å¯¼æ–‡å­—

        æ¨¡å‹å­¦ä¹ ï¼šçœ‹åˆ°<recall> + ç‰¹å®šembeddingæ—¶ï¼Œç”Ÿæˆå¯¹åº”çš„è®°å¿†å†…å®¹ï¼Œå¿½ç•¥ä¸Šä¸‹æ–‡å¹²æ‰°
        """
        text = self.texts[text_idx]
        embedding = self.embeddings[text_idx]
        
        # æ·»åŠ å™ªå£°åˆ°embeddingï¼ˆå¯é€‰ï¼Œç”¨äºæ•°æ®å¢å¼ºï¼‰
        if self.noise_std > 0:
            noise = torch.randn_like(embedding) * self.noise_std
            noisy_embedding = embedding + noise
        else:
            noisy_embedding = embedding.clone()

        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®ï¼Œè®¾å¤‡åˆ†é…ç”±Acceleratorå¤„ç†
        noisy_embedding = noisy_embedding.to(self.model_dtype)

        # ===== æ·»åŠ éšæœºä¸Šä¸‹æ–‡å¹²æ‰° =====
        # ä¼˜å…ˆä»SFTå®Œæ•´æ–‡æœ¬ä¸­éšæœºé€‰æ‹©å¹¶åœ¨æ€è€ƒéƒ¨åˆ†å†…éƒ¨æˆªæ–­ï¼Œå¦‚æœæ²¡æœ‰SFTæ•°æ®åˆ™ä»è®°å¿†æ¡ç›®ä¸­é€‰æ‹©
        # æ¯ä¸ªè®­ç»ƒæ ·æœ¬éƒ½é‡æ–°éšæœºé€‰æ‹©ä¸Šä¸‹æ–‡ï¼Œç¡®ä¿æ¯ä¸ªepochçš„ä¸Šä¸‹æ–‡éƒ½ä¸åŒ
        import random
        context_tokens = []
        context_idx = None
        context_text = ""
        tail_text = ""
        actual_tokenizer = self._get_tokenizer()
        override = context_override or {}
        if override.get("prefix_text"):
            context_text = override.get("prefix_text", "")
            tail_text = override.get("suffix_text", "")
        if not context_text:
            if len(self.sft_full_texts) > 0:
                sft_data = random.choice(self.sft_full_texts)
                prefix_text, _ = self._split_sft_at_thinking(sft_data)
                context_text = prefix_text
            if not context_text and len(self.texts) > 1:
                other_indices = [i for i in range(len(self.texts)) if i != text_idx]
                context_idx = random.choice(other_indices)
                context_text = self.texts[context_idx]
        if context_text:
            context_tokens = actual_tokenizer(context_text, add_special_tokens=False)['input_ids']
        # å¦‚æœåªæœ‰ä¸€ä¸ªè®°å¿†æ¡ç›®ä¸”æ²¡æœ‰SFTæ•°æ®ï¼Œåˆ™æ²¡æœ‰ä¸Šä¸‹æ–‡ï¼ˆcontext_tokenså·²åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨ï¼‰

        # ===== æ„é€ æ ¸å¿ƒè®­ç»ƒåºåˆ— =====
        activation_prompt = random.choice(self.activation_prompts).strip()
        end_prompt = random.choice(self.end_prompts).strip()
        activation_tokens = actual_tokenizer(activation_prompt, add_special_tokens=False)['input_ids'] if activation_prompt else []

        # æ„é€ ç›®æ ‡æ–‡æœ¬ï¼šè®°å¿†å†…å®¹ + </recall> + ç»“æŸå¼•å¯¼
        target_text = f"{text}{self.recall_end_token}{end_prompt}"
        if tail_text:
            tail_text_clean = tail_text.strip()
            if tail_text_clean:
                separator = "" if target_text.endswith("\n") else "\n"
                target_text = f"{target_text}{separator}{tail_text_clean}"
        target_tokens = actual_tokenizer(target_text, add_special_tokens=False)['input_ids']

        # å°†<recall>ç¼–ç ä¸ºtoken
        recall_tokens = actual_tokenizer(self.recall_start_token, add_special_tokens=False)['input_ids']
        recall_token_count = len(recall_tokens)

        # æ„é€ æ ¸å¿ƒè¾“å…¥åºåˆ—ï¼š<recall> + <|memory_pad|>
        core_input_tokens = (
            recall_tokens +  # <recall>æ ‡ç­¾
            [self.memory_pad_id]  # <|memory_pad|> tokenï¼Œå°†è¢«å‘é‡æ›¿æ¢
        )

        # ===== æ„é€ å®Œæ•´åºåˆ— =====
        base_input_len = len(context_tokens) + len(activation_tokens) + len(core_input_tokens)

        if self.max_length is not None:
            total_length = base_input_len + len(target_tokens)
        if total_length > self.max_length:
                # é¢„ç•™æ ¸å¿ƒè¾“å…¥ï¼ˆ<recall> + <|memory_pad|>ï¼‰
                min_input_len = len(activation_tokens) + len(core_input_tokens)
                available_input_len = self.max_length - len(target_tokens)
                if available_input_len < min_input_len:
                    # æ— æ³•å®¹çº³å…¨éƒ¨ç›®æ ‡æ–‡æœ¬ï¼Œæˆªæ–­ç›®æ ‡æ–‡æœ¬å¹¶ä¿ç•™æ ¸å¿ƒè¾“å…¥
                    available_input_len = min_input_len
                    max_target_len = max(self.max_length - available_input_len, 1)
                    target_tokens = target_tokens[:max_target_len]
                # åªä¿ç•™ä¸‹æ–‡ï¼Œç¡®ä¿æ ¸å¿ƒåºåˆ—å­˜åœ¨
                allowed_context_len = max(0, available_input_len - min_input_len)
                if len(context_tokens) > allowed_context_len:
                    context_tokens = context_tokens[:allowed_context_len]
                base_input_len = allowed_context_len + min_input_len

        full_input_tokens = context_tokens + activation_tokens + core_input_tokens
        prefix_len = len(full_input_tokens)
        prefix_labels = [-100] * prefix_len
        recall_start_idx = len(context_tokens) + len(activation_tokens)
        for offset, token_id in enumerate(recall_tokens):
            pos = recall_start_idx + offset
            if 0 <= pos < prefix_len:
                prefix_labels[pos] = token_id
        recall_label_slice = prefix_labels[recall_start_idx:recall_start_idx + recall_token_count]
        if len(recall_label_slice) != recall_token_count or any(label == -100 for label in recall_label_slice):
            raise RuntimeError(
                f"âŒ <recall>æ ‡ç­¾æœªæ­£ç¡®è®¾ç½®ï¼Œä½ç½®[{recall_start_idx}, {recall_start_idx + recall_token_count}) "
                f"labels={recall_label_slice}"
            )
        full_target_tokens = prefix_labels + target_tokens

        # è®¡ç®—embeddingæ’å…¥ä½ç½®ï¼ˆä¸Šä¸‹æ–‡ + æ¿€æ´»è¯­ + <recall> ä¹‹åï¼‰
        embedding_position = len(context_tokens) + len(activation_tokens) + recall_token_count

        # æœ€ç»ˆæ ‡ç­¾
        labels = full_target_tokens

        sample = {
            'sequence_tokens': torch.tensor(full_input_tokens, dtype=torch.long),
            'embedding_to_insert': noisy_embedding,
            'embedding_position': embedding_position,
            'labels': torch.tensor(labels, dtype=torch.long),
            'recall_token_count': recall_token_count,
            'text': text,
            'text_idx': text_idx,
            'context_text': context_text,  # ç°åœ¨å¯èƒ½æ˜¯SFTæ€è€ƒæ–‡æœ¬ï¼ˆæˆªæ–­åï¼‰æˆ–è®°å¿†æ¡ç›®æ–‡æœ¬
            'context_length': len(context_tokens),
            'activation_prompt': activation_prompt,
            'end_prompt': end_prompt
        }
        if override.get("sample_type"):
            sample['sample_type'] = override["sample_type"]
        else:
            sample['sample_type'] = 'memory'
        return sample

class EnhancedTextMemoryModel(nn.Module):
    """å¢å¼ºçš„æ–‡æœ¬è®°å¿†æ¨¡å‹"""
    
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        first_param = next(base_model.parameters())
        self.model_dtype = first_param.dtype
        self.model_device = first_param.device
        print(f"ğŸ”§ EnhancedTextMemoryModel æ•°æ®ç±»å‹: {self.model_dtype}, è®¾å¤‡: {self.model_device}")
        
    def forward(self, input_ids, embeddings_to_insert=None, embedding_positions=None, attention_mask=None, labels=None, memory_pad_token_id=None):
        """
        å‰å‘ä¼ æ’­ - æ”¯æŒæ ‡å‡†å‰å‘ä¼ æ’­ï¼ˆSFTï¼‰å’Œè®°å¿†å‘é‡æ’å…¥ï¼ˆè®°å¿†æ¡ç›®ï¼‰
        
        Args:
            input_ids: [batch_size, seq_len] tokenåºåˆ—
            embeddings_to_insert: [batch_size, embed_dim] è¦æ’å…¥çš„è¡¨å¾å‘é‡ï¼ˆå¯é€‰ï¼ŒSFTæ—¶ä¸ºNoneï¼‰
            embedding_positions: [batch_size] è¡¨å¾å‘é‡æ’å…¥ä½ç½®ï¼ˆå¯é€‰ï¼ŒSFTæ—¶ä¸ºNoneï¼‰
            attention_mask: [batch_size, seq_len] æ³¨æ„åŠ›æ©ç 
            labels: [batch_size, seq_len] æ ‡ç­¾
            memory_pad_token_id: <|memory_pad|> token IDï¼Œç”¨äºéªŒè¯æ³¨å…¥ä½ç½®ï¼ˆå¯é€‰ï¼‰
        """
        
        # ç¡®ä¿æ‰€æœ‰è¾“å…¥åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        input_ids = input_ids.to(self.model_device)
        attention_mask = attention_mask.to(self.model_device) if attention_mask is not None else None
        if labels is not None:
            labels = labels.to(self.model_device)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ’å…¥è®°å¿†å‘é‡
        # æ£€æŸ¥æ˜¯å¦æœ‰æ ·æœ¬éœ€è¦æ’å…¥ï¼ˆposition >= 0è¡¨ç¤ºéœ€è¦æ’å…¥ï¼‰
        need_memory_injection = (
            embeddings_to_insert is not None and 
            embedding_positions is not None and
            embeddings_to_insert.numel() > 0 and
            (embedding_positions >= 0).any()  # è‡³å°‘æœ‰ä¸€ä¸ªæ ·æœ¬çš„position >= 0
        )
        
        if need_memory_injection:
            # è®°å¿†æ¡ç›®è®­ç»ƒæˆ–æ··åˆbatchï¼šéœ€è¦æ’å…¥è®°å¿†å‘é‡
            embeddings_to_insert = embeddings_to_insert.to(self.model_device)
            embedding_positions = embedding_positions.to(self.model_device)
            
            # é€šè¿‡embeddingå±‚è·å–token embeddings
            embedding_layer = self.base_model.get_input_embeddings()
            token_embeddings = embedding_layer(input_ids)  # [batch_size, seq_len, embed_dim]
            
            # å¯¹äºæ··åˆbatchï¼Œåªå¯¹éœ€è¦æ’å…¥çš„æ ·æœ¬è¿›è¡Œæ’å…¥ï¼ˆposition >= 0ï¼‰
            # å¯¹äºSFTæ ·æœ¬ï¼ˆposition < 0ï¼‰ï¼Œè·³è¿‡æ’å…¥
            valid_mask = embedding_positions >= 0
            if valid_mask.all():
                # æ‰€æœ‰æ ·æœ¬éƒ½éœ€è¦æ’å…¥
                token_embeddings = inject_memory_embedding_to_inputs_embeds(
                    token_embeddings, embedding_positions, embeddings_to_insert,
                    input_ids=input_ids, memory_pad_token_id=memory_pad_token_id
                )
            else:
                # æ··åˆbatchï¼šåªå¯¹æœ‰æ•ˆæ ·æœ¬æ’å…¥
                for i in range(len(embedding_positions)):
                    if embedding_positions[i] >= 0:
                        pos = embedding_positions[i].item()
                        token_embeddings[i, pos] = embeddings_to_insert[i]
            
            # ä½¿ç”¨ä¿®æ”¹åçš„embeddingsè¿›è¡Œå‰å‘ä¼ æ’­
            outputs = self.base_model(
                inputs_embeds=token_embeddings,
                attention_mask=attention_mask,
                return_dict=True
            )
        else:
            # çº¯SFT batchï¼šæ ‡å‡†å‰å‘ä¼ æ’­
            outputs = self.base_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True
            )
        
        logits = outputs.logits
        
        # è®¡ç®—æŸå¤±
        loss = None
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )
        
        return {
            'loss': loss,
            'logits': logits
        }

class MixedMemorySFTDataset(Dataset):
    """æ··åˆæ•°æ®é›†ï¼šåŒ…å«è®°å¿†æ¡ç›®å’ŒSFTæ•°æ®ï¼Œæ¯ä¸ªepoché‡æ–°æŠ½å–"""
    
    def __init__(
        self,
        memory_texts,
        memory_embeddings,
        sft_messages_list,  # SFTæ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯æ ‡å‡†åŒ–çš„messages
        tokenizer,
        base_model,
        max_length=3000,
        noise_std=0.01,
        is_main_process_fn=None,
        sft_full_texts=None,
        activation_prompts=None,
        end_prompts=None,
        memory_ratio=0.5,  # è®°å¿†æ¡ç›®åœ¨æ··åˆæ•°æ®ä¸­çš„æ¯”ä¾‹
        guide_text=None,
        sft_message_source_indices=None,
        sft_full_source_indices=None,
    ):
        self.memory_texts = memory_texts
        self.memory_embeddings = memory_embeddings
        self.full_sft_messages_list = sft_messages_list if sft_messages_list is not None else []
        self.sft_message_source_indices = (
            sft_message_source_indices
            if sft_message_source_indices is not None
            else list(range(len(self.full_sft_messages_list)))
        )
        self.sft_messages_list = self.full_sft_messages_list
        self.tokenizer = tokenizer
        self.base_model = base_model
        self.max_length = max_length
        self.noise_std = noise_std
        self._is_main_process_fn = is_main_process_fn
        self.sft_full_texts = sft_full_texts if sft_full_texts is not None else []
        self.sft_full_source_indices = (
            sft_full_source_indices
            if sft_full_source_indices is not None
            else list(range(len(self.sft_full_texts)))
        )
        self.activation_prompts = _ensure_prompt_list(activation_prompts, "activation_prompts")
        self.end_prompts = _ensure_prompt_list(end_prompts, "end_prompts")
        self.memory_ratio = memory_ratio
        self.guide_text = guide_text or ""
        
        # åˆ›å»ºè®°å¿†æ¡ç›®æ•°æ®é›†ï¼ˆç”¨äºç”Ÿæˆè®°å¿†è®­ç»ƒæ ·æœ¬ï¼‰
        self.memory_dataset = EnhancedTextMemoryDataset(
            memory_texts,
            memory_embeddings,
            tokenizer,
            base_model,
            max_length=max_length,
            noise_std=noise_std,
            is_main_process_fn=is_main_process_fn,
            sft_full_texts=self.sft_full_texts,
            activation_prompts=activation_prompts,
            end_prompts=end_prompts,
            guide_text=self.guide_text,
        )
        
        # åˆå§‹åŒ–æ··åˆæ•°æ®ç´¢å¼•
        self.last_sft_only_indices = []
        self.last_sft_full_indices = []
        self.refresh_epoch_data()
        
        if self.is_main_process():
            print(f"âœ… æ··åˆæ•°æ®é›†åˆå§‹åŒ–å®Œæˆ")
            print(f"   è®°å¿†æ¡ç›®æ•°é‡: {len(memory_texts)}")
            print(f"   SFTæ•°æ®æ•°é‡: {len(sft_messages_list)}")
            print(f"   æ··åˆåæ€»æ ·æœ¬æ•°: {self.total_samples}")
            print(f"   è®°å¿†æ¡ç›®æ¯”ä¾‹: {memory_ratio:.1%}")
    
    def is_main_process(self):
        """åˆ¤æ–­å½“å‰æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
        if callable(self._is_main_process_fn):
            try:
                return bool(self._is_main_process_fn())
            except Exception:
                pass
        try:
            if dist.is_available() and dist.is_initialized():
                return dist.get_rank() == 0
        except Exception:
            pass
        return True
    
    def refresh_epoch_data(self):
        """æ¯ä¸ªepochå¼€å§‹æ—¶é‡æ–°æŠ½å–æ•°æ®"""
        memory_count = len(self.memory_texts)
        sft_count = len(self.sft_messages_list)
        
        self.mixed_indices = []
        self.last_sft_only_indices = []
        self.last_sft_full_indices = []
        if memory_count > 0:
            memory_indices = list(range(memory_count))
            random.shuffle(memory_indices)
            memory_full_count = memory_count // 2  # éœ€è¦åœ¨å°¾éƒ¨æ‹¼æ¥SFTçš„è®°å¿†æ•°é‡
            memory_front_count = memory_count - memory_full_count
            
            # è®°å¿†ç±»å‹Aï¼šä»…å‰ç½®SFT
            for idx in memory_indices[:memory_front_count]:
                self.mixed_indices.append(('memory_front', idx, None))
            
            # è®°å¿†ç±»å‹Bï¼šå‰ç½®+åç½®SFT
            if len(self.memory_dataset.sft_full_texts) > 0 and memory_full_count > 0:
                sft_full_indices = self._sample_indices(len(self.memory_dataset.sft_full_texts), memory_full_count)
                for mem_idx, sft_idx in zip(memory_indices[memory_front_count:], sft_full_indices):
                    self.mixed_indices.append(('memory_full', mem_idx, sft_idx))
                self.last_sft_full_indices = sft_full_indices[:]
            else:
                for idx in memory_indices[memory_front_count:]:
                    self.mixed_indices.append(('memory_front', idx, None))
                self.last_sft_full_indices = []
            
            # çº¯SFTæ ·æœ¬ï¼ˆæ•°é‡ä¸ºè®°å¿†æ¡ç›®çš„ä¸€åŠï¼Œå‘ä¸‹å–æ•´ï¼Œè‡³å°‘ä¸º1ï¼‰
            sft_only_target = memory_count // 2
            if memory_count == 1:
                sft_only_target = 1
            if sft_count > 0 and sft_only_target > 0:
                sft_only_indices = self._sample_indices(sft_count, sft_only_target)
                for sft_idx in sft_only_indices:
                    self.mixed_indices.append(('sft', sft_idx, None))
                self.last_sft_only_indices = sft_only_indices[:]
            else:
                self.last_sft_only_indices = []
        else:
            # æ²¡æœ‰è®°å¿†æ¡ç›®ï¼Œåªèƒ½è¿”å›SFTæ ·æœ¬
            sample_sft = min(32, sft_count)
            sft_only_indices = self._sample_indices(sft_count, sample_sft)
            for sft_idx in sft_only_indices:
                self.mixed_indices.append(('sft', sft_idx, None))
            self.last_sft_only_indices = sft_only_indices[:]
        
        random.shuffle(self.mixed_indices)
        self.total_samples = len(self.mixed_indices)
        
        # åˆ·æ–°è®°å¿†æ•°æ®é›†çš„ä¸Šæ–‡ï¼ˆæ¯ä¸ªepoché‡æ–°æŠ½å–ï¼‰
        self.memory_dataset.refresh_epoch_data()
        
        if self.is_main_process():
            type_a = sum(1 for item in self.mixed_indices if item[0] == 'memory_front')
            type_b = sum(1 for item in self.mixed_indices if item[0] == 'memory_full')
            type_c = sum(1 for item in self.mixed_indices if item[0] == 'sft')
            print(f"âœ… æ··åˆæ•°æ®åˆ·æ–°å®Œæˆ: {self.total_samples} ä¸ªæ ·æœ¬ (è®°å¿†-å‰ç½®: {type_a}, è®°å¿†-å‰åæ‹¼æ¥: {type_b}, çº¯SFT: {type_c})")
            if self.last_sft_only_indices:
                preview = min(5, len(self.last_sft_only_indices))
                preview_indices = sorted(self.last_sft_only_indices[:preview])
                mapped = sorted(
                    self.sft_message_source_indices[idx]
                    if idx < len(self.sft_message_source_indices)
                    else idx
                    for idx in preview_indices
                )
                print(f"   ğŸ“‹ çº¯SFTæ ·æœ¬åŸå§‹ç´¢å¼•(å‰{preview}æ¡): {mapped}")
                if len(self.last_sft_only_indices) > preview:
                    print(f"   ... å…± {len(self.last_sft_only_indices)} æ¡çº¯SFTæ ·æœ¬")
            if self.last_sft_full_indices:
                preview = min(5, len(self.last_sft_full_indices))
                preview_indices = sorted(self.last_sft_full_indices[:preview])
                mapped = sorted(
                    self.sft_full_source_indices[idx]
                    if idx < len(self.sft_full_source_indices)
                    else idx
                    for idx in preview_indices
                )
                print(f"   ğŸ“‹ å¤¹å¿ƒSFTæ ·æœ¬åŸå§‹ç´¢å¼•(å‰{preview}æ¡): {mapped}")
                if len(self.last_sft_full_indices) > preview:
                    print(f"   ... å…± {len(self.last_sft_full_indices)} æ¡å¤¹å¿ƒSFTæ ·æœ¬")
    
    def __len__(self):
        return self.total_samples
    
    def __getitem__(self, idx):
        """è·å–è®­ç»ƒæ ·æœ¬"""
        entry = self.mixed_indices[idx]
        data_type = entry[0]
        data_idx = entry[1]
        extra = entry[2] if len(entry) > 2 else None
        
        if data_type == 'memory_front':
            sample = self.memory_dataset._get_memory_decode_sample(data_idx)
            sample['sample_type'] = 'memory_front'
            return sample
        elif data_type == 'memory_full':
            context_override = self._build_context_override(extra)
            sample = self.memory_dataset._get_memory_decode_sample(data_idx, context_override=context_override)
            sample['sample_type'] = 'memory_full'
            return sample
        else:
            sample = self._get_sft_sample(data_idx)
            sample['sample_type'] = 'sft_only'
            return sample
    
    def _get_tokenizer(self):
        """è·å–å®é™…çš„tokenizerå¯¹è±¡"""
        if hasattr(self.tokenizer, 'tokenizer'):
            return self.tokenizer.tokenizer
        else:
            return self.tokenizer
    
    def _get_sft_sample(self, sft_idx):
        """æ„é€ SFTè®­ç»ƒæ ·æœ¬"""
        messages = self.sft_messages_list[sft_idx]
        
        # ä½¿ç”¨tokenizerå°†messagesè½¬æ¢ä¸ºinput_idså’Œlabels
        actual_tokenizer = self._get_tokenizer()
        
        # ä½¿ç”¨apply_chat_templateè½¬æ¢ä¸ºinput_ids
        batch_inputs = actual_tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=False,
            return_dict=True, return_tensors="pt"
        )
        
        input_ids = batch_inputs["input_ids"][0]  # [seq_len]
        attention_mask = batch_inputs.get("attention_mask", (input_ids != 0).long())[0]
        
        # é»˜è®¤å…¨éƒ¨maskï¼Œåç»­åªæ”¾å¼€assistantæ®µè½
        labels_tensor = torch.full_like(input_ids, -100)
        
        # è®¡ç®—æ¯æ¡messageç»“æŸæ—¶çš„é•¿åº¦ï¼Œç”¨äºå®šä½assistantå†…å®¹åŒºé—´
        prefix_lengths = []
        for end_idx in range(len(messages)):
            prefix_slice = messages[: end_idx + 1]
            prefix_inputs = actual_tokenizer.apply_chat_template(
                prefix_slice,
                tokenize=True,
                add_generation_prompt=False,
                return_dict=True,
                return_tensors="pt"
            )
            prefix_ids = prefix_inputs["input_ids"][0]
            prefix_lengths.append(prefix_ids.shape[0])
        
        total_len = input_ids.shape[0]
        if prefix_lengths and prefix_lengths[-1] != total_len:
            # ç†è®ºä¸Šåº”è¯¥å®Œå…¨ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™å–äº¤é›†ä»¥é¿å…è¶Šç•Œ
            total_len = min(total_len, prefix_lengths[-1])
        
        prev_len = 0
        for msg_idx, message in enumerate(messages):
            curr_len = prefix_lengths[msg_idx] if msg_idx < len(prefix_lengths) else total_len
            curr_len = min(curr_len, total_len)
            if message.get("role") == "assistant":
                labels_tensor[prev_len:curr_len] = input_ids[prev_len:curr_len]
            prev_len = curr_len
        
        # æ„é€ åºåˆ—tokensï¼ˆç”¨äºcollate_fnï¼‰
        sequence_tokens = input_ids.clone()
        
        # è·å–embeddingç»´åº¦ï¼ˆä»æ¨¡å‹é…ç½®æˆ–é»˜è®¤å€¼ï¼‰
        try:
            hidden_size = getattr(self.base_model.config, "hidden_size", 4096)
        except:
            hidden_size = 4096
        
        # è¿”å›æ ¼å¼ä¸è®°å¿†æ¡ç›®æ ·æœ¬ä¸€è‡´
        return {
            'sequence_tokens': sequence_tokens,
            'labels': labels_tensor,
            'embedding_to_insert': torch.zeros(1, hidden_size),  # å ä½ç¬¦ï¼ŒSFTä¸éœ€è¦embedding
            'embedding_position': -1,  # -1è¡¨ç¤ºSFTæ ·æœ¬ï¼Œä¸éœ€è¦æ’å…¥
            'context_text': '',
            'text': actual_tokenizer.decode(input_ids, skip_special_tokens=True),
            'activation_prompt': '',
            'end_prompt': '',
            'recall_token_count': 0,
            'context_length': 0,
            'is_sft': True,  # æ ‡è®°ä¸ºSFTæ ·æœ¬
        }
    
    def _build_context_override(self, sft_full_idx):
        if sft_full_idx is None:
            return None
        sft_full_texts = getattr(self.memory_dataset, "sft_full_texts", [])
        if not sft_full_texts:
            return None
        if sft_full_idx < 0 or sft_full_idx >= len(sft_full_texts):
            sft_full_idx = sft_full_idx % len(sft_full_texts)
        try:
            prefix_text, suffix_text = self.memory_dataset._split_sft_at_thinking(sft_full_texts[sft_full_idx])
            return {
                "prefix_text": prefix_text,
                "suffix_text": suffix_text,
                "sample_type": "memory_full"
            }
        except Exception:
            return None
    
    @staticmethod
    def _sample_indices(pool_size: int, sample_count: int) -> List[int]:
        if pool_size <= 0 or sample_count <= 0:
            return []
        if sample_count <= pool_size:
            return random.sample(range(pool_size), sample_count)
        return [random.randrange(pool_size) for _ in range(sample_count)]

class EnhancedTextMemoryTrainer:
    """å¢å¼ºçš„æ–‡æœ¬è®°å¿†è®­ç»ƒå™¨ - æ”¯æŒå¤šGPU"""
    
    def _get_tokenizer(self):
        """è·å–çœŸæ­£çš„tokenizerï¼ˆå¦‚æœä¼ å…¥çš„æ˜¯processorï¼Œåˆ™è¿”å›processor.tokenizerï¼‰"""
        if hasattr(self.tokenizer, 'tokenizer'):
            # å¦‚æœä¼ å…¥çš„æ˜¯processorï¼Œè¿”å›å…¶å†…éƒ¨çš„tokenizer
            return self.tokenizer.tokenizer
        else:
            # å¦‚æœä¼ å…¥çš„æ˜¯tokenizerï¼Œç›´æ¥è¿”å›
            return self.tokenizer

    def __init__(
        self,
        model_name,
        device=None,
        lora_r=8,
        lora_alpha=32,
        lora_dropout=0.1,
        original_device=None,
        preloaded_model=None,
        preloaded_tokenizer=None,
        gradient_accumulation_steps=1,
        max_memory=None,
        generation_config=None,
        epoch_end_hook=None,
        lora_target_modules=None,
        dataset_max_length=3000,
        test_sample_count=2,
        test_max_new_tokens=300,
        test_use_cache=False,
        activation_prompts=None,
        end_prompts=None,
        guide_text=None,
    ):

        # æ³¨æ„ï¼šCUDA_VISIBLE_DEVICES å·²ç»åœ¨ app.py ä¸­æ­£ç¡®è®¾ç½®ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤è®¾ç½®
        # åªä¿å­˜åŸå§‹ç¯å¢ƒå˜é‡ç”¨äºcleanupæ—¶æ¢å¤
        self._original_cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES')

        self.model_name = model_name
        self.specified_device = device
        self.original_device = original_device or device  # ä¿å­˜åŸå§‹è®¾å¤‡ä¿¡æ¯ç”¨äºæ˜¾ç¤º
        self.ddp_enabled = False
        self.local_rank = None

        # âš ï¸ æ³¨æ„ï¼šä¸‹é¢è¿™äº›tokenå­—ç¬¦ä¸²ä¼šåœ¨å¤šä¸ªæ–¹æ³•ï¼ˆç‰¹åˆ«æ˜¯_preloadedè·¯å¾„ï¼‰é‡Œç«‹å³ä½¿ç”¨
        # è¿‡å»å®ƒä»¬æ˜¯åœ¨_check_and_add_special_tokens()é‡Œä¸´æ—¶èµ‹å€¼ï¼Œç”±äºç°åœ¨æ”¯æŒå¤–éƒ¨ä¼ å…¥å·²åŠ è½½æ¨¡å‹ï¼Œ
        # éœ€è¦åœ¨æ„é€ å‡½æ•°æœ€å¼€å§‹å°±æ˜¾å¼è®¾ç½®ï¼Œé¿å…â€œå…ˆè®¿é—®åå®šä¹‰â€å¯¼è‡´AttributeErrorã€‚
        self.recall_start_token = '<recall>'
        self.recall_end_token = '</recall>'
        self.im_start_token = '<|im_start|>'
        self.im_end_token = '<|im_end|>'
        # æ˜¾ç¤ºæ­£ç¡®çš„è®¾å¤‡ä¿¡æ¯
        display_device = self.original_device or device
        if isinstance(display_device, str) and display_device.startswith('cuda:'):
            print(f"   ä½¿ç”¨GPUè®¾å¤‡: {display_device}")
        elif display_device == "auto":
            print("   è‡ªåŠ¨é€‰æ‹©è®¾å¤‡")
        else:
            print(f"   ä½¿ç”¨è®¾å¤‡: {display_device}")
        # é…ç½®æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        self.gradient_accumulation_steps = gradient_accumulation_steps

        # æ ¹æ®è®¾å¤‡é…ç½®å†³å®šæ˜¯å¦å¯ç”¨DDP
        use_ddp = False
        if isinstance(device, list) and len(device) > 1:
            use_ddp = True
            print(f"   å¤šGPUæ¨¡å¼: å¯ç”¨DDPï¼ŒGPUæ•°é‡: {len(device)}")
        elif device == "auto":
            if torch.cuda.is_available() and torch.cuda.device_count() > 1:
                use_ddp = True
                print(f"   å¤šGPUæ¨¡å¼: è‡ªåŠ¨æ£€æµ‹å¤šGPUï¼Œå¯ç”¨DDP")

        # å¯¹äºå•GPUé…ç½®ï¼Œåœ¨Acceleratoråˆå§‹åŒ–å‰è®¾ç½®å½“å‰è®¾å¤‡
        if isinstance(device, str) and device.startswith('cuda:'):
            # æ£€æŸ¥CUDA_VISIBLE_DEVICES
            cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
            if cuda_visible and cuda_visible.strip():
                # CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„åçš„è®¾å¤‡cuda:0
                torch.cuda.set_device(0)
            else:
                # æœªè®¾ç½®CUDA_VISIBLE_DEVICESï¼Œç›´æ¥ä½¿ç”¨ç‰©ç†è®¾å¤‡
                device_idx = int(device.split(':')[1])
                torch.cuda.set_device(device_idx)
        
        # åˆå§‹åŒ–Acceleratorï¼Œæ”¯æŒå¤šGPUå’Œæ¢¯åº¦ç´¯ç§¯
        self.accelerator = Accelerator(
            mixed_precision='bf16',
            gradient_accumulation_steps=self.gradient_accumulation_steps,
            # å¦‚æœæ˜¯å¤šGPUï¼Œå¯ç”¨DDP
            # æ³¨æ„ï¼šDDPéœ€è¦åœ¨torchrunä¸‹å¯åŠ¨ï¼Œè¿™é‡Œåªæ˜¯é…ç½®
        )

        # æ ¹æ®è®¾å¤‡é…ç½®è®¾ç½®ç›¸å…³å˜é‡
        if device is None:
            self.use_auto_device = False
            self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.multi_gpu_list = None
        elif isinstance(device, list):
            # å¤šGPUé…ç½®
            if len(device) > 0:
                self.use_auto_device = False
                self.primary_device = torch.device(device[0])
                self.multi_gpu_list = device
                print(f"   ä½¿ç”¨å¤šGPUåˆ—è¡¨: {device}ï¼Œä¸»è®¾å¤‡: {device[0]}")
        elif isinstance(device, str) and device.startswith('cuda:'):
            # å•GPUé…ç½®
            # å¦‚æœè®¾ç½®äº†CUDA_VISIBLE_DEVICESï¼Œéœ€è¦ä½¿ç”¨é‡æ–°æ˜ å°„åçš„è®¾å¤‡
            cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
            if cuda_visible and cuda_visible.strip():
                # CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„åçš„è®¾å¤‡
                self.primary_device = torch.device("cuda:0")
                print(f"   CUDA_VISIBLE_DEVICES={cuda_visible}ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„è®¾å¤‡ cuda:0ï¼ˆå¯¹åº”ç‰©ç†GPU {device}ï¼‰")
            else:
                # æœªè®¾ç½®CUDA_VISIBLE_DEVICESï¼Œç›´æ¥ä½¿ç”¨ç‰©ç†è®¾å¤‡
                self.primary_device = torch.device(device)
                print(f"   ä½¿ç”¨è®¾å¤‡ {device}")
            self.use_auto_device = False
            self.multi_gpu_list = None
        elif device == "auto":
            self.use_auto_device = True
            self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.multi_gpu_list = None
        else:
            # CPUæˆ–å…¶ä»–
            self.use_auto_device = False
            self.primary_device = torch.device('cpu')
            self.multi_gpu_list = None

        # LoRAé…ç½®å‚æ•°
        self.lora_r = lora_r
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        # LoRAç›®æ ‡æ¨¡å—ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
        self.lora_target_modules = lora_target_modules
        self.max_memory = max_memory
        self.generation_config = generation_config or {}
        self.epoch_end_hook = epoch_end_hook
        self.dataset_max_length = dataset_max_length
        self.test_sample_count = max(1, int(test_sample_count))
        self.test_max_new_tokens = max(1, int(test_max_new_tokens))
        self.test_use_cache = bool(test_use_cache)
        self.activation_prompts = _ensure_prompt_list(activation_prompts, "activation_prompts")
        self.end_prompts = _ensure_prompt_list(end_prompts, "end_prompts")
        self.guide_text = guide_text or ""

# è®¾å¤‡å˜é‡å·²åœ¨å‰é¢è®¾ç½®
        
        # ç‰¹æ®Štokenå®šä¹‰
        self.special_tokens = ['<recall>', '</recall>']
        self.recall_start_token = '<recall>'
        self.recall_end_token = '</recall>'

        print(f"ğŸ¤– åˆå§‹åŒ–å¢å¼ºæ–‡æœ¬è®°å¿†è®­ç»ƒå™¨...")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   è®¾å¤‡é…ç½®: {device}")

        # è‹¥ç”± torchrun å¯åŠ¨ï¼Œè‡ªåŠ¨å¯ç”¨DDPå¹¶å›ºå®šåˆ°å•å¡
        if 'LOCAL_RANK' in os.environ and not self.accelerator.state.initialized:
            self.local_rank = int(os.environ['LOCAL_RANK'])
            os.environ.setdefault('RANK', os.environ.get('RANK', '0'))
            os.environ.setdefault('WORLD_SIZE', os.environ.get('WORLD_SIZE', '1'))
            torch.cuda.set_device(self.local_rank)
            if not (dist.is_available() and dist.is_initialized()):
                dist.init_process_group(backend='nccl', timeout=timedelta(minutes=60))
            self.ddp_enabled = True
            # åœ¨DDPä¸‹å¼ºåˆ¶å•å¡åŠ è½½ï¼Œè¦†ç›–è‡ªåŠ¨åˆ†é…
            self.use_auto_device = False
            self.multi_gpu_list = None
            self.primary_device = torch.device(f'cuda:{self.local_rank}')
            self.specified_device = f'cuda:{self.local_rank}'
            if self.is_main_process():
                print(f"ğŸ§© DDPå·²å¯ç”¨ï¼ŒLOCAL_RANK={self.local_rank}")
        
        # å¤„ç†é¢„åŠ è½½æ¨¡å‹æˆ–åŠ è½½æ–°æ¨¡å‹
        if preloaded_model is not None and preloaded_tokenizer is not None:
            # ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡å‹
            print("   ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡å‹å’Œtokenizer")
            
            # ç¡®ä¿é¢„åŠ è½½æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            first_param = next(preloaded_model.parameters())
            current_device = first_param.device
            target_device = self.primary_device
            
            if current_device != target_device:
                print(f"   âš ï¸ é¢„åŠ è½½æ¨¡å‹åœ¨ {current_device}ï¼Œéœ€è¦ç§»åŠ¨åˆ° {target_device}")
                preloaded_model = preloaded_model.to(target_device)
                print(f"   âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° {target_device}")
            
            self.base_model = preloaded_model
            self.tokenizer = preloaded_tokenizer
            # é¢„åŠ è½½çš„tokenizeråº”è¯¥å·²ç»åŒ…å«äº†æ­£ç¡®çš„ç‰¹æ®Štokenï¼Œç›´æ¥è®¾ç½®token IDs
            self._set_special_token_ids()
            self._skip_model_loading = True
        else:
            # æ­£å¸¸åŠ è½½æ¨¡å‹
            self._load_model()
            # æ£€æŸ¥ç‰¹æ®Štoken
            self._check_and_add_special_tokens()
            self._skip_model_loading = False

        # è®°å½•åŸå§‹embedding
        self._save_original_embeddings()

        # è®¾ç½®LoRA
        self._setup_lora()

        # åˆ›å»ºåŒ…è£…æ¨¡å‹
        self.model = EnhancedTextMemoryModel(self.base_model)

        # é™å†…å­˜ï¼šå¯¹åŸºç¡€æ¨¡å‹å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å¹¶å…³é—­use_cache
        try:
            if hasattr(self.base_model, 'gradient_checkpointing_enable'):
                self.base_model.gradient_checkpointing_enable()
            if hasattr(self.base_model, 'config'):
                setattr(self.base_model.config, 'use_cache', False)
        except Exception:
            pass

        # DDPåŒ…è£…
        if self.ddp_enabled:
            self.model = DDP(self.model, device_ids=[self.local_rank], output_device=self.local_rank, find_unused_parameters=False)
        
        # æ˜¾ç¤ºå‚æ•°ç»Ÿè®¡
        self._print_parameters()

    def is_main_process(self):
        if hasattr(self, 'accelerator'):
            return self.accelerator.is_main_process
        return (not self.ddp_enabled) or (dist.get_rank() == 0)
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ - æ”¯æŒå¤šGPUé…ç½®"""
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨local_files_only
        import os
        model_path = self.model_name
        if not os.path.isabs(model_path):
            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(script_dir)
            model_path = os.path.abspath(os.path.join(project_root, model_path))
        
        # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œä½¿ç”¨local_files_onlyé¿å…modelscopeå°è¯•ä»ç½‘ç»œä¸‹è½½
        is_local_path = os.path.exists(model_path) and os.path.isdir(model_path)
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path if is_local_path else self.model_name,
            trust_remote_code=True,
            local_files_only=is_local_path
        )
        
        try:
            # æ ¹æ®è®¾å¤‡é…ç½®é€‰æ‹©device_map
            if self.use_auto_device:
                device_map = "auto"
                print("   ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…")
            elif hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                # å¤šGPUé…ç½®
                device_map = "auto"
                print(f"   ä½¿ç”¨å¤šGPUè‡ªåŠ¨åˆ†é…: {self.multi_gpu_list}")

                # è®¾ç½®ç¯å¢ƒå˜é‡é™åˆ¶å¯è§GPU
                import os
                if 'CUDA_VISIBLE_DEVICES' not in os.environ:
                    gpu_indices = [gpu.split(':')[1] for gpu in self.multi_gpu_list if gpu.startswith('cuda:')]
                    if gpu_indices:
                        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(gpu_indices)
                        print(f"   è®¾ç½®CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")

                # max_memoryä¼šåœ¨æ¨¡å‹åŠ è½½æ—¶å•ç‹¬ä¼ é€’ï¼Œä¸å½±å“device_map
                if hasattr(self, 'max_memory') and self.max_memory:
                    print(f"   å°†ä½¿ç”¨max_memoryæ§åˆ¶GPUåˆ†å¸ƒ: {self.max_memory}")
                else:
                    print(f"   ä½¿ç”¨è‡ªåŠ¨GPUåˆ†å¸ƒ (æœªè®¾ç½®max_memory)")
            elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                # å•GPUæŒ‡å®š - å¦‚æœæœåŠ¡å™¨å·²è®¾ç½®CUDA_VISIBLE_DEVICESï¼Œä½¿ç”¨cuda:0
                import os
                if 'CUDA_VISIBLE_DEVICES' in os.environ:
                    # æœåŠ¡å™¨å·²è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨cuda:0
                    device_map = {"": 0}
                    print(f"   æœåŠ¡å™¨å·²è®¾ç½®CUDA_VISIBLE_DEVICESï¼Œä½¿ç”¨cuda:0 (åŸå§‹è®¾å¤‡: {self.specified_device})")
                else:
                    # æœåŠ¡å™¨æœªè®¾ç½®ï¼Œä½¿ç”¨åŸå§‹è®¾å¤‡ç´¢å¼•
                    device_index = int(self.specified_device.split(':')[1])
                    device_map = {"": device_index}
                    print(f"   ä½¿ç”¨æŒ‡å®šå•GPU: {self.specified_device}")
            elif self.specified_device == "cpu":
                device_map = {"": "cpu"}
                print(f"   ä½¿ç”¨CPUè®¾å¤‡")
            else:
                # é»˜è®¤æƒ…å†µ
                if hasattr(self, 'primary_device') and self.primary_device.type == 'cuda':
                    device_map = {"": self.primary_device.index}
                else:
                    device_map = "auto"
                print(f"   ä½¿ç”¨é»˜è®¤è®¾å¤‡æ˜ å°„: {device_map}")
            
            print(f"   å®é™…ä½¿ç”¨è®¾å¤‡æ˜ å°„: {device_map}")
            
            # æ£€æŸ¥æ¨¡å‹ç±»å‹ï¼šå¦‚æœæ˜¯Qwen3-VLï¼Œéœ€è¦ä½¿ç”¨Qwen3VLForConditionalGeneration
            # æ£€æŸ¥config.jsonæ–‡ä»¶æ¥ç¡®å®šæ¨¡å‹ç±»å‹
            import json
            config_file = os.path.join(model_path if is_local_path else self.model_name, "config.json")
            is_qwen3vl = False
            if os.path.exists(config_file):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        model_config = json.load(f)
                        model_type = model_config.get("model_type", "").lower()
                        if "qwen3_vl" in model_type or "qwen3-vl" in model_type:
                            is_qwen3vl = True
                            print(f"   æ£€æµ‹åˆ°Qwen3-VLæ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨Qwen3VLForConditionalGenerationåŠ è½½")
                except:
                    pass
            
            # å‡†å¤‡åŠ è½½å‚æ•°
            load_kwargs = {
                "torch_dtype": "auto",
                "device_map": device_map,
                "trust_remote_code": True,
                "local_files_only": is_local_path
            }

            # å¦‚æœæœ‰max_memoryé…ç½®ï¼Œæ·»åŠ å®ƒ
            if hasattr(self, 'max_memory') and self.max_memory and device_map == "auto":
                load_kwargs["max_memory"] = self.max_memory
                print(f"   æ·»åŠ max_memoryå‚æ•°: {self.max_memory}")

            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼
            if is_qwen3vl:
                # ä½¿ç”¨Qwen3VLForConditionalGenerationåŠ è½½Qwen3-VLæ¨¡å‹
                from transformers import Qwen3VLForConditionalGeneration
                self.base_model = Qwen3VLForConditionalGeneration.from_pretrained(
                    model_path if is_local_path else self.model_name,
                    **load_kwargs
                )
            else:
                # ä½¿ç”¨AutoModelForCausalLMåŠ è½½æ™®é€šæ–‡æœ¬æ¨¡å‹
                self.base_model = AutoModelForCausalLM.from_pretrained(
                    model_path if is_local_path else self.model_name,
                    **load_kwargs
                )
            
            # è·å–å®é™…è®¾å¤‡ä¿¡æ¯
            first_param = next(self.base_model.parameters())
            model_dtype = first_param.dtype
            model_device = first_param.device
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   å®é™…è®¾å¤‡: {model_device}")
            print(f"   æ•°æ®ç±»å‹: {model_dtype}")
            
            # æ˜¾ç¤ºè®¾å¤‡æ˜ å°„ä¿¡æ¯
            if hasattr(self.base_model, 'hf_device_map'):
                print(f"   è®¾å¤‡æ˜ å°„è¯¦æƒ…: {self.base_model.hf_device_map}")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å›é€€ç­–ç•¥
            print("ğŸ”„ å°è¯•å›é€€åˆ°å•GPUæ¨¡å¼...")
            
            try:
                # ç¡®å®šå›é€€è®¾å¤‡
                if hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                    fallback_device = self.multi_gpu_list[0]
                elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                    fallback_device = self.specified_device
                else:
                    fallback_device = 'cuda:0'
                
                # æå–è®¾å¤‡ç´¢å¼•
                if fallback_device.startswith('cuda:'):
                    device_index = int(fallback_device.split(':')[1])
                    device_map = {"": device_index}
                else:
                    device_map = {"": "cpu"}
                
                print(f"   å›é€€è®¾å¤‡æ˜ å°„: {device_map}")
                
                # æ£€æŸ¥æ¨¡å‹ç±»å‹ï¼šå¦‚æœæ˜¯Qwen3-VLï¼Œéœ€è¦ä½¿ç”¨Qwen3VLForConditionalGeneration
                import json
                config_file = os.path.join(model_path if is_local_path else self.model_name, "config.json")
                is_qwen3vl = False
                if os.path.exists(config_file):
                    try:
                        with open(config_file, 'r', encoding='utf-8') as f:
                            model_config = json.load(f)
                            model_type = model_config.get("model_type", "").lower()
                            if "qwen3_vl" in model_type or "qwen3-vl" in model_type:
                                is_qwen3vl = True
                                print(f"   æ£€æµ‹åˆ°Qwen3-VLæ¨¡å‹ç±»å‹ï¼Œä½¿ç”¨Qwen3VLForConditionalGenerationåŠ è½½")
                    except:
                        pass
                
                # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼
                if is_qwen3vl:
                    # ä½¿ç”¨Qwen3VLForConditionalGenerationåŠ è½½Qwen3-VLæ¨¡å‹
                    from transformers import Qwen3VLForConditionalGeneration
                    self.base_model = Qwen3VLForConditionalGeneration.from_pretrained(
                        model_path if is_local_path else self.model_name,
                        torch_dtype="auto",
                        device_map=device_map,
                        trust_remote_code=True,
                        local_files_only=is_local_path
                    )
                else:
                    # ä½¿ç”¨AutoModelForCausalLMåŠ è½½æ™®é€šæ–‡æœ¬æ¨¡å‹
                    self.base_model = AutoModelForCausalLM.from_pretrained(
                        model_path if is_local_path else self.model_name,
                        torch_dtype="auto",
                        device_map=device_map,
                        trust_remote_code=True,
                        local_files_only=is_local_path
                    )
                
                first_param = next(self.base_model.parameters())
                print(f"âœ… ä½¿ç”¨å›é€€è®¾å¤‡åŠ è½½æˆåŠŸ: {first_param.device}")
                
            except Exception as fallback_error:
                print(f"âŒ å›é€€åŠ è½½ä¹Ÿå¤±è´¥: {fallback_error}")
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: åŸé”™è¯¯={e}, å›é€€é”™è¯¯={fallback_error}")
    
    def _check_and_add_special_tokens(self):
        """æ£€æŸ¥å¹¶æ·»åŠ ç‰¹æ®Štokenï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        # æ£€æŸ¥recall_start_tokenå’Œrecall_end_token
        recall_start_id = self.tokenizer.convert_tokens_to_ids(self.recall_start_token)
        recall_end_id = self.tokenizer.convert_tokens_to_ids(self.recall_end_token)
        
        tokens_to_add = []
        if recall_start_id == self.tokenizer.unk_token_id:
            tokens_to_add.append(self.recall_start_token)
        if recall_end_id == self.tokenizer.unk_token_id:
            tokens_to_add.append(self.recall_end_token)
        
        # è·å–æœ€ç»ˆçš„token ID
        final_recall_start_id = self.tokenizer.convert_tokens_to_ids(self.recall_start_token)
        final_recall_end_id = self.tokenizer.convert_tokens_to_ids(self.recall_end_token)

        if tokens_to_add:
            # tokenä¸å­˜åœ¨ï¼Œéœ€è¦æ·»åŠ 
            print(f"âš ï¸ ä»¥ä¸‹ç‰¹æ®Štokenä¸å­˜åœ¨ï¼Œæ­£åœ¨æ·»åŠ : {tokens_to_add}")
            original_vocab_size = len(self.tokenizer)

            # æ·»åŠ ç‰¹æ®Štoken
            for token in tokens_to_add:
                self.tokenizer.add_tokens(token)

            new_vocab_size = len(self.tokenizer)
            print(f"   è¯è¡¨å¤§å°: {original_vocab_size} -> {new_vocab_size} (+{new_vocab_size - original_vocab_size})")

            # è°ƒæ•´æ¨¡å‹embeddingå±‚
            print("   è°ƒæ•´æ¨¡å‹embeddingå±‚...")
            self.base_model.resize_token_embeddings(len(self.tokenizer))

            # è·å–æ–°æ·»åŠ çš„token ID
            final_recall_start_id = self.tokenizer.convert_tokens_to_ids(self.recall_start_token)
            final_recall_end_id = self.tokenizer.convert_tokens_to_ids(self.recall_end_token)
            
            # åˆå§‹åŒ–æ–°tokençš„æƒé‡
            print("   åˆå§‹åŒ–æ–°tokenæƒé‡...")
            try:
                embedding_layer = self.base_model.get_input_embeddings()
                # <recall> token: ä½¿ç”¨"æ€»ç»“"å’Œ"å›å¿†"çš„åµŒå…¥å‘é‡ä¹‹å’Œ
                if self.recall_start_token in tokens_to_add:
                    recall_start_id = self.tokenizer.convert_tokens_to_ids(self.recall_start_token)
                    ref_words = ["æ€»ç»“", "å›å¿†"]
                    ref_embeddings = []
                    used_refs = []
                    
                    for word in ref_words:
                        ref_id = self.tokenizer.convert_tokens_to_ids(word)
                        if ref_id != self.tokenizer.unk_token_id:
                            ref_embeddings.append(embedding_layer.weight[ref_id].clone().detach())
                            used_refs.append(word)
                    
                    if len(ref_embeddings) > 0:
                        new_embedding = ref_embeddings[0]
                        for ref_emb in ref_embeddings[1:]:
                            new_embedding = new_embedding + ref_emb
                        
                        # ç›´æ¥å½’ä¸€åŒ–ï¼šç¼©æ”¾åˆ°ç¬¬ä¸€ä¸ªå‚è€ƒtokençš„èŒƒæ•°ï¼Œç„¶åæ·»åŠ å°çš„æ­£äº¤æ‰°åŠ¨ä»¥åŒºåˆ†
                        if len(ref_embeddings) > 1:
                            target_norm = ref_embeddings[0].norm()
                            current_norm = new_embedding.norm()
                            if current_norm > 0:
                                new_embedding = new_embedding / current_norm * target_norm
                            
                            # æ·»åŠ å°çš„æ­£äº¤æ‰°åŠ¨ï¼Œé¿å…ä¸å‚è€ƒtokenè¿‡äºç›¸ä¼¼
                            ref1_normalized = ref_embeddings[0] / ref_embeddings[0].norm()
                            new_normalized = new_embedding / new_embedding.norm()
                            proj = torch.dot(new_normalized, ref1_normalized) * ref1_normalized
                            orthogonal = new_normalized - proj
                            if orthogonal.norm() > 1e-6:
                                orthogonal = orthogonal / orthogonal.norm()
                                perturbation_scale = 0.1
                                new_embedding = new_embedding + orthogonal * perturbation_scale * target_norm
                                new_embedding = new_embedding / new_embedding.norm() * target_norm
                        
                        embedding_layer.weight.data[recall_start_id] = new_embedding
                        ref_str = " + ".join(used_refs)
                        print(f"   âœ… {self.recall_start_token} (ID: {recall_start_id}) åˆå§‹åŒ–å®Œæˆï¼ˆå‚è€ƒ: {ref_str}ï¼‰")
                    else:
                        print(f"   âš ï¸ {self.recall_start_token} çš„å‚è€ƒtokenéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                
                # </recall> token: ä½¿ç”¨"å›å¿†"å’Œ"ç»“æŸ"çš„åµŒå…¥å‘é‡ä¹‹å’Œ
                if self.recall_end_token in tokens_to_add:
                    recall_end_id = self.tokenizer.convert_tokens_to_ids(self.recall_end_token)
                    ref_words = ["å›å¿†", "ç»“æŸ"]
                    ref_embeddings = []
                    used_refs = []
                    
                    for word in ref_words:
                        ref_id = self.tokenizer.convert_tokens_to_ids(word)
                        if ref_id != self.tokenizer.unk_token_id:
                            ref_embeddings.append(embedding_layer.weight[ref_id].clone().detach())
                            used_refs.append(word)
                    
                    if len(ref_embeddings) > 0:
                        new_embedding = ref_embeddings[0]
                        for ref_emb in ref_embeddings[1:]:
                            new_embedding = new_embedding + ref_emb
                        
                        # ç›´æ¥å½’ä¸€åŒ–ï¼šç¼©æ”¾åˆ°ç¬¬ä¸€ä¸ªå‚è€ƒtokençš„èŒƒæ•°ï¼Œç„¶åæ·»åŠ å°çš„æ­£äº¤æ‰°åŠ¨ä»¥åŒºåˆ†
                        if len(ref_embeddings) > 1:
                            target_norm = ref_embeddings[0].norm()
                            current_norm = new_embedding.norm()
                            if current_norm > 0:
                                new_embedding = new_embedding / current_norm * target_norm
                            
                            # æ·»åŠ å°çš„æ­£äº¤æ‰°åŠ¨ï¼Œé¿å…ä¸å‚è€ƒtokenè¿‡äºç›¸ä¼¼
                            ref1_normalized = ref_embeddings[0] / ref_embeddings[0].norm()
                            new_normalized = new_embedding / new_embedding.norm()
                            proj = torch.dot(new_normalized, ref1_normalized) * ref1_normalized
                            orthogonal = new_normalized - proj
                            if orthogonal.norm() > 1e-6:
                                orthogonal = orthogonal / orthogonal.norm()
                                perturbation_scale = 0.1
                                new_embedding = new_embedding + orthogonal * perturbation_scale * target_norm
                                new_embedding = new_embedding / new_embedding.norm() * target_norm
                        
                        embedding_layer.weight.data[recall_end_id] = new_embedding
                        ref_str = " + ".join(used_refs)
                        print(f"   âœ… {self.recall_end_token} (ID: {recall_end_id}) åˆå§‹åŒ–å®Œæˆï¼ˆå‚è€ƒ: {ref_str}ï¼‰")
                    else:
                        print(f"   âš ï¸ {self.recall_end_token} çš„å‚è€ƒtokenéƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")

            except Exception as e:
                print(f"   âš ï¸ åˆå§‹åŒ–tokenæƒé‡æ—¶å‡ºé”™: {e}")
            
            print(f"âœ… ç‰¹æ®Štokenæ·»åŠ å®Œæˆ: {self.recall_start_token} (ID: {final_recall_start_id}), {self.recall_end_token} (ID: {final_recall_end_id})")
        else:
            # tokenå·²å­˜åœ¨
            print(f"âœ… ç‰¹æ®Štokenæ£€æŸ¥é€šè¿‡: {self.recall_start_token} (ID: {final_recall_start_id}), {self.recall_end_token} (ID: {final_recall_end_id})")

        # è®¾ç½®special_token_idsä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
        self.special_token_ids = {
            self.recall_start_token: final_recall_start_id,
            self.recall_end_token: final_recall_end_id
        }
    
    def _check_special_tokens(self):
        """æ£€æŸ¥ç‰¹æ®Štokenæ˜¯å¦å­˜åœ¨ï¼ˆå·²åºŸå¼ƒï¼Œä½¿ç”¨_check_and_add_special_tokensä»£æ›¿ï¼‰"""
        # è¿™ä¸ªæ–¹æ³•ä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹æ€§ï¼Œä½†å®é™…è°ƒç”¨çš„æ˜¯_check_and_add_special_tokens
        self._check_and_add_special_tokens()
        
        # è®¾ç½®special_token_idsä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨
        self.special_token_ids = {
            self.recall_start_token: self.tokenizer.convert_tokens_to_ids(self.recall_start_token),
            self.recall_end_token: self.tokenizer.convert_tokens_to_ids(self.recall_end_token)
        }

    def _set_special_token_ids(self):
        """ç›´æ¥è®¾ç½®ç‰¹æ®Štoken IDsï¼ˆå‡è®¾tokenizerå·²ç»åŒ…å«äº†æ­£ç¡®çš„tokenï¼‰"""
        tokenizer = self._get_tokenizer()
        self.recall_start_id = tokenizer.convert_tokens_to_ids(self.recall_start_token)
        self.recall_end_id = tokenizer.convert_tokens_to_ids(self.recall_end_token)
        self.memory_pad_id = tokenizer.convert_tokens_to_ids("<|memory_pad|>")
        self.im_start_id = tokenizer.convert_tokens_to_ids(self.im_start_token)
        self.im_end_id = tokenizer.convert_tokens_to_ids(self.im_end_token)

        if self.recall_start_id == tokenizer.unk_token_id:
            raise ValueError(f"âŒ {self.recall_start_token} tokenä¸å­˜åœ¨äºtokenizerä¸­ï¼")
        if self.recall_end_id == tokenizer.unk_token_id:
            raise ValueError(f"âŒ {self.recall_end_token} tokenä¸å­˜åœ¨äºtokenizerä¸­ï¼")
        if self.memory_pad_id == tokenizer.unk_token_id:
            raise ValueError(f"âŒ <|memory_pad|> tokenä¸å­˜åœ¨äºtokenizerä¸­ï¼")

        print(f"âœ… ç‰¹æ®Štoken IDsè®¾ç½®å®Œæˆ: {self.recall_start_token}={self.recall_start_id}, {self.recall_end_token}={self.recall_end_id}, <|memory_pad|>={self.memory_pad_id}")

        # è®¾ç½®special_token_idsä¾›å…¶ä»–æ–¹æ³•ä½¿ç”¨ï¼ˆåŒ…å«æ‰€æœ‰ç‰¹æ®Štokenï¼‰
        self.special_token_ids = {
            self.recall_start_token: self.recall_start_id,
            self.recall_end_token: self.recall_end_id,
            "<|memory_pad|>": self.memory_pad_id
        }
        
        # è®¾ç½®å¯è®­ç»ƒçš„ç‰¹æ®Štokenï¼ˆä¸åŒ…æ‹¬<|memory_pad|>ï¼Œå› ä¸ºå®ƒåªæ˜¯å ä½ç¬¦ï¼‰
        self.trainable_special_token_ids = {
            self.recall_start_token: self.recall_start_id,
            self.recall_end_token: self.recall_end_id
        }
    
    def _save_original_embeddings(self):
        """ä¿å­˜åŸå§‹ç‰¹æ®Štokençš„embedding - ä¿æŒæ•°æ®ç±»å‹"""
        embedding_layer = self.base_model.get_input_embeddings()
        self.original_embeddings = {}
        
        for token, token_id in self.special_token_ids.items():
            # ä¿æŒåŸå§‹æ•°æ®ç±»å‹
            self.original_embeddings[token] = embedding_layer.weight[token_id].clone().detach()
        
        print(f"ğŸ“ å·²ä¿å­˜ {len(self.original_embeddings)} ä¸ªç‰¹æ®Štokençš„åŸå§‹embedding")
        print(f"   åŸå§‹embeddingæ•°æ®ç±»å‹: {list(self.original_embeddings.values())[0].dtype}")
    
    def _setup_lora(self):
        """è®¾ç½®LoRAé…ç½® - ä¿®æ”¹ä¸ºä¸ä¿å­˜æ•´ä¸ªembeddingå±‚"""
        print("âš¡ é…ç½®LoRA...")
        print(f"   LoRAå‚æ•°: r={self.lora_r}, alpha={self.lora_alpha}, dropout={self.lora_dropout}")

        if hasattr(self.base_model, "peft_config"):
            raise RuntimeError(
                "åŠ è½½çš„åŸºç¡€æ¨¡å‹ä»åŒ…å«LoRA/PEFTé…ç½®ï¼Œè¯·ç¡®è®¤ä¸Šä¸€æ¬¡è®­ç»ƒè¾“å‡ºç›®å½•å·²è¢«æ¸…ç†åå†é‡è¯•"
            )
        
        # ç¡®å®štarget_modules
        if self.lora_target_modules is None:
            # é»˜è®¤é…ç½®ï¼šæ‰€æœ‰æ¨¡å—
            target_modules = [
                "q_proj", "v_proj", "k_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]
        else:
            # ä½¿ç”¨ä¼ å…¥çš„é…ç½®
            target_modules = self.lora_target_modules
        
        print(f"   LoRAç›®æ ‡æ¨¡å—: {target_modules}")
        print(f"   æ¨¡å—æ•°é‡: {len(target_modules)} (é»˜è®¤7ä¸ªï¼Œå½“å‰{len(target_modules)}ä¸ª)")
        if len(target_modules) < 7:
            reduction = (1 - len(target_modules) / 7) * 100
            print(f"   âš¡ LoRAå‚æ•°å‡å°‘çº¦ {reduction:.1f}%ï¼Œæ˜¾å­˜å ç”¨ç›¸åº”å‡å°‘")
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.lora_r,
            lora_alpha=self.lora_alpha,
            lora_dropout=self.lora_dropout,
            target_modules=target_modules
            # ç§»é™¤ modules_to_save=["embed_tokens"]
        )
        
        self.base_model = get_peft_model(self.base_model, lora_config)
        print(f"âœ… LoRAé…ç½®å®Œæˆ")
        
        # å†æ¬¡æ£€æŸ¥æ•°æ®ç±»å‹å’Œè®¾å¤‡
        first_param = next(self.base_model.parameters())
        model_dtype = first_param.dtype
        model_device = first_param.device
        print(f"ğŸ”§ LoRAåæ¨¡å‹æ•°æ®ç±»å‹: {model_dtype}, è®¾å¤‡: {model_device}")
        
        # æ·»åŠ ï¼šåªå…è®¸ç‰¹æ®Štokençš„embeddingå¯è®­ç»ƒ
        self._freeze_embeddings_except_special_tokens()

    def _freeze_embeddings_except_special_tokens(self):
        """å†»ç»“é™¤äº†ç‰¹æ®Štokenä»¥å¤–çš„æ‰€æœ‰embeddingå‚æ•° - ä¿®å¤ç‰ˆ"""
        print("ğŸ§Š å†»ç»“é™¤ç‰¹æ®Štokenå¤–çš„æ‰€æœ‰embeddingå‚æ•°...")
        
        # è·å–æ­£ç¡®çš„embeddingå±‚ - ä½¿ç”¨get_input_embeddings()æ–¹æ³•ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰
        # å¯¹äºQwen3-VLæ¨¡å‹ï¼Œè¿™ä¼šè‡ªåŠ¨æ‰¾åˆ°æ­£ç¡®çš„embeddingå±‚
        try:
            embedding_layer = self.base_model.get_input_embeddings()
        except AttributeError:
            # å¦‚æœget_input_embeddings()ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            print("âš ï¸ æ— æ³•é€šè¿‡get_input_embeddings()è·å–embeddingå±‚ï¼Œå°è¯•ç›´æ¥è®¿é—®...")
            try:
                embedding_layer = self.base_model.model.model.embed_tokens
            except:
                print("âŒ æ— æ³•æ‰¾åˆ°embeddingå±‚ï¼")
                return
            
        # å»ºç«‹è¡Œçº§æ¢¯åº¦æ©ç ï¼šä»…å…è®¸ç‰¹æ®Štokenï¼ˆä¸å«<|memory_pad|>ï¼‰æ›´æ–°
        vocab_size = embedding_layer.weight.shape[0]
        row_mask = torch.zeros(vocab_size, dtype=torch.bool, device=embedding_layer.weight.device)
        for token_id in self.trainable_special_token_ids.values():
            row_mask[token_id] = True

        # å¼€å¯å…¨å±€requires_gradï¼Œä½¿ç”¨hookå±è”½éè®­ç»ƒtokençš„æ¢¯åº¦
        embedding_layer.weight.requires_grad_(True)

        def _mask_grad(grad):
            if grad is None:
                return grad
            mask = row_mask.to(grad.device, dtype=grad.dtype).unsqueeze(-1)
            return grad * mask

        embedding_layer.weight.register_hook(_mask_grad)

        total_embedding_params = embedding_layer.weight.numel()
        trainable_params = sum(embedding_layer.weight[token_id].numel() for token_id in self.trainable_special_token_ids.values())
        is_trainable = all(row_mask[token_id].item() for token_id in self.trainable_special_token_ids.values())
        
        print(f"âœ… embeddingå±‚è®¾ç½®å®Œæˆ:")
        print(f"   embeddingå±‚è·¯å¾„: {embedding_layer.__class__.__name__}")
        print(f"   æ€»embeddingå‚æ•°: {total_embedding_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({len(self.trainable_special_token_ids)} ä¸ªç‰¹æ®Štoken)")
        print(f"   å¯è®­ç»ƒtoken: {list(self.trainable_special_token_ids.keys())}")
        print(f"   å†»ç»“å‚æ•°: {total_embedding_params - trainable_params:,}")
        print(f"   ç‰¹æ®Štoken embeddingæ˜¯å¦å¯è®­ç»ƒ: {is_trainable}")
        
        # è°ƒè¯•ä¿¡æ¯
        if not is_trainable:
            print("âš ï¸ è­¦å‘Š: ç‰¹æ®Štoken embeddingæ— æ³•è®¾ç½®ä¸ºå¯è®­ç»ƒï¼å°è¯•å¤‡ç”¨æ–¹æ³•...")
            # å¤‡ç”¨æ–¹æ³•
            for token, token_id in self.trainable_special_token_ids.items():
                param_pointer = embedding_layer.weight[token_id]
                param_pointer.requires_grad = True
                print(f"   {token} ID={token_id}: {param_pointer.requires_grad}")
    
    def _print_parameters(self):
        """æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°ç»Ÿè®¡ - æ›´æ–°ä¸ºåªç»Ÿè®¡ç‰¹æ®Štoken embedding"""
        print("ğŸ“Š å‚æ•°ç»Ÿè®¡ (ä»…ç‰¹æ®Štoken embeddingå¯è®­ç»ƒ):")
        
        # è·å–æ­£ç¡®çš„embeddingå±‚è·¯å¾„ - ä½¿ç”¨get_input_embeddings()æ–¹æ³•ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰
        try:
            embedding_layer = self.base_model.get_input_embeddings()
            # åªç»Ÿè®¡å¯è®­ç»ƒçš„ç‰¹æ®Štokenï¼ˆä¸åŒ…æ‹¬<|memory_pad|>ï¼‰
            trainable_token_embeddings = [embedding_layer.weight[token_id] for token_id in self.trainable_special_token_ids.values()]
            # æ‰€æœ‰ç‰¹æ®Štokenç”¨äºæ˜¾ç¤ºçŠ¶æ€
            all_special_token_embeddings = [embedding_layer.weight[token_id] for token_id in self.special_token_ids.values()]
        except AttributeError:
            # å¦‚æœget_input_embeddings()ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            try:
                embedding_layer = self.base_model.model.model.embed_tokens
                trainable_token_embeddings = [embedding_layer.weight[token_id] for token_id in self.trainable_special_token_ids.values()]
                all_special_token_embeddings = [embedding_layer.weight[token_id] for token_id in self.special_token_ids.values()]
            except Exception as e:
                print(f"âš ï¸ æ— æ³•è·å–embeddingå±‚: {e}")
                embedding_layer = None
                trainable_token_embeddings = []
                all_special_token_embeddings = []
        
        # ç»Ÿè®¡å‚æ•°
        lora_params = 0
        embedding_params = 0
        other_params = 0
        
        # æ£€æŸ¥embeddingå±‚æ˜¯å¦å¯è®­ç»ƒï¼ˆåªç»Ÿè®¡å¯è®­ç»ƒçš„tokenï¼‰
        if embedding_layer is not None:
            is_trainable = all(emb.requires_grad for emb in trainable_token_embeddings)
            if is_trainable:
                embedding_params = sum(emb.numel() for emb in trainable_token_embeddings)
        
        # ç»Ÿè®¡æ‰€æœ‰å‚æ•°
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                if 'lora' in name.lower():
                    lora_params += param.numel()
                elif 'embed' in name.lower() and 'embed_tokens.weight' in name:
                    # å·²åœ¨å‰é¢è®¡ç®—ï¼Œä¸é‡å¤è®¡ç®—
                    pass
                else:
                    other_params += param.numel()
        
        total_trainable = lora_params + embedding_params + other_params
        total_params = sum(p.numel() for p in self.model.parameters())
        
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {total_trainable:,}")
        print(f"     - LoRAå‚æ•°: {lora_params:,}")
        print(f"     - ç‰¹æ®ŠToken Embeddingå‚æ•°: {embedding_params:,}")
        print(f"     - å…¶ä»–å‚æ•°: {other_params:,}")
        print(f"   å¯è®­ç»ƒæ¯”ä¾‹: {100 * total_trainable / total_params:.4f}%")
        
        # æ£€æŸ¥ç‰¹æ®Štokençš„embeddingçŠ¶æ€
        if embedding_layer is not None and all_special_token_embeddings:
            print(f"\nğŸ¯ ç‰¹æ®ŠtokençŠ¶æ€:")
            for token, token_id in self.special_token_ids.items():
                special_token_embedding = embedding_layer.weight[token_id]
                is_trainable = token in self.trainable_special_token_ids
                trainable_mark = " (å¯è®­ç»ƒ)" if is_trainable else " (ä¸å¯è®­ç»ƒï¼Œå ä½ç¬¦)"
                print(f"   {token} (ID={token_id}): requires_grad={special_token_embedding.requires_grad}{trainable_mark}")
                print(f"     èŒƒå›´: [{special_token_embedding.min().item():.6f}, {special_token_embedding.max().item():.6f}]")
    
    def load_data(self, pt_file_path):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        print(f"ğŸ“– åŠ è½½æ•°æ®: {pt_file_path}")
        
        if not os.path.exists(pt_file_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {pt_file_path}")
        
        data = torch.load(pt_file_path, map_location='cpu')
        texts = data['texts']
        embeddings = data['embeddings']
        
        print(f"   æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"   è¡¨å¾å‘é‡å½¢çŠ¶: {embeddings.shape}")
        print(f"   åŸå§‹embeddingæ•°æ®ç±»å‹: {embeddings.dtype}")
        
        return texts, embeddings
    
    def create_dataloader(self, texts, embeddings, batch_size=2, shuffle=True, noise_std=0.01, sft_full_texts=None):
        """åˆ›å»ºå¢å¼ºçš„æ•°æ®åŠ è½½å™¨"""
        dataset = EnhancedTextMemoryDataset(
            texts,
            embeddings,
            self.tokenizer,
            self.base_model,
            max_length=self.dataset_max_length,
            noise_std=noise_std,
            is_main_process_fn=self.is_main_process,
            sft_full_texts=sft_full_texts,
            activation_prompts=self.activation_prompts,
            end_prompts=self.end_prompts,
            guide_text=self.guide_text,
        )
        # è®© Accelerator æ¥ç®¡ sampler/loader
        loader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            collate_fn=enhanced_collate_fn
        )
        return loader, dataset
    
    def create_mixed_dataloader(
        self,
        memory_texts,
        memory_embeddings,
        sft_messages_list,
        batch_size=2,
        shuffle=True,
        noise_std=0.01,
        sft_full_texts=None,
        sft_message_source_indices=None,
        sft_full_source_indices=None
    ):
        """åˆ›å»ºæ··åˆæ•°æ®åŠ è½½å™¨ï¼ˆè®°å¿†æ¡ç›®+SFTæ•°æ®ï¼‰"""
        dataset = MixedMemorySFTDataset(
            memory_texts,
            memory_embeddings,
            sft_messages_list,
            self.tokenizer,
            self.base_model,
            max_length=self.dataset_max_length,
            noise_std=noise_std,
            is_main_process_fn=self.is_main_process,
            sft_full_texts=sft_full_texts,
            activation_prompts=self.activation_prompts,
            end_prompts=self.end_prompts,
            memory_ratio=0.5,  # è®°å¿†æ¡ç›®å 50%
            guide_text=self.guide_text,
            sft_message_source_indices=sft_message_source_indices,
            sft_full_source_indices=sft_full_source_indices
        )
        # è®© Accelerator æ¥ç®¡ sampler/loader
        loader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            collate_fn=enhanced_collate_fn
        )
        return loader, dataset
    
    def train_epoch(self, dataloader, dataset, optimizer, epoch_idx=0):
        """è®­ç»ƒä¸€ä¸ªepoch - åœ¨å¼€å§‹æ—¶åˆ·æ–°æ•°æ®é…å¯¹"""
        # æ¯ä¸ªepochå¼€å§‹æ—¶åˆ·æ–°æ•°æ®é…å¯¹
        if self.is_main_process():
            print(f"\nğŸ”„ Epoch {epoch_idx + 1} æ•°æ®åˆ·æ–°ä¸­...")
        dataset.refresh_epoch_data()
        # åˆ†å¸ƒå¼é‡‡æ ·å™¨è®¾ç½®epoch
        if self.ddp_enabled and isinstance(dataloader.sampler, DistributedSampler):
            dataloader.sampler.set_epoch(epoch_idx)
        
        self.model.train()
        total_loss = 0
        accumulation_step = 0

        progress_bar = tqdm(dataloader, desc="è®­ç»ƒ", disable=not self.is_main_process())

        for batch in progress_bar:
            input_ids = batch['input_ids']
            embeddings_to_insert = batch['embeddings_to_insert']
            embedding_positions = batch['embedding_positions']
            attention_mask = batch['attention_mask']
            labels = batch['labels']
            batch_info = batch.get('batch_info', {})
            has_sft = batch_info.get('has_sft', False)
            has_memory = batch_info.get('has_memory', False)

            # å‰å‘ä¼ æ’­ï¼šç»Ÿä¸€ä½¿ç”¨forwardæ–¹æ³•ï¼Œæ¨¡å‹å†…éƒ¨ä¼šæ ¹æ®embeddings_to_insertæ˜¯å¦ä¸ºå ä½ç¬¦è‡ªåŠ¨åˆ¤æ–­
            # å¯¹äºSFTæ ·æœ¬ï¼Œembeddings_to_insertæ˜¯å…¨é›¶å ä½ç¬¦ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä½¿ç”¨æ ‡å‡†å‰å‘ä¼ æ’­
            outputs = self.model(
                input_ids=input_ids,
                embeddings_to_insert=embeddings_to_insert,
                embedding_positions=embedding_positions,
                attention_mask=attention_mask,
                labels=labels,
                memory_pad_token_id=self.memory_pad_id
            )

            loss = outputs['loss']

            # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±é™¤ä»¥ç´¯ç§¯æ­¥æ•°
            loss = loss / self.gradient_accumulation_steps

            # åå‘ä¼ æ’­
            self.accelerator.backward(loss)

            accumulation_step += 1

            # æ¯gradient_accumulation_stepsæ­¥æ‰§è¡Œä¸€æ¬¡ä¼˜åŒ–å™¨æ­¥éª¤
            if accumulation_step % self.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(self.base_model.parameters(), max_norm=1.0)
                optimizer.step()
                optimizer.zero_grad()

            # ç´¯ç§¯æŸå¤±ï¼ˆæ³¨æ„ï¼šè¿™é‡Œç´¯ç§¯çš„æ˜¯åŸå§‹æŸå¤±ï¼Œä¸æ˜¯é™¤ä»¥ç´¯ç§¯æ­¥æ•°çš„æŸå¤±ï¼‰
            total_loss += loss.item() * self.gradient_accumulation_steps

            if self.is_main_process():
                progress_bar.set_postfix({
                    'loss': f'{loss.item() * self.gradient_accumulation_steps:.6f}'
                })

        # å¤„ç†æœ€åä¸€ä¸ªepochä¸­å‰©ä½™çš„æ¢¯åº¦ç´¯ç§¯
        if accumulation_step % self.gradient_accumulation_steps != 0:
            torch.nn.utils.clip_grad_norm_(self.base_model.parameters(), max_norm=1.0)
            optimizer.step()
            optimizer.zero_grad()
        
        avg_loss = total_loss / len(dataloader)
        
        if self.is_main_process():
            print(f"   å¹³å‡æŸå¤±: {avg_loss:.6f}")

        return {
            'total_loss': avg_loss
        }
    
    def test_memory_recall(self, texts, embeddings, num_samples=5, max_new_tokens=300, sft_full_texts=None):
        """æµ‹è¯•è®°å¿†å›å¿†èƒ½åŠ› - ä½¿ç”¨ä¸è®­ç»ƒä¸€è‡´çš„æ•°æ®æ„å»ºå½¢å¼ï¼ˆSFTå®Œæ•´æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰"""
        
        if not self.is_main_process():
            return {"skipped_on_non_main_process": True}

        print(f"\nğŸ§  æµ‹è¯•è®°å¿†å›å¿†èƒ½åŠ› (æ£€æµ‹token ID)...")
        effective_sample_count = min(num_samples, len(texts))
        print(f"   æµ‹è¯•æ ·æœ¬æ•°: {effective_sample_count}")
        print(f"   æœ€å¤§ç”Ÿæˆé•¿åº¦: {max_new_tokens}")
        if sft_full_texts:
            print(f"   ä½¿ç”¨SFTå®Œæ•´æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡: {len(sft_full_texts)} æ¡")
        
        import random
        test_indices = random.sample(range(len(texts)), effective_sample_count)
        
        tokenizer = self._get_tokenizer()
        recall_start_id = self.special_token_ids.get('<recall>')
        recall_id = self.special_token_ids.get('<|recall|>')
        recall_end_id = self.special_token_ids.get('</recall>')
        
        if recall_start_id is None:
            recall_start_id = tokenizer.convert_tokens_to_ids('<recall>')
        if recall_end_id is None:
            recall_end_id = tokenizer.convert_tokens_to_ids('</recall>')
        
        print(f"ğŸ” ç‰¹æ®Štoken ID:")
        print(f"   <recall>: {recall_start_id}")
        if recall_id is not None:
            print(f"   <|recall|>: {recall_id}")
        print(f"   </recall>: {recall_end_id}")
        print(f"   EOS token: {tokenizer.eos_token_id}")
        
        self.merged_model.eval()

        gen_cfg = self.generation_config or {}
        cfg_max_new_tokens = gen_cfg.get("max_new_tokens")
        do_sample = gen_cfg.get("do_sample", True)
        temperature = gen_cfg.get("temperature", 1.0)
        top_p = gen_cfg.get("top_p", 0.95)
        top_k = gen_cfg.get("top_k", 20)
        repetition_penalty = gen_cfg.get("repetition_penalty", 1.0)

        pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id
        eos_token_id = tokenizer.eos_token_id
        
        # ä½¿ç”¨trainerä¸­å·²è®¾ç½®çš„memory_pad_id
        memory_pad_id = self.memory_pad_id
        
        # ç¼–ç <recall> token
        recall_tokens = tokenizer(self.recall_start_token, add_special_tokens=False)['input_ids']
        recall_token_count = len(recall_tokens)
        
        # ä¸ºäº†æµ‹è¯•æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œæµ‹è¯•æ—¶ä¹Ÿåº”è¯¥æœ‰ä¸Šä¸‹æ–‡
        # ä½¿ç”¨å’Œè®­ç»ƒæ—¶ä¸€æ ·çš„ä¸Šä¸‹æ–‡å¤„ç†æ–¹å¼ï¼šä»SFTæ•°æ®ä¸­éšæœºé€‰æ‹©å¹¶æˆªæ–­
        test_context_text = ""
        
        # å¦‚æœæä¾›äº†SFTæ•°æ®ï¼Œä½¿ç”¨å’Œè®­ç»ƒæ—¶ä¸€æ ·çš„æˆªæ–­æ–¹å¼
        if sft_full_texts and len(sft_full_texts) > 0:
            import random
            # å–ä¸€ä¸ªå®‰å…¨çš„embeddingæ ·æœ¬ï¼ˆtensor/listéƒ½å¯ï¼‰
            def _pick_one_embedding(embs):
                try:
                    hidden_size = getattr(self.merged_model.config, "hidden_size", 4096)
                except Exception:
                    hidden_size = 4096
                if isinstance(embs, torch.Tensor):
                    if embs.numel() == 0:
                        return torch.zeros((1, hidden_size), device=embs.device)
                    return embs[:1]
                if isinstance(embs, (list, tuple)) and len(embs) > 0:
                    first = embs[0]
                    if isinstance(first, torch.Tensor):
                        if first.dim() == 1:
                            first = first.unsqueeze(0)
                        return first[:1]
                    try:
                        return torch.tensor(first, dtype=torch.float32).unsqueeze(0)
                    except Exception:
                        return torch.zeros((1, hidden_size))
                return torch.zeros((1, hidden_size))

            # éšæœºé€‰æ‹©ä¸€ä¸ªSFTæ•°æ®
            sft_data = random.choice(sft_full_texts)
            # ä½¿ç”¨å’Œè®­ç»ƒæ—¶ä¸€æ ·çš„æˆªæ–­æ–¹æ³•ï¼š_split_sft_at_thinking
            # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„datasetå¯¹è±¡æ¥ä½¿ç”¨è¿™ä¸ªæ–¹æ³•
            temp_dataset_for_context = EnhancedTextMemoryDataset(
                texts[:1] if texts else ["dummy"],  # åªéœ€è¦ä¸€ä¸ªdummy text
                _pick_one_embedding(embeddings),  # åªéœ€è¦ä¸€ä¸ªdummy embedding
                self.tokenizer,
                self.merged_model,
                max_length=self.dataset_max_length,
                noise_std=0.0,
                is_main_process_fn=self.is_main_process,
                sft_full_texts=sft_full_texts,
                activation_prompts=self.activation_prompts,
                end_prompts=self.end_prompts,
                guide_text=self.guide_text,
            )
            # ä½¿ç”¨å’Œè®­ç»ƒæ—¶ä¸€æ ·çš„æˆªæ–­æ–¹æ³•
            test_context_text, _ = temp_dataset_for_context._split_sft_at_thinking(sft_data)
        
        # æµ‹è¯•æ—¶ä½¿ç”¨å›ºå®šçš„æ¿€æ´»æç¤ºè¯­ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªï¼Œç¡®ä¿æµ‹è¯•ä¸€è‡´æ€§ï¼‰
        test_activation_prompt = self.activation_prompts[0].strip() if self.activation_prompts else ""
        
        # ç¼–ç ä¸Šä¸‹æ–‡å’Œæ¿€æ´»æç¤ºè¯­
        context_tokens = tokenizer(test_context_text, add_special_tokens=False)['input_ids'] if test_context_text else []
        activation_tokens = tokenizer(test_activation_prompt, add_special_tokens=False)['input_ids'] if test_activation_prompt else []
        
        # æ„é€ æ ¸å¿ƒè¾“å…¥åºåˆ—ï¼š<recall> + <|memory_pad|>
        core_input_tokens = recall_tokens + [memory_pad_id]
        
        # æ„é€ å®Œæ•´è¾“å…¥åºåˆ—
        full_input_tokens = context_tokens + activation_tokens + core_input_tokens
        embedding_position = len(context_tokens) + len(activation_tokens) + recall_token_count
        
        print(f"ğŸ“‹ æµ‹è¯•é…ç½®:")
        print(f"   ä¸Šä¸‹æ–‡: {'æœ‰ (' + str(len(context_tokens)) + ' tokens)' if test_context_text else 'æ— '}")
        print(f"   æ¿€æ´»æç¤ºè¯­: {test_activation_prompt if test_activation_prompt else 'æ— '}")
        print(f"   æ³¨æ„: ç»“æŸæç¤ºè¯­æ˜¯è®­ç»ƒæ—¶çš„ç›®æ ‡ï¼Œä¸æ˜¯è¾“å…¥çš„ä¸€éƒ¨åˆ†")
        print(f"   è¾“å…¥åºåˆ—é•¿åº¦: {len(full_input_tokens)}")
        print(f"   Embeddingæ’å…¥ä½ç½®: {embedding_position}")
        
        for i, idx in enumerate(test_indices):
            # æ¯æ¬¡æµ‹è¯•å‰å½»åº•æ¸…ç†æ¨¡å‹çŠ¶æ€ï¼Œç¡®ä¿æµ‹è¯•ç‹¬ç«‹
            self.merged_model.eval()
            
            # æ¸…ç†æ¨¡å‹å†…éƒ¨çŠ¶æ€ï¼ˆå¦‚æœæœ‰DDPåŒ…è£…ï¼Œéœ€è¦è®¿é—®base_modelï¼‰
            base_model = self.merged_model.module if hasattr(self.merged_model, 'module') else self.merged_model
            if hasattr(base_model, 'reset_cache'):
                base_model.reset_cache()
            if hasattr(base_model, 'base_model') and hasattr(base_model.base_model, 'reset_cache'):
                base_model.base_model.reset_cache()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            original_text = texts[idx]
            # ç›´æ¥ä½¿ç”¨åŸå§‹embeddingï¼Œä¸é€šè¿‡datasetè·å–
            embedding_to_insert = embeddings[idx]
            # ç¡®ä¿æ ·æœ¬embeddingä¸å…±äº«å¼•ç”¨
            if isinstance(embedding_to_insert, torch.Tensor):
                embedding_to_insert = embedding_to_insert.clone()
            
            print(f"\n{'='*80}")
            print(f"ğŸ§ª æµ‹è¯•æ ·æœ¬ {i+1}/{num_samples} (ç´¢å¼•: {idx})")
            print(f"ğŸ“ åŸå§‹æ–‡æœ¬: {original_text}")
            if test_context_text:
                print(f"ğŸ“‹ æµ‹è¯•ä¸Šä¸‹æ–‡: {test_context_text[:200]}..." if len(test_context_text) > 200 else f"ğŸ“‹ æµ‹è¯•ä¸Šä¸‹æ–‡: {test_context_text}")
            if test_activation_prompt:
                print(f"ğŸ“‹ æ¿€æ´»æç¤ºè¯­: {test_activation_prompt}")
            print(f"ğŸ“‹ æœŸæœ›ç”Ÿæˆ: è®°å¿†æ–‡æœ¬ + </recall> + ç»“æŸæç¤ºè¯­")
            
            try:
                # ç›´æ¥ä½¿ç”¨åŸå§‹embeddingæ„å»ºæµ‹è¯•è¾“å…¥ï¼ˆä¸é€šè¿‡datasetï¼‰
                sequence_tokens = torch.tensor(full_input_tokens, dtype=torch.long)
                
                # æ„å»ºè¾“å…¥embeddingsï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                embedding_layer = self.merged_model.get_input_embeddings()
                device = next(self.merged_model.parameters()).device
                
                # ç¡®ä¿embeddingåœ¨æ­£ç¡®çš„è®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸Š
                # è·å–æ¨¡å‹çš„æ•°æ®ç±»å‹
                model_dtype = next(self.merged_model.parameters()).dtype
                embedding_to_insert = embedding_to_insert.to(device).to(model_dtype)
                
                # å°†tokenåºåˆ—è½¬æ¢ä¸ºembeddings
                sequence_tokens = sequence_tokens.to(device)
                token_embeddings = embedding_layer(sequence_tokens.unsqueeze(0))  # [1, seq_len, embed_dim]
                
                # æ›¿æ¢embedding placeholderä½ç½®çš„embeddingä¸ºå®é™…çš„è®°å¿†å‘é‡
                token_embeddings[0, embedding_position] = embedding_to_insert
                
                # æ„å»ºattention mask
                attention_mask = torch.ones(1, sequence_tokens.shape[0], device=device, dtype=torch.long)
                
                prefix_embeddings = token_embeddings
                prefix_attention_mask = attention_mask
                
                print(f"ğŸš€ è¾“å…¥åºåˆ—: [ä¸Šä¸‹æ–‡] + <recall> + [è®°å¿†å‘é‡] (æ€»é•¿åº¦: {sequence_tokens.shape[0]})")

                requested_max_new_tokens = max_new_tokens or self.test_max_new_tokens
                cfg_limit = cfg_max_new_tokens or requested_max_new_tokens
                effective_max_new_tokens = min(requested_max_new_tokens, cfg_limit, self.test_max_new_tokens)
                print(
                    f"ğŸ¯ å¼€å§‹ç”Ÿæˆï¼ˆdo_sample={do_sample}, temperature={temperature}, top_p={top_p}, "
                    f"top_k={top_k}, repetition_penalty={repetition_penalty}, max_new_tokens={effective_max_new_tokens})..."
                )

                with torch.no_grad():
                    # ç¡®ä¿æ¯æ¬¡ç”Ÿæˆéƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¼ å…¥past_key_values
                    generate_kwargs = {
                        "inputs_embeds": prefix_embeddings,
                        "attention_mask": prefix_attention_mask,
                        "max_new_tokens": effective_max_new_tokens,
                        "pad_token_id": pad_token_id if pad_token_id is not None else eos_token_id,
                        "eos_token_id": eos_token_id,
                        "return_dict_in_generate": True,
                        "do_sample": do_sample,
                        "use_cache": self.test_use_cache,
                        "past_key_values": None,  # æ˜ç¡®è®¾ç½®ä¸ºNoneï¼Œç¡®ä¿ä¸ä½¿ç”¨ä¹‹å‰çš„ç¼“å­˜
                    }

                    if do_sample:
                        generate_kwargs["temperature"] = temperature
                        generate_kwargs["top_p"] = top_p
                        if top_k is not None:
                            generate_kwargs["top_k"] = max(int(top_k), 0)
                    # elseåˆ†æ”¯ï¼šdo_sample=Falseæ—¶ä¸éœ€è¦è®¾ç½®temperatureã€top_pã€top_k

                    if repetition_penalty and repetition_penalty != 1.0:
                        generate_kwargs["repetition_penalty"] = repetition_penalty

                    generated_output = self.merged_model.generate(**generate_kwargs)

                sequences = generated_output.sequences if hasattr(generated_output, "sequences") else generated_output
                generated_ids_full = sequences[0].tolist()
                prefix_len = prefix_embeddings.shape[1]
                generated_ids = generated_ids_full[prefix_len:] if len(generated_ids_full) > prefix_len else generated_ids_full

                if len(generated_ids) >= effective_max_new_tokens:
                    print(f"   âš ï¸ è¾¾åˆ°æœ€å¤§ç”Ÿæˆé•¿åº¦ {effective_max_new_tokens}")

                recall_end_count = generated_ids.count(recall_end_id) if recall_end_id is not None else 0
                recall_start_count = generated_ids.count(recall_start_id) if recall_start_id is not None else 0
                recall_mid_count = generated_ids.count(recall_id) if recall_id is not None else 0
                eos_count = generated_ids.count(eos_token_id) if eos_token_id is not None else 0

                print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
                print(f"   æ€»ç”Ÿæˆtokenæ•°: {len(generated_ids)}")
                print(f"   ç”Ÿæˆçš„token IDåˆ—è¡¨: {generated_ids[:10]}..." if len(generated_ids) > 10 else f"   ç”Ÿæˆçš„token IDåˆ—è¡¨: {generated_ids}")
                print(f"   </recall> å‡ºç°æ¬¡æ•°: {recall_end_count}")
                print(f"   <recall> å‡ºç°æ¬¡æ•°: {recall_start_count}")
                if recall_id is not None:
                    print(f"   <|recall|> å‡ºç°æ¬¡æ•°: {recall_mid_count}")
                print(f"   EOS token å‡ºç°æ¬¡æ•°: {eos_count}")
                
                generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=False)
                generated_text_clean = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
                
                print(f"\nğŸ“¤ ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬ï¼ˆåŒ…å«special tokensï¼‰ï¼š")
                print(f"     {generated_text}")
                
                print(f"\nğŸ§¹ ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆç§»é™¤special tokensï¼‰ï¼š")
                print(f"     {generated_text_clean}")
                
                if original_text in generated_text_clean:
                    print(f"ğŸ¯ ç”Ÿæˆæ–‡æœ¬åŒ…å«å®Œæ•´åŸæ–‡")
                elif generated_text_clean in original_text:
                    print(f"ğŸ¯ ç”Ÿæˆæ–‡æœ¬æ˜¯åŸæ–‡çš„ä¸€éƒ¨åˆ†")
                elif len(generated_text_clean) > 0 and original_text.startswith(generated_text_clean[:50]):
                    print(f"ğŸ¯ ç”Ÿæˆæ–‡æœ¬ä¸åŸæ–‡å¼€å¤´åŒ¹é…")
                else:
                    print(f"â“ ç”Ÿæˆæ–‡æœ¬ä¸åŸæ–‡å·®å¼‚è¾ƒå¤§")
                    
            except Exception as e:
                print(f"âŒ ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
            finally:
                # æ¸…ç†æ‰€æœ‰å¯èƒ½å­˜åœ¨çš„å˜é‡å’Œç¼“å­˜
                if 'sequence_tokens' in locals():
                    del sequence_tokens
                if 'embedding_to_insert' in locals():
                    del embedding_to_insert
                if 'token_embeddings' in locals():
                    del token_embeddings
                if 'prefix_embeddings' in locals():
                    del prefix_embeddings
                if 'prefix_attention_mask' in locals():
                    del prefix_attention_mask
                if 'generated_output' in locals():
                    del generated_output
                if 'generated_ids' in locals():
                    del generated_ids
                # æ¸…ç†æ¨¡å‹å¯èƒ½ä¿ç•™çš„å†…éƒ¨çŠ¶æ€
                if hasattr(self.merged_model, 'module'):
                    base_model = self.merged_model.module
                else:
                    base_model = self.merged_model
                
                if hasattr(base_model, 'reset_cache'):
                    base_model.reset_cache()
                if hasattr(base_model, 'base_model') and hasattr(base_model.base_model, 'reset_cache'):
                    base_model.base_model.reset_cache()
                
                # æ¸…ç†æ˜¾å­˜ç¼“å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰CUDAæ“ä½œå®Œæˆ
                
                # æ¸…ç†Pythonå˜é‡å¼•ç”¨
                import gc
                gc.collect()
        
        print(f"\n{'='*80}")
        print("ğŸ” è§‚å¯Ÿä»¥ä¸Štoken IDè¾“å‡ºï¼Œç‰¹åˆ«æ³¨æ„:")
        print("   1. æ˜¯å¦ç”Ÿæˆäº†</recall> token ID")
        print("   2. token IDè®¡æ•°ä¸è§£ç æ–‡æœ¬æ˜¯å¦ä¸€è‡´")
        print("   3. ç”Ÿæˆåºåˆ—çš„å®Œæ•´æ€§")
        
        return {"test_completed": True}
    
    def compare_embeddings(self):
        """æ¯”è¾ƒè®­ç»ƒå‰åç‰¹æ®Štoken embeddingçš„å˜åŒ–"""
        print("\nğŸ” åˆ†æç‰¹æ®Štoken embeddingå˜åŒ–...")
        
        current_embedding_layer = self.merged_model.get_input_embeddings()
        results = {}
        
        for token, token_id in self.special_token_ids.items():
            original_emb = self.original_embeddings[token]
            current_emb = current_embedding_layer.weight[token_id]
            
            # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´å†è®¡ç®—
            if original_emb.dtype != current_emb.dtype:
                original_emb = original_emb.to(current_emb.dtype)
            
            change = torch.abs(current_emb - original_emb).mean().item()
            cosine_sim = nn.CosineSimilarity(dim=0)(current_emb, original_emb).item()
            
            results[token] = {
                'change': change,
                'cosine_similarity': cosine_sim,
                'before_range': (original_emb.min().item(), original_emb.max().item()),
                'after_range': (current_emb.min().item(), current_emb.max().item())
            }
            
            print(f"\n   ğŸ“Š {token} (ID: {token_id}):")
            print(f"      å¹³å‡å˜åŒ–: {change:.6f}")
            print(f"      ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim:.6f}")
            print(f"      è®­ç»ƒå‰èŒƒå›´: [{original_emb.min().item():.4f}, {original_emb.max().item():.4f}]")
            print(f"      è®­ç»ƒåèŒƒå›´: [{current_emb.min().item():.4f}, {current_emb.max().item():.4f}]")
        
        return results
    
    def merge_and_save_model(self, save_path):
        """åˆå¹¶LoRAæƒé‡å¹¶ä¿å­˜æ¨¡å‹"""
        if not self.is_main_process():
            return None
        print("ğŸ”„ åˆå¹¶LoRAæƒé‡...")

        merged_model = self.base_model.merge_and_unload()

        if os.path.isdir(save_path):
            print(f"ğŸ§¹ æ¸…ç†å·²æœ‰çš„æ¨¡å‹è¾“å‡ºç›®å½•: {save_path}")
            shutil.rmtree(save_path)

        os.makedirs(save_path, exist_ok=True)

        # ç”±äºPEFTçŠ¶æ€å¯èƒ½æ··ä¹±ï¼Œå°è¯•ä¸åŒçš„ä¿å­˜æ–¹å¼
        try:
            # é¦–å…ˆå°è¯•æ­£å¸¸ä¿å­˜
            merged_model.save_pretrained(save_path)
        except Exception as e:
            print(f"âš ï¸ æ­£å¸¸ä¿å­˜å¤±è´¥: {e}ï¼Œå°è¯•å¤‡ç”¨ä¿å­˜æ–¹æ³•...")
            # å¦‚æœæ­£å¸¸ä¿å­˜å¤±è´¥ï¼Œå°è¯•ç¦ç”¨adapterç›¸å…³çš„ä¿å­˜
            try:
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ¨¡å‹ï¼Œç§»é™¤æ‰€æœ‰PEFTç›¸å…³å±æ€§
                import copy
                temp_model = copy.deepcopy(merged_model)
                # ç§»é™¤å¯èƒ½å¯¼è‡´é—®é¢˜çš„PEFTå±æ€§
                peft_attrs = ['peft_config', 'active_adapters', 'adapter_config']
                for attr in peft_attrs:
                    if hasattr(temp_model, attr):
                        delattr(temp_model, attr)

                temp_model.save_pretrained(save_path)
                print("âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ä¿å­˜æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ å¤‡ç”¨ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")
                raise e  # æŠ›å‡ºåŸå§‹é”™è¯¯

        # ä¿å­˜tokenizer/processor
        # å¦‚æœself.tokenizeræ˜¯AutoProcessorï¼Œsave_pretrainedä¼šä¿å­˜æ‰€æœ‰ç»„ä»¶ï¼ˆtokenizerã€image_processorã€video_processorç­‰ï¼‰
        # å¦‚æœself.tokenizeræ˜¯AutoTokenizerï¼Œåªä¿å­˜tokenizer
        self.tokenizer.save_pretrained(save_path)

        # å¦‚æœself.tokenizeræ˜¯AutoProcessorï¼Œå·²ç»ä¿å­˜äº†æ‰€æœ‰ç»„ä»¶
        # å¦‚æœself.tokenizeræ˜¯AutoTokenizerï¼Œéœ€è¦ç¡®ä¿processorçš„å…¶ä»–ç»„ä»¶ä¹Ÿè¢«ä¿å­˜
        # ä½†ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬åœ¨training_service.pyçš„_save_processor_to_pathä¸­ä¼šå¤„ç†processorçš„å®Œæ•´ä¿å­˜
        print(f"âœ… åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

        # ç¡®ä¿ç‰¹æ®Štokenåœ¨è¯æ±‡è¡¨ä¸­ï¼ˆè°ƒè¯•ç”¨ï¼‰
        tokenizer = self._get_tokenizer()
        print("ğŸ” ä¿å­˜æ—¶æ£€æŸ¥ç‰¹æ®Štoken...")
        special_tokens = ["<recall>", "</recall>"]
        for token in special_tokens:
            if token in tokenizer.get_vocab():
                token_id = tokenizer.convert_tokens_to_ids(token)
                print(f"   âœ… {token} å­˜åœ¨ (ID: {token_id})")
            else:
                print(f"   âŒ {token} ä¸å­˜åœ¨äºè¯æ±‡è¡¨ä¸­ï¼")
                # é‡æ–°æ·»åŠ 
                num_added = tokenizer.add_tokens([token], special_tokens=True)
                if num_added > 0:
                    print(f"   ğŸ”§ é‡æ–°æ·»åŠ äº† {token}")
                    # é‡æ–°ä¿å­˜
                    tokenizer.save_pretrained(save_path)

        # ç¡®ä¿ç‰¹æ®Štokenåœ¨ç‰¹æ®Štokenåˆ—è¡¨ä¸­
        if hasattr(tokenizer, 'special_tokens_map'):
            additional_special = tokenizer.special_tokens_map.get('additional_special_tokens', [])
            for token in special_tokens:
                if token not in additional_special:
                    print(f"   âš ï¸ {token} ä¸åœ¨ç‰¹æ®Štokenåˆ—è¡¨ä¸­ï¼Œé‡æ–°æ·»åŠ ")
                    if hasattr(tokenizer, 'add_special_tokens'):
                        tokenizer.add_special_tokens({"additional_special_tokens": [token]})
                        # é‡æ–°ä¿å­˜
                        tokenizer.save_pretrained(save_path)

        # ä¿å­˜merged_modelå¼•ç”¨ä¾›compare_embeddings()ä½¿ç”¨
        self.merged_model = merged_model

        return save_path  # è¿”å›ä¿å­˜è·¯å¾„ä¾›åç»­ä½¿ç”¨

    def cleanup(self):
        """æ¸…ç†è®­ç»ƒå™¨åˆ›å»ºçš„æ‰€æœ‰æ¨¡å‹å®ä¾‹"""
        print("ğŸ§¹ æ¸…ç†è®­ç»ƒå™¨æ¨¡å‹å®ä¾‹...")

        try:
            # æ¸…ç†merged_modelï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(self, 'merged_model') and self.merged_model is not None:
                try:
                    self.merged_model.cpu()
                except:
                    pass
                del self.merged_model
                self.merged_model = None

            # æ¸…ç†LoRAåŒ…è£…æ¨¡å‹
            if hasattr(self, 'model') and self.model is not None:
                try:
                    self.model.cpu()
                except:
                    pass
                del self.model
                self.model = None

            # æ¸…ç†base_modelï¼ˆå¦‚æœä¸æ˜¯é¢„åŠ è½½çš„ï¼‰
            if hasattr(self, 'base_model') and self.base_model is not None and not getattr(self, '_skip_model_loading', False):
                try:
                    self.base_model.cpu()
                except:
                    pass
                del self.base_model
                self.base_model = None

            # æ¸…ç†tokenizerï¼ˆå¦‚æœä¸æ˜¯é¢„åŠ è½½çš„ï¼‰
            if hasattr(self, 'tokenizer') and self.tokenizer is not None and not getattr(self, '_skip_model_loading', False):
                del self.tokenizer
                self.tokenizer = None

            # æ¸…ç†accelerator
            if hasattr(self, 'accelerator') and self.accelerator is not None:
                try:
                    self.accelerator.free_memory()
                except:
                    pass
                # æ³¨æ„ï¼šacceleratorå®ä¾‹æœ¬èº«é€šå¸¸ä¸éœ€è¦æ˜¾å¼åˆ é™¤

            # å¼ºåˆ¶åƒåœ¾å›æ”¶å’Œæ˜¾å­˜æ¸…ç†
            import gc
            for _ in range(3):
                gc.collect()

            if torch.cuda.is_available():
                torch.cuda.synchronize()
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.empty_cache()

            # æ¢å¤åŸå§‹CUDA_VISIBLE_DEVICESï¼ˆå¦‚æœåœ¨åˆå§‹åŒ–æ—¶ä¿®æ”¹è¿‡ï¼‰
            if hasattr(self, '_original_cuda_visible_devices') and self._original_cuda_visible_devices is not None:
                original_value = self._original_cuda_visible_devices
                os.environ['CUDA_VISIBLE_DEVICES'] = original_value
                print(f"æ¢å¤åŸå§‹CUDA_VISIBLE_DEVICES: {original_value}")
            elif 'CUDA_VISIBLE_DEVICES' in os.environ and hasattr(self, '_original_cuda_visible_devices'):
                # å¦‚æœåˆå§‹åŒ–æ—¶è®¾ç½®äº†ç¯å¢ƒå˜é‡ï¼Œç°åœ¨åˆ é™¤å®ƒ
                del os.environ['CUDA_VISIBLE_DEVICES']
                print("åˆ é™¤CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡")

            print("âœ… è®­ç»ƒå™¨æ¸…ç†å®Œæˆ")

        except Exception as e:
            print(f"âš ï¸ æ¸…ç†è®­ç»ƒå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")

    def train(
        self,
        pt_file_path,
        num_epochs=20,
        batch_size=4,
        learning_rate=1e-4,
        noise_std=0.01,
        save_path="enhanced_memory_model",
        sft_full_texts=None,
        sft_messages_list=None,
        sft_full_source_indices=None,
        sft_message_source_indices=None
    ):
        """å¢å¼ºçš„è®­ç»ƒæµç¨‹ - æ”¯æŒæ··åˆè®­ç»ƒï¼ˆè®°å¿†æ¡ç›®+SFTæ•°æ®ï¼‰"""
        
        if self.is_main_process():
            print(f"\nğŸš€ å¼€å§‹å¢å¼ºæ–‡æœ¬è®°å¿†è®­ç»ƒï¼ˆæ··åˆæ¨¡å¼ï¼‰")
        print(f"   æ•°æ®æ–‡ä»¶: {pt_file_path}")
        print(f"   æ€»è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   å™ªå£°æ ‡å‡†å·®: {noise_std}")
        print(f"   ä¿å­˜è·¯å¾„: {save_path}")
        if sft_full_texts:
            print(f"   SFTå®Œæ•´æ–‡æœ¬æ•°é‡: {len(sft_full_texts)}")
        if sft_messages_list:
            print(f"   SFTæ¶ˆæ¯åˆ—è¡¨æ•°é‡: {len(sft_messages_list)}")
        
        # åŠ è½½æ•°æ®
        texts, embeddings = self.load_data(pt_file_path)
        
        if self.is_main_process():
            print(f"\nğŸ“Š è®­ç»ƒæ•°æ®:")
            print(f"   è®°å¿†æ¡ç›®æ•°é‡: {len(texts)}")
        
        # åˆ›å»ºæ··åˆæ•°æ®åŠ è½½å™¨ï¼ˆå¦‚æœæä¾›äº†SFTæ•°æ®ï¼‰
        if sft_messages_list and len(sft_messages_list) > 0:
            train_loader, dataset = self.create_mixed_dataloader(
                texts,
                embeddings,
                sft_messages_list,
                batch_size,
                True,
                noise_std,
                sft_full_texts=sft_full_texts,
                sft_message_source_indices=sft_message_source_indices,
                sft_full_source_indices=sft_full_source_indices
            )
        else:
            # å›é€€åˆ°åŸæœ‰çš„æ•°æ®åŠ è½½å™¨
            train_loader, dataset = self.create_dataloader(
                texts, embeddings, batch_size, True, noise_std, sft_full_texts=sft_full_texts
            )
        
        # ä¼˜åŒ–å™¨ - ç¡®ä¿åŒ…å«ç‰¹æ®Štoken embedding
        optimizer_params = []

        # å…ˆåŠ å…¥æ‰€æœ‰å·²è®¾ç½®ä¸ºå¯è®­ç»ƒçš„å‚æ•°
        optimizer_params.extend([p for p in self.base_model.parameters() if p.requires_grad])

        # è·å–embeddingå±‚ï¼Œç¡®ä¿ç‰¹æ®ŠtokenåŒ…å«åœ¨ä¼˜åŒ–å™¨ä¸­
        # ä½¿ç”¨get_input_embeddings()æ–¹æ³•ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ¨¡å‹ç±»å‹ï¼‰
        try:
            embedding_layer = self.base_model.get_input_embeddings()
        except AttributeError:
            # å¦‚æœget_input_embeddings()ä¸å­˜åœ¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            try:
                embedding_layer = self.base_model.model.model.embed_tokens
            except:
                print("âš ï¸ ä¼˜åŒ–å™¨åˆ›å»ºæ—¶æ— æ³•æ‰¾åˆ°embeddingå±‚")
                embedding_layer = None

        if embedding_layer is not None:
            # æ£€æŸ¥å¯è®­ç»ƒçš„ç‰¹æ®Štokenæ˜¯å¦å·²è®¾ç½®ä¸ºå¯è®­ç»ƒï¼ˆä¸åŒ…æ‹¬<|memory_pad|>ï¼‰
            for token, token_id in self.trainable_special_token_ids.items():
                special_token_embedding = embedding_layer.weight[token_id]
                if not special_token_embedding.requires_grad:
                    print(f"âš ï¸ {token} embeddingæœªè®¾ç½®ä¸ºå¯è®­ç»ƒï¼Œæ‰‹åŠ¨æ·»åŠ åˆ°ä¼˜åŒ–å™¨...")
                    special_token_embedding.requires_grad_(True)
                    # å¦‚æœä¸åœ¨optimizer_paramsä¸­ï¼Œæ·»åŠ å®ƒ
                    if all(id(special_token_embedding) != id(p) for p in optimizer_params):
                        optimizer_params.append(special_token_embedding)

        optimizer = optim.AdamW(
            optimizer_params,
            lr=learning_rate,
            weight_decay=0.01
        )
        # è®© Accelerator æ¥ç®¡ï¼ˆæ¨¡å‹/ä¼˜åŒ–å™¨/æ•°æ®ï¼‰
        self.model, optimizer, train_loader = self.accelerator.prepare(
            self.model, optimizer, train_loader
        )
        
        # è®­ç»ƒå¾ªç¯
        training_history = {
            'total_loss': []
        }
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            
            # æ¯ä¸ªepochå¼€å§‹æ—¶ï¼Œæ‰“å°ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ä»¥ä¾›æ£€æŸ¥
            if self.is_main_process() and len(dataset) > 0:
                try:
                    # ä¼˜å…ˆæ‰¾åˆ°è®°å¿†æ¡ç›®æ ·æœ¬ï¼ˆä¸æ˜¯SFTæ ·æœ¬ï¼‰
                    sample_idx = 0
                    sample = None
                    for i in range(min(10, len(dataset))):  # æœ€å¤šæ£€æŸ¥å‰10ä¸ªæ ·æœ¬
                        candidate = dataset[i]
                        if not candidate.get('is_sft', False):
                            sample = candidate
                            sample_idx = i
                            break
                    if sample is None:
                        # å¦‚æœå‰10ä¸ªéƒ½æ˜¯SFTæ ·æœ¬ï¼Œå°±ä½¿ç”¨ç¬¬ä¸€ä¸ª
                        sample_idx = 0
                        sample = dataset[sample_idx]
                    
                    sample_type = sample.get('sample_type', 'memory')
                    type_display = {
                        'memory_front': 'è®°å¿†æ¡ç›®ï¼ˆå‰ç½®SFTï¼‰',
                        'memory_full': 'è®°å¿†æ¡ç›®ï¼ˆå‰åæ‹¼æ¥SFTï¼‰',
                        'sft_only': 'SFTæ ·æœ¬',
                        'memory': 'è®°å¿†æ¡ç›®'
                    }.get(sample_type, sample_type)
                    context_text = sample.get('context_text', '')
                    memory_text = sample.get('text', '')
                    activation_prompt = sample.get('activation_prompt', '')
                    end_prompt = sample.get('end_prompt', '')
                    is_sft = sample.get('is_sft', False)
                    
                    print(f"\nğŸ“‹ Epoch {epoch+1} è®­ç»ƒæ ·æœ¬ç¤ºä¾‹ï¼ˆç´¢å¼• {sample_idx}ï¼Œç±»å‹: {type_display}ï¼‰:")
                    if not is_sft:
                        print(f"   ä¸Šä¸‹æ–‡ï¼ˆæˆªæ–­çš„SFTæ–‡æœ¬ï¼‰: {context_text[:200]}..." if len(context_text) > 200 else f"   ä¸Šä¸‹æ–‡ï¼ˆæˆªæ–­çš„SFTæ–‡æœ¬ï¼‰: {context_text}")
                        print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                        print(f"   è®°å¿†æ¿€æ´»å¼•å¯¼: {activation_prompt if activation_prompt else '(ç©º)'}")
                        print(f"   å›å¿†ç»“æŸå¼•å¯¼: {end_prompt if end_prompt else '(ç©º)'}")
                        print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                        print(f"   è®°å¿†æ–‡æœ¬: {memory_text[:200]}..." if len(memory_text) > 200 else f"   è®°å¿†æ–‡æœ¬: {memory_text}")
                    else:
                        print(f"   SFTæ ·æœ¬æ–‡æœ¬: {memory_text[:200]}..." if len(memory_text) > 200 else f"   SFTæ ·æœ¬æ–‡æœ¬: {memory_text}")
                    print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
                    # æ˜¾ç¤ºå®Œæ•´çš„è®­ç»ƒæ ·æœ¬ï¼šè¾“å…¥åºåˆ— + ç›®æ ‡åºåˆ—ï¼ˆåŒ…å«æ‰€æœ‰ç‰¹æ®Štokenï¼‰
                    tokenizer = self._get_tokenizer()

                    # è·å–è¾“å…¥åºåˆ—ï¼ˆä¸Šä¸‹æ–‡ + <recall> + <|memory_pad|>ï¼‰
                    input_tokens = sample.get('sequence_tokens')
                    # è·å–æ ‡ç­¾åºåˆ—ï¼ˆ-100 * è¾“å…¥é•¿åº¦ + ç›®æ ‡æ–‡æœ¬tokenï¼‰
                    labels = sample.get('labels')

                    if input_tokens is not None and labels is not None:
                        # å°†tensorè½¬æ¢ä¸ºlist
                        if isinstance(input_tokens, torch.Tensor):
                            input_tokens = input_tokens.cpu().tolist()
                        if isinstance(labels, torch.Tensor):
                            labels = labels.cpu().tolist()

                        recall_token_count = sample.get('recall_token_count', 1)
                        # è®¡ç®—<recall>åœ¨labelsä¸­çš„ä½ç½®ï¼šcontext + activation_promptä¹‹å
                        context_length = sample.get('context_length', 0)
                        activation_prompt = sample.get('activation_prompt', '')
                        tokenizer = self._get_tokenizer()
                        activation_tokens = tokenizer(activation_prompt, add_special_tokens=False)['input_ids'] if activation_prompt else []
                        recall_label_start = context_length + len(activation_tokens)
                        prefix_len = len(input_tokens)  # è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆåŒ…æ‹¬<recall>å’Œ<|memory_pad|>ï¼‰
                        
                        # æ˜¾ç¤ºä½ç½®ä¿¡æ¯
                        embedding_position = sample.get('embedding_position', 0)
                        recall_position_in_input = context_length + len(activation_tokens)
                        print(f"   <recall>åœ¨è¾“å…¥åºåˆ—ä¸­çš„ä½ç½®: {recall_position_in_input}")
                        print(f"   è®°å¿†å‘é‡æ’å…¥ä½ç½®: {embedding_position}")
                        print(f"   ä½ç½®å…³ç³»: {'âœ… è®°å¿†å‘é‡åœ¨<recall>ä¹‹å' if embedding_position >= recall_position_in_input + recall_token_count else 'âš ï¸ ä½ç½®å¼‚å¸¸'}")

                        # æ„é€ å®Œæ•´åºåˆ—ï¼šæ­£ç¡®åŒºåˆ†è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—
                        # æ•°æ®ç»“æ„ï¼š
                        # - è¾“å…¥åºåˆ— (input_tokens): [context] [activation] <recall> <|memory_pad|>
                        # - æ ‡ç­¾åºåˆ— (labels): [-100...] [<recall>çš„ID] [-100] [memory_text] </recall> [end_prompt]
                        # æ‰“å°æ—¶åº”è¯¥ï¼š
                        # - è¾“å…¥éƒ¨åˆ†ï¼ˆi < prefix_lenï¼‰ï¼šä½¿ç”¨input_tokensï¼ˆåŒ…æ‹¬<recall>å’Œ<|memory_pad|>ï¼‰
                        # - ç›®æ ‡éƒ¨åˆ†ï¼ˆi >= prefix_lenï¼‰ï¼šä½¿ç”¨labelsï¼ˆä»memory_textå¼€å§‹ï¼‰
                        full_sequence = []
                        for i in range(len(labels)):
                            if i < prefix_len:
                                # è¾“å…¥åºåˆ—éƒ¨åˆ†ï¼šå§‹ç»ˆä½¿ç”¨input_tokensï¼ˆå³ä½¿labels[i]ä¸æ˜¯-100ï¼Œå¦‚<recall>ä½ç½®ï¼‰
                                full_sequence.append(input_tokens[i])
                            else:
                                # ç›®æ ‡åºåˆ—éƒ¨åˆ†ï¼šä½¿ç”¨labelsï¼ˆè¿™äº›æ˜¯memory_text + </recall> + end_promptï¼‰
                                full_sequence.append(labels[i])

                        # è§£ç å®Œæ•´åºåˆ—
                        decoded_sample = tokenizer.decode(full_sequence, skip_special_tokens=False)

                        # æ˜¾ç¤ºå®Œæ•´æ ·æœ¬ï¼Œä½†é™åˆ¶æ€»é•¿åº¦ä»¥é¿å…è¾“å‡ºè¿‡é•¿
                        max_display_len = 800
                        if len(decoded_sample) > max_display_len:
                            # æ˜¾ç¤ºå¼€å¤´å’Œç»“å°¾å„ä¸€åŠ
                            half_len = max_display_len // 2
                            preview = decoded_sample[:half_len] + f"\n...[ä¸­é—´çœç•¥{len(decoded_sample) - max_display_len}å­—ç¬¦]...\n" + decoded_sample[-half_len:]
                        else:
                            preview = decoded_sample
                        print(f"   å®Œæ•´è®­ç»ƒæ ·æœ¬ ({len(decoded_sample)}å­—ç¬¦):")
                        print(f"   {preview}")
                    else:
                        print(f"   âš ï¸ æ— æ³•è·å–sequence_tokensæˆ–labelsï¼Œè·³è¿‡å®Œæ•´æ ·æœ¬æ˜¾ç¤º")
                except Exception as e:
                    print(f"âš ï¸ æ‰“å°è®­ç»ƒæ ·æœ¬å¤±è´¥: {e}")
            
            # è®­ç»ƒä¸€ä¸ªepoch - æ•°æ®é›†ä¼šåœ¨train_epochå†…åˆ·æ–°
            epoch_results = self.train_epoch(train_loader, dataset, optimizer, epoch_idx=epoch)
            
            # è®°å½•å†å²
            training_history['total_loss'].append(epoch_results['total_loss'])
            
            # æ³¨æ„ï¼šå¦‚æœä½¿ç”¨æ··åˆæ•°æ®é›†ï¼ŒSFTæ•°æ®å·²ç»åŒ…å«åœ¨è®­ç»ƒä¸­ï¼Œä¸éœ€è¦epoch_end_hook
            # å¦‚æœä½¿ç”¨ä¼ ç»Ÿæ•°æ®é›†ï¼Œä»ç„¶å¯ä»¥è°ƒç”¨epoch_end_hookï¼ˆä½†é€šå¸¸ä¸éœ€è¦ï¼‰
            # è¿™é‡Œä¿ç•™hookè°ƒç”¨ä»¥ä¿æŒå‘åå…¼å®¹ï¼Œä½†æ··åˆè®­ç»ƒæ¨¡å¼ä¸‹ä¸ä¼šä½¿ç”¨
            if not isinstance(dataset, MixedMemorySFTDataset):
                try:
                    if callable(self.epoch_end_hook):
                        self.epoch_end_hook(epoch, self)
                except Exception as hook_err:
                    if self.is_main_process():
                        print(f"âš ï¸ epoch_end_hook æ‰§è¡Œå¤±è´¥ä½†å·²å¿½ç•¥: {hook_err}")
            
            # # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
            # if (epoch + 1) % 5 == 0:
            #     print(f"ğŸ”„ å·²å®Œæˆ {epoch+1}/{num_epochs} epochs")
                
            #     # ä¿å­˜æ£€æŸ¥ç‚¹
            #     checkpoint_path = f"{save_path}_checkpoint_{epoch+1}"
            #     os.makedirs(checkpoint_path, exist_ok=True)
            #     self.base_model.save_pretrained(checkpoint_path)
            #     self.tokenizer.save_pretrained(checkpoint_path)
            #     print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")
        
        # åˆå¹¶å¹¶ä¿å­˜æ¨¡å‹
        if self.is_main_process():
            print(f"\nğŸ“¦ è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹...")
            final_model = self.merge_and_save_model(save_path)
        else:
            final_model = None
        
        # åˆ†æembeddingå˜åŒ–
        embedding_analysis = self.compare_embeddings() if self.is_main_process() else {}
        
        # æµ‹è¯•è®°å¿†å›å¿†èƒ½åŠ›
        if self.is_main_process():
            print(f"\nğŸ§  å¼€å§‹æµ‹è¯•è®°å¿†å›å¿†èƒ½åŠ›...")
        test_results = self.test_memory_recall(
            texts,
            embeddings,
            num_samples=self.test_sample_count,
            max_new_tokens=self.test_max_new_tokens,
            sft_full_texts=sft_full_texts
        )
        
        # ä¿å­˜ç»“æœ
        results = {
            'training_history': training_history,
            'embedding_analysis': embedding_analysis,
            'memory_test_results': test_results,
            'total_epochs': num_epochs,
            'final_loss': epoch_results['total_loss'],
            'training_config': {
                'num_epochs': num_epochs,
                'batch_size': batch_size,
                'learning_rate': learning_rate,
                'noise_std': noise_std
            }
        }
        
        if self.is_main_process():
            with open(f"{save_path}/training_results.json", 'w') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
        
        if self.is_main_process():
            print(f"\nğŸ‰ å¢å¼ºæ–‡æœ¬è®°å¿†è®­ç»ƒå®Œæˆï¼")
            print(f"   æ€»è®­ç»ƒè½®æ•°: {num_epochs}")
            print(f"   æœ€ç»ˆæ€»ä½“æŸå¤±: {epoch_results['total_loss']:.6f}")
            print(f"   è®°å¿†æ–‡æœ¬æ€»æ•°: {len(texts)}")
            print(f"   æ¨¡å‹ä¿å­˜è·¯å¾„: {save_path}")
            print(f"   æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {save_path}/training_results.json")
        
        return results

    def expose_training_handles(self):
        """æš´éœ²è®­ç»ƒå¥æŸ„ï¼Œä¾›å¤–éƒ¨SFTå¤ç”¨LoRAæ¨¡å‹"""
        return {
            "model": self.base_model,
            "tokenizer": self.tokenizer,
            "accelerator": getattr(self, "accelerator", None)
        }

def main():
    """ä¸»å‡½æ•° - å¢å¼ºè®­ç»ƒ"""
    
    # é…ç½®å‚æ•°
    MODEL_NAME = "./Qwen2.5-7B-Instruct-with-special-tokens-embedding-trained"
    PT_FILE_PATH = "datasets/embeddings/text_embeddings.pt"
    
    # è®­ç»ƒå‚æ•°
    NUM_EPOCHS = 20      # æ€»è®­ç»ƒè½®æ•°
    BATCH_SIZE = 4
    LEARNING_RATE = 1e-4
    NOISE_STD = 0.0
    SAVE_PATH = "Qwen2.5-7B-Instruct-with-special-tokens-memory-trained"
    
    # æŒ‡å®šè®¾å¤‡
    DEVICE = "cuda:0"
    
    print("ğŸš€ å¢å¼ºæ–‡æœ¬è®°å¿†è®­ç»ƒç¨‹åº")
    print("=" * 70)
    print(f"æ¨¡å‹: {MODEL_NAME}")
    print(f"æ•°æ®: {PT_FILE_PATH}")
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"è®­ç»ƒæ–¹å¼: åŒä¸Šä¸‹æ–‡è®°å¿†è®­ç»ƒ ({NUM_EPOCHS}è½®)")
    print("=" * 70)
    
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(PT_FILE_PATH):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {PT_FILE_PATH}")
        return
    
    if not os.path.exists(MODEL_NAME):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_NAME}")
        return
    
    try:
        # åˆå§‹åŒ–è®­ç»ƒå™¨ï¼Œä¼ é€’è®¾å¤‡å‚æ•°
        trainer = EnhancedTextMemoryTrainer(model_name=MODEL_NAME, device=DEVICE)
        
        # å¼€å§‹è®­ç»ƒ
        results = trainer.train(
            pt_file_path=PT_FILE_PATH,
            num_epochs=NUM_EPOCHS,
            batch_size=BATCH_SIZE,
            learning_rate=LEARNING_RATE,
            noise_std=NOISE_STD,
            save_path=SAVE_PATH
        )
        
        if trainer.is_main_process():
            print("\nâœ… è®­ç»ƒæµç¨‹å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'LOCAL_RANK' in os.environ and dist.is_initialized():
            dist.destroy_process_group()

if __name__ == "__main__":
    main()
