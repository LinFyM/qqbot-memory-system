import torch
from modelscope import AutoModelForCausalLM, AutoTokenizer

class SpecialTokensManager:
    """ç‰¹æ®Štokenç®¡ç†å™¨ - æ”¯æŒå¤šGPU"""
    
    def __init__(self, model_path, device=None):
        self.model_path = model_path
        self.specified_device = device
        
        # è®¾å¤‡å¤„ç†é€»è¾‘ - ä¸å…¶ä»–è®­ç»ƒå™¨ä¿æŒä¸€è‡´
        if device is None:
            self.use_auto_device = False
            self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            self.multi_gpu_list = None
        elif isinstance(device, list):
            if len(device) > 0:
                self.use_auto_device = False
                self.primary_device = torch.device(device[0])
                self.multi_gpu_list = device
                print(f"   ä½¿ç”¨å¤šGPUåˆ—è¡¨: {device}ï¼Œä¸»è®¾å¤‡: {device[0]}")
            else:
                self.use_auto_device = True
                self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.multi_gpu_list = None
        elif isinstance(device, str):
            if device == "auto":
                self.use_auto_device = True
                self.primary_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                self.multi_gpu_list = None
            else:
                self.use_auto_device = False
                self.primary_device = torch.device(device)
                self.multi_gpu_list = None
        else:
            self.use_auto_device = False
            self.primary_device = device
            self.multi_gpu_list = None
            
        self.model = None
        self.tokenizer = None
        
        # è¦æ·»åŠ çš„ç‰¹æ®Štoken
        self.special_tokens = ["<recall>", "<|recall|>", "</recall>"]  # ä¿ç•™<|recall|>ä»¥é˜²ä»åœ¨ä½¿ç”¨
        # å‚è€ƒtokenæ˜ å°„ï¼ˆç”¨äºåˆå§‹åŒ–æƒé‡ï¼‰
        # <recall>: ä½¿ç”¨"æ€»ç»“"å’Œ"å›å¿†"çš„åµŒå…¥å‘é‡ä¹‹å’Œ
        # </recall>: ä½¿ç”¨"å›å¿†"å’Œ"ç»“æŸ"çš„åµŒå…¥å‘é‡ä¹‹å’Œ
        self.reference_tokens = {
            "<recall>": ["æ€»ç»“", "å›å¿†"],
            "</recall>": ["å›å¿†", "ç»“æŸ"]
        }
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ - æ”¯æŒå¤šGPUé…ç½®"""
        print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {self.model_path}")
        print(f"ğŸ¯ æŒ‡å®šè®¾å¤‡: {self.specified_device}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path, 
            trust_remote_code=True
        )
        
        try:
            # æ ¹æ®è®¾å¤‡é…ç½®é€‰æ‹©device_map
            if self.use_auto_device:
                device_map = "auto"
                print("   ä½¿ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…")
            elif hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                # å¤šGPUé…ç½®
                device_map = "auto"
                print(f"   ä½¿ç”¨å¤šGPUè‡ªåŠ¨åˆ†é…: {self.multi_gpu_list}")
                
                # è®¾ç½®ç¯å¢ƒå˜é‡é™åˆ¶å¯è§GPU
                import os
                if 'CUDA_VISIBLE_DEVICES' not in os.environ:
                    gpu_indices = [gpu.split(':')[1] for gpu in self.multi_gpu_list if gpu.startswith('cuda:')]
                    if gpu_indices:
                        os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(gpu_indices)
                        print(f"   è®¾ç½®CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")
            elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                # å•GPUæŒ‡å®š
                device_index = int(self.specified_device.split(':')[1])
                device_map = {"": device_index}
                print(f"   ä½¿ç”¨æŒ‡å®šå•GPU: {self.specified_device}")
            elif self.specified_device == "cpu":
                device_map = {"": "cpu"}
                print(f"   ä½¿ç”¨CPUè®¾å¤‡")
            else:
                # é»˜è®¤æƒ…å†µ
                if hasattr(self, 'primary_device') and self.primary_device.type == 'cuda':
                    device_map = {"": self.primary_device.index}
                else:
                    device_map = "auto"
                print(f"   ä½¿ç”¨é»˜è®¤è®¾å¤‡æ˜ å°„: {device_map}")
            
            print(f"   å®é™…ä½¿ç”¨è®¾å¤‡æ˜ å°„: {device_map}")
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype="auto",
                device_map=device_map,
                trust_remote_code=True
            )
            
            # è·å–å®é™…è®¾å¤‡ä¿¡æ¯
            first_param = next(self.model.parameters())
            model_dtype = first_param.dtype
            model_device = first_param.device
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"   å®é™…è®¾å¤‡: {model_device}")
            print(f"   æ•°æ®ç±»å‹: {model_dtype}")
            
            # æ˜¾ç¤ºè®¾å¤‡æ˜ å°„ä¿¡æ¯
            if hasattr(self.model, 'hf_device_map'):
                print(f"   è®¾å¤‡æ˜ å°„è¯¦æƒ…: {self.model.hf_device_map}")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # å›é€€ç­–ç•¥
            print("ğŸ”„ å°è¯•å›é€€åˆ°å•GPUæ¨¡å¼...")
            
            try:
                # ç¡®å®šå›é€€è®¾å¤‡
                if hasattr(self, 'multi_gpu_list') and self.multi_gpu_list:
                    fallback_device = self.multi_gpu_list[0]
                elif isinstance(self.specified_device, str) and self.specified_device.startswith('cuda:'):
                    fallback_device = self.specified_device
                else:
                    fallback_device = 'cuda:0'
                
                # æå–è®¾å¤‡ç´¢å¼•
                if fallback_device.startswith('cuda:'):
                    device_index = int(fallback_device.split(':')[1])
                    device_map = {"": device_index}
                else:
                    device_map = {"": "cpu"}
                
                print(f"   å›é€€è®¾å¤‡æ˜ å°„: {device_map}")
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path,
                    torch_dtype="auto",
                    device_map=device_map,
                    trust_remote_code=True
                )
                
                first_param = next(self.model.parameters())
                print(f"âœ… ä½¿ç”¨å›é€€è®¾å¤‡åŠ è½½æˆåŠŸ: {first_param.device}")
                
            except Exception as fallback_error:
                print(f"âŒ å›é€€åŠ è½½ä¹Ÿå¤±è´¥: {fallback_error}")
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å®Œå…¨å¤±è´¥: åŸé”™è¯¯={e}, å›é€€é”™è¯¯={fallback_error}")
        
    def add_special_tokens(self, perturbation_std=0.02):
        """æ·»åŠ ç‰¹æ®Štokenå¹¶åˆå§‹åŒ–æƒé‡"""
        if self.model is None or self.tokenizer is None:
            self.load_model()
            
        # æ£€æŸ¥åŸå§‹è¯è¡¨å¤§å°
        original_vocab_size = len(self.tokenizer)
        print(f"ğŸ“Š åŸå§‹è¯è¡¨å¤§å°: {original_vocab_size}")
        
        # æ·»åŠ æ–°token
        print("â• æ·»åŠ ç‰¹æ®Štoken...")
        for token in self.special_tokens:
            self.tokenizer.add_tokens(token)
            
        new_vocab_size = len(self.tokenizer)
        print(f"ğŸ“Š æ–°è¯è¡¨å¤§å°: {new_vocab_size} (+{new_vocab_size - original_vocab_size})")
        
        # è°ƒæ•´æ¨¡å‹embeddingå±‚
        print("ğŸ”§ è°ƒæ•´æ¨¡å‹embeddingå±‚...")
        self.model.resize_token_embeddings(len(self.tokenizer))
        
        # è·å–æ–°tokençš„ID
        token_ids = {}
        for token in self.special_tokens:
            token_ids[token] = self.tokenizer.convert_tokens_to_ids(token)
            
        print(f"ğŸ†” æ–°token ID: {token_ids}")
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_token_weights(token_ids, perturbation_std)
        
        return token_ids
        
    def _initialize_token_weights(self, token_ids, perturbation_std):
        """ä½¿ç”¨ä¸­æ–‡å‚è€ƒtokenåˆå§‹åŒ–æƒé‡ï¼ˆä½¿ç”¨å‚è€ƒtokenåµŒå…¥å‘é‡ä¹‹å’Œï¼‰"""
        print(f"ğŸ¯ åˆå§‹åŒ–tokenæƒé‡ï¼ˆä½¿ç”¨å‚è€ƒtokenåµŒå…¥å‘é‡ä¹‹å’Œï¼‰...")
            
        # è·å–embeddingå±‚å’Œlm_head
        embedding_layer = self.model.get_input_embeddings()
        lm_head = self.model.lm_head
        
        with torch.no_grad():
            for target_token, ref_words in self.reference_tokens.items():
                if target_token not in token_ids:
                    continue

                target_id = token_ids[target_token]
                
                # è·å–å‚è€ƒtoken ID
                ref_token_ids = []
                for word in ref_words:
                    ref_ids = self.tokenizer.encode(word, add_special_tokens=False)
                    if len(ref_ids) != 1:
                        print(f"   âš ï¸ å‚è€ƒè¯ '{word}' ä¸æ˜¯å•ä¸ªtokenï¼Œè·³è¿‡")
                        continue
                    ref_token_ids.append(ref_ids[0])
                    print(f"   {target_token} ä½¿ç”¨å‚è€ƒè¯ '{word}' (ID: {ref_ids[0]})")

                if len(ref_token_ids) == 0:
                    print(f"   âš ï¸ {target_token} æ²¡æœ‰æœ‰æ•ˆçš„å‚è€ƒtokenï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                    continue

                # è®¡ç®—å‚è€ƒtokenåµŒå…¥å‘é‡çš„å’Œï¼Œç„¶åå½’ä¸€åŒ–
                base_embedding = None
                base_lm_weight = None
                ref_embeddings_list = []
                ref_lm_weights_list = []

                for ref_id in ref_token_ids:
                    ref_emb = embedding_layer.weight.data[ref_id].clone()
                    ref_lm = lm_head.weight.data[ref_id].clone()
                    ref_embeddings_list.append(ref_emb)
                    ref_lm_weights_list.append(ref_lm)

                    if base_embedding is None:
                        base_embedding = ref_emb
                        base_lm_weight = ref_lm
                    else:
                        base_embedding = base_embedding + ref_emb
                        base_lm_weight = base_lm_weight + ref_lm

                # ç›´æ¥å½’ä¸€åŒ–ï¼šç¼©æ”¾åˆ°ç¬¬ä¸€ä¸ªå‚è€ƒtokençš„èŒƒæ•°ï¼Œç„¶åæ·»åŠ å°çš„æ­£äº¤æ‰°åŠ¨ä»¥åŒºåˆ†
                if len(ref_embeddings_list) > 1:
                    target_emb_norm = ref_embeddings_list[0].norm()
                    target_lm_norm = ref_lm_weights_list[0].norm()
                    current_emb_norm = base_embedding.norm()
                    current_lm_norm = base_lm_weight.norm()
                    if current_emb_norm > 0:
                        base_embedding = base_embedding / current_emb_norm * target_emb_norm
                    if current_lm_norm > 0:
                        base_lm_weight = base_lm_weight / current_lm_norm * target_lm_norm
                    
                    # æ·»åŠ å°çš„æ­£äº¤æ‰°åŠ¨ï¼Œé¿å…ä¸å‚è€ƒtokenè¿‡äºç›¸ä¼¼
                    import torch
                    ref1_emb_normalized = ref_embeddings_list[0] / ref_embeddings_list[0].norm()
                    base_emb_normalized = base_embedding / base_embedding.norm()
                    proj_emb = torch.dot(base_emb_normalized, ref1_emb_normalized) * ref1_emb_normalized
                    orthogonal_emb = base_emb_normalized - proj_emb
                    if orthogonal_emb.norm() > 1e-6:
                        orthogonal_emb = orthogonal_emb / orthogonal_emb.norm()
                        perturbation_scale = 0.1
                        base_embedding = base_embedding + orthogonal_emb * perturbation_scale * target_emb_norm
                        base_embedding = base_embedding / base_embedding.norm() * target_emb_norm
                    
                    ref1_lm_normalized = ref_lm_weights_list[0] / ref_lm_weights_list[0].norm()
                    base_lm_normalized = base_lm_weight / base_lm_weight.norm()
                    proj_lm = torch.dot(base_lm_normalized, ref1_lm_normalized) * ref1_lm_normalized
                    orthogonal_lm = base_lm_normalized - proj_lm
                    if orthogonal_lm.norm() > 1e-6:
                        orthogonal_lm = orthogonal_lm / orthogonal_lm.norm()
                        perturbation_scale = 0.1
                        base_lm_weight = base_lm_weight + orthogonal_lm * perturbation_scale * target_lm_norm
                        base_lm_weight = base_lm_weight / base_lm_weight.norm() * target_lm_norm

                # ä½¿ç”¨å½’ä¸€åŒ–åçš„embeddingä½œä¸ºåˆå§‹åŒ–
                embedding_layer.weight.data[target_id] = base_embedding
                lm_head.weight.data[target_id] = base_lm_weight

                ref_str = " + ".join(ref_words)
                print(f"   âœ… {target_token} (ID: {target_id}) åˆå§‹åŒ–å®Œæˆï¼ˆå‚è€ƒ: {ref_str}ï¼‰")
                
    def save_model(self, save_path):
        """ä¿å­˜æ·»åŠ äº†ç‰¹æ®Štokençš„æ¨¡å‹"""
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {save_path}")
        
        self.tokenizer.save_pretrained(save_path)
        self.model.save_pretrained(save_path)
        
        print("âœ… ä¿å­˜å®Œæˆ")
        return save_path
        
    def process(self, save_path, perturbation_std=0.02):
        """å®Œæ•´å¤„ç†æµç¨‹"""
        print("ğŸš€ å¼€å§‹æ·»åŠ ç‰¹æ®Štoken...")
        
        self.load_model()
        token_ids = self.add_special_tokens(perturbation_std)
        model_path = self.save_model(save_path)
        
        print("ğŸ‰ ç‰¹æ®Štokenæ·»åŠ å®Œæˆ!")
        return model_path, token_ids