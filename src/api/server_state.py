# -*- coding: utf-8 -*-
"""
æœåŠ¡å™¨å…¨å±€çŠ¶æ€ç®¡ç†
åŒ…å«æ¨¡å‹ã€é…ç½®ã€è®°å¿†åº“ç­‰å…¨å±€å¯¹è±¡
"""
import logging
import os
import sys
import yaml
import torch
import threading
from pathlib import Path
from typing import Optional, Dict, Any, List
from transformers import Qwen3VLForConditionalGeneration, AutoProcessor

# å¯¼å…¥è®°å¿†ç›¸å…³æ¨¡å—
from memory.vector_db import MemoryVectorDB
from memory.token_manager import MemoryTokenManager
from utils.common import get_project_root, resolve_path

_log = logging.getLogger(__name__)

# =============================================================================
# å…¨å±€çŠ¶æ€å˜é‡
# =============================================================================
model: Optional[Qwen3VLForConditionalGeneration] = None
processor: Optional[AutoProcessor] = None
device: Optional[str] = None
config: Dict[str, Any] = {}
memory_db: Optional[MemoryVectorDB] = None
recall_token_ids: Dict[str, int] = {}  # ç‰¹æ®Štoken IDæ˜ å°„
token_manager: Optional[MemoryTokenManager] = None

# è®­ç»ƒç›¸å…³å…¨å±€çŠ¶æ€
is_training: bool = False
training_lock = threading.Lock()
model_lock = threading.Lock()  # æ¨¡å‹æ¨ç†é”ï¼Œç¡®ä¿ä¸²è¡Œ
training_scheduler = None
# è®°å½•æœåŠ¡å™¨å…¥å£è„šæœ¬åŠå‚æ•°ï¼Œä¾¿äºè®­ç»ƒåé‡å¯
server_script_path: Optional[str] = None
server_script_args: Optional[List[str]] = None

# æœåŠ¡å™¨åŸºç¡€URL
server_base_url: str = "http://127.0.0.1:9999"

# ä¸Šä¼ ç›®å½•é…ç½®
IMAGE_UPLOAD_DIR: str = ""
VIDEO_UPLOAD_DIR: str = ""
AUDIO_UPLOAD_DIR: str = ""
FILE_UPLOAD_DIR: str = ""


def load_config(config_path: Optional[str] = None) -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ configs/config_qwen3vl.yaml
    
    Returns:
        é…ç½®å­—å…¸
    """
    global config
    
    if config_path is None:
        project_root = get_project_root()
        config_path = project_root / "configs" / "config_qwen3vl.yaml"
    else:
        config_path = Path(config_path)
    
    if not config_path.exists():
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åŠ è½½ç»Ÿä¸€çš„æç¤ºè¯é…ç½®
    prompt_library = {}
    prompt_config_path = config.get("prompt_config_path")
    if not prompt_config_path:
        prompt_config_path = "configs/prompts.yaml"
    try:
        prompt_path = resolve_path(prompt_config_path)
        if prompt_path.exists():
            with open(prompt_path, "r", encoding="utf-8") as pf:
                prompt_library = yaml.safe_load(pf) or {}
            _log.info(f"âœ… å·²åŠ è½½æç¤ºè¯é…ç½®æ–‡ä»¶: {prompt_path}")
        else:
            _log.warning(f"âš ï¸ æç¤ºè¯é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}")
    except Exception as e:
        _log.warning(f"âš ï¸ åŠ è½½æç¤ºè¯é…ç½®å¤±è´¥: {e}")
        prompt_library = {}
    
    config["prompt_library"] = prompt_library
    
    # å‘åå…¼å®¹ï¼šä¿æŒ config['prompt'] å¯ç”¨
    if prompt_library.get("chat"):
        config["prompt"] = prompt_library["chat"]
    else:
        config.setdefault("prompt", {})
    
    memory_cfg = config.setdefault("memory", {}).setdefault("training", {})
    memory_training_prompts = prompt_library.get("memory_training", {})
    if "guides" not in memory_cfg and memory_training_prompts.get("guides"):
        memory_cfg["guides"] = memory_training_prompts["guides"]
    if "guide_text" not in memory_cfg and memory_training_prompts.get("guide_text"):
        memory_cfg["guide_text"] = memory_training_prompts["guide_text"]
    
    config["memory_extraction_prompts"] = prompt_library.get("memory_extraction", {})
    config["memory_vectorization_prompts"] = prompt_library.get("memory_vectorization", {})
    
    _log.info(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    return config


def initialize_model(model_path: str, target_device: str):
    """
    åˆå§‹åŒ–æ¨¡å‹å’Œå¤„ç†å™¨
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        target_device: ç›®æ ‡è®¾å¤‡
    """
    global model, processor, device, memory_db, recall_token_ids, token_manager
    
    # åˆ¤æ–­æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å‹
    is_trained_model = "trained" in model_path or "token_added" in model_path
    model_type = "è®­ç»ƒæ¨¡å‹" if is_trained_model else "åŸºç¡€æ¨¡å‹"
    
    _log.info("=" * 60)
    _log.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–Qwen3-VLæ¨¡å‹...")
    _log.info(f"ğŸ“¦ æ¨¡å‹ç±»å‹: {model_type}")
    _log.info(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    _log.info(f"ğŸ–¥ï¸  è®¾å¤‡: {target_device}")
    if is_trained_model:
        model_name = os.path.basename(model_path)
        _log.info(f"ğŸ“… æ¨¡å‹æ—¶é—´æˆ³: {model_name}")
    _log.info("=" * 60)
    
    # è§£ææ¨¡å‹è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„ï¼‰
    model_path_resolved = resolve_path(model_path)
    
    if not model_path_resolved.exists():
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path_resolved}")
    
    # æ£€æŸ¥CUDAä¿¡æ¯
    _log.info("æ£€æŸ¥CUDAç¯å¢ƒ...")
    cuda_available = torch.cuda.is_available()
    _log.info(f"ğŸ”§ CUDAå¯ç”¨: {cuda_available}")
    if cuda_available:
        cuda_device_count = torch.cuda.device_count()
        _log.info(f"ğŸ”§ CUDAè®¾å¤‡æ•°é‡: {cuda_device_count}")
        cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
        if cuda_visible:
            _log.info(f"ğŸ”§ CUDA_VISIBLE_DEVICES: {cuda_visible}")
    
    # åŠ è½½å¤„ç†å™¨
    _log.info("åŠ è½½AutoProcessor...")
    processor = AutoProcessor.from_pretrained(
        str(model_path_resolved),
        trust_remote_code=True,
        local_files_only=True
    )
    _log.info("âœ… ProcessoråŠ è½½æˆåŠŸ")
    
    # ç¡®ä¿chat_templateè¢«æ­£ç¡®åŠ è½½
    if processor.chat_template is None:
        import json
        chat_template_path = model_path_resolved / "chat_template.json"
        if chat_template_path.exists():
            try:
                with open(chat_template_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    processor.chat_template = data["chat_template"]
                _log.info("âœ… æ‰‹åŠ¨åŠ è½½chat_templateæˆåŠŸ")
            except Exception as e:
                _log.warning(f"âš ï¸ æ‰‹åŠ¨åŠ è½½chat_templateå¤±è´¥: {e}")
    
    # é…ç½®åŠ è½½å‚æ•°
    _log.info("åŠ è½½Qwen3VLForConditionalGeneration...")
    load_kwargs = {
        "torch_dtype": "auto",
        "trust_remote_code": True,
        "local_files_only": True
    }
    
    # é…ç½®è®¾å¤‡æ˜ å°„
    cuda_visible = os.environ.get("CUDA_VISIBLE_DEVICES")
    cuda_visible_set = bool(cuda_visible)
    
    if isinstance(target_device, list):
        # å¤šGPUé…ç½®
        if cuda_visible:
            _log.info(f"ğŸ”§ æ£€æµ‹åˆ°CUDA_VISIBLE_DEVICES={cuda_visible}")
        _log.info(f"ğŸ”§ å¤šGPUæ¨¡å¼: æŒ‡å®šè®¾å¤‡{target_device}")
        load_kwargs["device_map"] = "auto"
    elif target_device.startswith("cuda"):
        # å•GPUé…ç½®
        if cuda_visible_set and cuda_visible:
            device_map_device = "cuda:0"
            _log.info(f"ğŸ”§ å•GPUæ¨¡å¼: CUDA_VISIBLE_DEVICES={cuda_visible}ï¼Œä½¿ç”¨é‡æ–°æ˜ å°„è®¾å¤‡ {device_map_device}ï¼ˆå¯¹åº”ç‰©ç†GPU {target_device}ï¼‰")
        else:
            device_map_device = target_device
        _log.info(f"ğŸ”§ å•GPUæ¨¡å¼: è®¾å¤‡æ˜ å°„åˆ° {target_device}")
        load_kwargs["device_map"] = {"": device_map_device}
    else:
        # CPUé…ç½®
        load_kwargs["device_map"] = "cpu"
        _log.info("ğŸ”§ CPUæ¨¡å¼: åŠ è½½åˆ°CPU")
    
    # åŠ è½½æ¨¡å‹
    model = Qwen3VLForConditionalGeneration.from_pretrained(
        str(model_path_resolved),
        **load_kwargs
    )
    
    # è·å–å®é™…è®¾å¤‡
    actual_device = next(model.parameters()).device
    device = target_device
    _log.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå®é™…è®¾å¤‡: {actual_device}")
    model.eval()
    
    # æ£€æŸ¥å¹¶æ·»åŠ ç‰¹æ®Štoken
    _log.info("æ£€æŸ¥å¹¶æ·»åŠ è®°å¿†ç›¸å…³ç‰¹æ®Štoken...")
    token_manager = MemoryTokenManager(model, processor)
    recall_token_ids = token_manager.check_and_add_tokens(perturbation_std=0.02)
    _log.info(f"âœ… ç‰¹æ®Štokenå¤„ç†å®Œæˆ: {recall_token_ids}")
    
    # åˆå§‹åŒ–è®°å¿†å‘é‡åº“
    memory_config = config.get("memory", {})
    memory_enabled = memory_config.get("enabled", False)
    
    if memory_enabled:
        _log.info("åˆå§‹åŒ–MemoryVectorDB...")
        memory_db_config = memory_config.get("memory_db", {})
        max_size = memory_db_config.get("max_size", 100000)
        enable_eviction = memory_db_config.get("enable_eviction", True)
        
        # è·å– embedding ç»´åº¦
        try:
            input_embeddings = model.get_input_embeddings()
            embedding_dim = input_embeddings.weight.shape[1]
            _log.info(f"ğŸ“Š ä»æ¨¡å‹ input_embeddings è·å–ç»´åº¦: {embedding_dim}")
        except Exception as e:
            embedding_dim = 4096
            _log.warning(f"âš ï¸ æ— æ³•ä»æ¨¡å‹è·å– embedding ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼: {embedding_dim}")
        
        memory_device = actual_device
        if hasattr(memory_device, "type"):
            memory_device = str(memory_device)
        memory_db = MemoryVectorDB(
            embedding_dim=embedding_dim,
            device=memory_device,
            max_size=max_size,
            enable_eviction=enable_eviction
        )
        
        # å°è¯•åŠ è½½å·²æœ‰çš„è®°å¿†æ•°æ®åº“
        memory_db_path = memory_config.get("memory_db_path")
        if memory_db_path:
            memory_db_path_resolved = resolve_path(memory_db_path)
            if memory_db_path_resolved.exists():
                try:
                    memory_db.load_from_pt(str(memory_db_path_resolved))
                    _log.info(f"âœ… å·²åŠ è½½è®°å¿†åº“: {memory_db_path_resolved} (æ¡ç›®æ•°: {len(memory_db)})")
                except Exception as e:
                    _log.warning(f"âš ï¸ åŠ è½½è®°å¿†åº“å¤±è´¥: {e}")
            else:
                _log.info(f"â„¹ï¸ è®°å¿†åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„ç©ºåº“: {memory_db_path_resolved}")
    else:
        _log.info("è®°å¿†åŠŸèƒ½æœªå¯ç”¨")
    
    _log.info("=" * 60)
    _log.info("âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    _log.info("=" * 60)


def get_model_and_processor():
    """
    è·å–å½“å‰çš„æ¨¡å‹å’Œå¤„ç†å™¨
    
    Returns:
        (model, processor) å…ƒç»„
    """
    if model is None or processor is None:
        raise RuntimeError("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize_model()")
    return model, processor


def setup_upload_directories(base_dir: Optional[Path] = None):
    """
    è®¾ç½®ä¸Šä¼ ç›®å½•
    
    Args:
        base_dir: åŸºç¡€ç›®å½•ï¼Œé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•
    """
    global IMAGE_UPLOAD_DIR, VIDEO_UPLOAD_DIR, AUDIO_UPLOAD_DIR, FILE_UPLOAD_DIR
    
    if base_dir is None:
        base_dir = get_project_root()
    
    IMAGE_UPLOAD_DIR = str(base_dir / "uploads" / "images")
    VIDEO_UPLOAD_DIR = str(base_dir / "uploads" / "videos")
    AUDIO_UPLOAD_DIR = str(base_dir / "uploads" / "audios")
    FILE_UPLOAD_DIR = str(base_dir / "uploads" / "files")
    
    # åˆ›å»ºç›®å½•
    for dir_path in [IMAGE_UPLOAD_DIR, VIDEO_UPLOAD_DIR, AUDIO_UPLOAD_DIR, FILE_UPLOAD_DIR]:
        os.makedirs(dir_path, exist_ok=True)
    
    _log.info(f"âœ… ä¸Šä¼ ç›®å½•å·²è®¾ç½®: {base_dir / 'uploads'}")


def find_latest_model(config_override: Optional[dict] = None) -> str:
    """
    æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹è·¯å¾„ï¼ˆä¼˜å…ˆè®­ç»ƒæ¨¡å‹ -> tokenæ·»åŠ æ¨¡å‹ -> åŸºç¡€æ¨¡å‹ï¼‰
    """
    cfg = config_override or config
    if not cfg:
        cfg = load_config()
    
    memory_cfg = cfg.get("memory", {}).get("training", {})
    trained_model_dir = resolve_path(memory_cfg.get("trained_model_dir", "./models/trained"))
    token_added_model_dir = resolve_path(memory_cfg.get("token_added_model_dir", "./models/token_added"))
    
    _log.info("=" * 60)
    _log.info("ğŸ” æŸ¥æ‰¾æœ€æ–°æ¨¡å‹è·¯å¾„")
    _log.info(f"ğŸ“ è®­ç»ƒæ¨¡å‹ç›®å½•: {trained_model_dir}")
    _log.info(f"ğŸ“ Tokenæ¨¡å‹ç›®å½•: {token_added_model_dir}")
    _log.info("=" * 60)
    
    if trained_model_dir.exists():
        model_dirs = [
            d for d in os.listdir(trained_model_dir)
            if (trained_model_dir / d).is_dir() and d.startswith("model_")
        ]
        if model_dirs:
            model_dirs.sort(reverse=True)
            latest = trained_model_dir / model_dirs[0]
            _log.info(f"âœ… ä½¿ç”¨æœ€æ–°è®­ç»ƒæ¨¡å‹: {latest}")
            return str(latest)
        _log.warning("âš ï¸ è®­ç»ƒæ¨¡å‹ç›®å½•å­˜åœ¨ä½†ä¸ºç©º")
    else:
        _log.warning(f"âš ï¸ è®­ç»ƒæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {trained_model_dir}")
    
    if token_added_model_dir.exists():
        model_dirs = [
            d for d in os.listdir(token_added_model_dir)
            if (token_added_model_dir / d).is_dir() and d.startswith("model_")
        ]
        if model_dirs:
            model_dirs.sort(reverse=True)
            latest = token_added_model_dir / model_dirs[0]
            _log.info(f"âœ… ä½¿ç”¨æœ€æ–°tokenæ·»åŠ æ¨¡å‹: {latest}")
            return str(latest)
        _log.warning("âš ï¸ Tokenæ¨¡å‹ç›®å½•å­˜åœ¨ä½†ä¸ºç©º")
    else:
        _log.warning(f"âš ï¸ Tokenæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {token_added_model_dir}")
    
    model_path = cfg.get("model", {}).get("path")
    if model_path:
        _log.info(f"â„¹ï¸ ä½¿ç”¨é…ç½®ä¸­çš„åŸºç¡€æ¨¡å‹è·¯å¾„: {model_path}")
        return model_path
    
    default_path = memory_cfg.get("base_model_path", "./models/Qwen3-VL-4B-Thinking")
    _log.info(f"â„¹ï¸ ä½¿ç”¨é»˜è®¤åŸºç¡€æ¨¡å‹è·¯å¾„: {default_path}")
    return default_path


def reload_latest_model(config_override: Optional[dict] = None, device_override: Optional[str] = None) -> str:
    """
    é‡æ–°åŠ è½½æœ€æ–°æ¨¡å‹ï¼Œå¹¶è¿”å›å®é™…ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„
    """
    cfg = config_override or config
    if not cfg:
        cfg = load_config()
    
    target_device = device_override or cfg.get("model", {}).get("device", "cuda:0")
    model_path = find_latest_model(cfg)
    
    _log.info("=" * 60)
    _log.info("ğŸ”„ é‡æ–°åŠ è½½æœ€æ–°æ¨¡å‹")
    _log.info(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
    _log.info(f"ğŸ–¥ï¸  ç›®æ ‡è®¾å¤‡: {target_device}")
    _log.info("=" * 60)
    
    initialize_model(model_path, target_device)
    _log.info("âœ… æ¨¡å‹é‡æ–°åŠ è½½å®Œæˆ")
    return model_path
