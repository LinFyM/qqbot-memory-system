# -*- coding: utf-8 -*-
"""
å›å¤å¤„ç†æ¨¡å—
åŒ…å« generate_reply ç­‰æ ¸å¿ƒç”Ÿæˆé€»è¾‘
"""
import logging
import threading
import torch
from typing import Dict, List, Any, Tuple, Optional

from chat.generate import custom_generate
from chat.prompting import build_system_prompt, extract_final_reply
from chat.history_manager import save_chat_history_to_storage
from utils.media_utils import is_image_url_valid
from transformers.generation.stopping_criteria import StoppingCriteriaList

_log = logging.getLogger(__name__)


class InterruptStoppingCriteria:
    """ä¸­æ–­åœæ­¢æ¡ä»¶"""
    def __init__(self, interrupt_event):
        self.interrupt_event = interrupt_event
    
    def __call__(self, input_ids, scores, **kwargs):
        if self.interrupt_event and self.interrupt_event.is_set():
            return True
        return False


def truncate_history_by_tokens(
    processor,
    chat_history: List[Dict[str, Any]],
    system_prompt: str,
    chat_type: str,
    chat_id: str,
    config: Dict[str, Any],
    max_tokens: int = 32000,
    interrupt_event: threading.Event = None
) -> List[Dict[str, Any]]:
    """
    æ ¹æ®tokenæ•°é‡æˆªæ–­èŠå¤©å†å²
    
    Args:
        processor: å¤„ç†å™¨
        chat_history: èŠå¤©å†å²
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        chat_type: "group" æˆ– "private"
        chat_id: ç¾¤IDæˆ–ç”¨æˆ·ID
        config: é…ç½®å­—å…¸
        max_tokens: æœ€å¤§tokenæ•°
        interrupt_event: ä¸­æ–­äº‹ä»¶
    
    Returns:
        æˆªæ–­åçš„å†å²
    """
    if chat_history is None:
        return []
    
    if interrupt_event and interrupt_event.is_set():
        return chat_history
    
    if processor is None:
        _log.warning("âš ï¸ å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æˆªæ–­")
        return chat_history
    
    # æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨
    full_messages = []
    if system_prompt:
        full_messages.append({
            "role": "system",
            "content": [{"type": "text", "text": system_prompt}]
        })
    full_messages.extend(chat_history)
    
    try:
        # Tokenizeæ£€æŸ¥é•¿åº¦
        inputs = processor.apply_chat_template(
            full_messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
            truncation=False,
            padding=False
        )
        
        current_length = inputs['input_ids'].shape[1]
        _log.info(f"ğŸ“ å½“å‰è¾“å…¥tokensé•¿åº¦: {current_length}, é™åˆ¶: {max_tokens}ï¼ˆ{chat_type} {chat_id}ï¼‰")
        
        if current_length <= max_tokens:
            _log.info(f"âœ… å†å²é•¿åº¦åœ¨é™åˆ¶å†…ï¼Œæ— éœ€æˆªæ–­ï¼ˆ{chat_type} {chat_id}ï¼‰")
            return chat_history
        
        _log.warning(f"âš ï¸ å†å²é•¿åº¦è¶…å‡ºé™åˆ¶ ({current_length} > {max_tokens})ï¼Œå¼€å§‹æˆªæ–­ï¼ˆ{chat_type} {chat_id}ï¼‰")
        
        removed_messages = []
        # ä»å¤´éƒ¨ç§»é™¤æ¶ˆæ¯ç›´åˆ°æ»¡è¶³é•¿åº¦è¦æ±‚
        while len(chat_history) > 1:
            removed_msg = chat_history.pop(0)  # ç§»é™¤æœ€æ—§çš„æ¶ˆæ¯
            removed_messages.append(removed_msg)
            
            full_messages = []
            if system_prompt:
                full_messages.append({
                    "role": "system",
                    "content": [{"type": "text", "text": system_prompt}]
                })
            full_messages.extend(chat_history)
            
            inputs = processor.apply_chat_template(
                full_messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt",
                truncation=False,
                padding=False
            )
            
            current_length = inputs['input_ids'].shape[1]
            
            if current_length <= max_tokens:
                _log.info(f"âœ… æˆªæ–­å®Œæˆï¼Œå½“å‰é•¿åº¦ï¼š{current_length}")
                break
        
        # ä¿å­˜è¢«ç§»é™¤çš„æ¶ˆæ¯
        if removed_messages:
            _log.info(f"ğŸ’¾ ä¿å­˜è¢«æˆªæ–­çš„ {len(removed_messages)} æ¡æ¶ˆæ¯åˆ°å­˜å‚¨")
            try:
                # å¼‚æ­¥ä¿å­˜ï¼Œé¿å…é˜»å¡
                threading.Thread(
                    target=save_chat_history_to_storage,
                    args=(config, chat_type, chat_id, removed_messages),
                    daemon=True
                ).start()
            except Exception as e:
                _log.error(f"âŒ ä¿å­˜æˆªæ–­æ¶ˆæ¯å¤±è´¥: {e}")
        
        return chat_history
        
    except Exception as e:
        _log.warning(f"âš ï¸ æˆªæ–­å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹å†å²")
        return chat_history


def generate_reply(
    model,
    processor,
    memory_db,
    recall_token_ids,
    config,
    chat_history: List[Dict[str, Any]],
    max_new_tokens: int = None,
    temperature: float = None,
    chat_type: str = None,
    chat_context: Dict[str, str] = None,
    interrupt_event: threading.Event = None,
    chat_id: str = None,
    response_dict: dict = None,
    log_full_io: bool = True,
    is_training: bool = False,
    training_lock: threading.Lock = None,
    model_lock: threading.Lock = None
) -> Tuple[Optional[str], bool, bool]:
    """
    ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›å¤
    
    Args:
        model: æ¨¡å‹å®ä¾‹
        processor: å¤„ç†å™¨å®ä¾‹
        memory_db: è®°å¿†æ•°æ®åº“
        recall_token_ids: ç‰¹æ®Štoken IDs
        config: é…ç½®å­—å…¸
        chat_history: èŠå¤©å†å²
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: æ¸©åº¦å‚æ•°
        chat_type: "group" æˆ– "private"
        chat_context: å¯¹è¯ä¸Šä¸‹æ–‡
        interrupt_event: ä¸­æ–­äº‹ä»¶
        chat_id: èŠå¤©ID
        response_dict: å“åº”å­—å…¸
        log_full_io: æ˜¯å¦è®°å½•å®Œæ•´è¾“å…¥è¾“å‡º
        is_training: æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼
        training_lock: è®­ç»ƒé”
    
    Returns:
        (å›å¤æ–‡æœ¬, æ˜¯å¦éœ€è¦å›å¤, æ˜¯å¦è¢«ä¸­æ–­)
    """
    # æ£€æŸ¥è®­ç»ƒæ¨¡å¼
    if training_lock and is_training:
        _log.warning("âš ï¸ å½“å‰å¤„äºè®­ç»ƒæ¨¡å¼ï¼Œæ‹’ç»ç”Ÿæˆå›å¤")
        raise RuntimeError("æœåŠ¡å™¨æ­£åœ¨è®­ç»ƒä¸­ï¼Œæš‚æ—¶æ— æ³•ç”Ÿæˆå›å¤")
    
    if model is None or processor is None:
        raise RuntimeError("æ¨¡å‹æœªåˆå§‹åŒ–")
    
    # ä»é…ç½®è¯»å–ç”Ÿæˆå‚æ•°
    gen_config = config.get("generation", {})
    if max_new_tokens is None:
        max_new_tokens = gen_config.get("max_new_tokens", 1000)
    if temperature is None:
        temperature = gen_config.get("temperature", 1.0)
    
    do_sample = gen_config.get("do_sample", True)
    top_p = gen_config.get("top_p", 0.95)
    top_k = gen_config.get("top_k", 20)
    
    try:
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = build_system_prompt(config, chat_type, chat_context)
        _log.debug(f"ğŸ“ ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}")
        
        # æ„å»ºå®Œæ•´æ¶ˆæ¯åˆ—è¡¨
        full_messages = []
        if system_prompt:
            full_messages.append({
                "role": "system",
                "content": [{"type": "text", "text": system_prompt}]
            })
        full_messages.extend(chat_history)
        
        _log.debug(f"å‡†å¤‡æ¨ç†è¾“å…¥ï¼Œç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}, å†å²æ¶ˆæ¯æ•°: {len(chat_history)}")
        
        # æ£€æŸ¥ä¸­æ–­
        if interrupt_event and interrupt_event.is_set():
            _log.warning("âš ï¸ åœ¨apply_chat_templateå‰æ£€æµ‹åˆ°ä¸­æ–­")
            return None, False, True
        
        # å‡†å¤‡è¾“å…¥
        _log.debug("å¼€å§‹apply_chat_template...")
        try:
            inputs = processor.apply_chat_template(
                full_messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt",
                truncation=False,
                padding=False
            )
        except Exception as e:
            # å¤„ç†å›¾ç‰‡ç›¸å…³é”™è¯¯ï¼šé€ä¸ªæ£€æŸ¥å›¾ç‰‡æœ‰æ•ˆæ€§
            error_msg = str(e)
            _log.warning(f"âš ï¸ apply_chat_templateå¤±è´¥: {e}ï¼Œå°è¯•ä¿®å¤å›¾ç‰‡é“¾æ¥")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡é”™è¯¯
            image_errors = ["UnidentifiedImageError", "cannot identify image file", "ConnectionError", "Timeout", "Failed to resolve"]
            if any(err in error_msg for err in image_errors):
                # é€ä¸ªæ£€æŸ¥å¹¶ç§»é™¤å¤±æ•ˆå›¾ç‰‡
                cleaned_messages = []
                for msg in full_messages:
                    if isinstance(msg.get("content"), list):
                        cleaned_content = []
                        for item in msg["content"]:
                            if item.get("type") == "image":
                                img_url = item.get("image", "")
                                if img_url.startswith("http") and not is_image_url_valid(img_url):
                                    _log.warning(f"âš ï¸ ç§»é™¤å¤±æ•ˆå›¾ç‰‡: {img_url}")
                                    continue
                                cleaned_content.append(item)
                            else:
                                cleaned_content.append(item)
                        if cleaned_content:
                            msg["content"] = cleaned_content
                            cleaned_messages.append(msg)
                    else:
                        cleaned_messages.append(msg)
                
                # é‡è¯•
                inputs = processor.apply_chat_template(
                    cleaned_messages,
                    tokenize=True,
                    add_generation_prompt=True,
                    return_dict=True,
                    return_tensors="pt",
                    truncation=False,
                    padding=False
                )
            else:
                raise e
        
        # ç§»åˆ°è®¾å¤‡
        device = next(model.parameters()).device
        inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}
        
        _log.info(f"âœ… apply_chat_templateæˆåŠŸï¼Œè¾“å…¥tokensé•¿åº¦: {inputs['input_ids'].shape[1]}")
        
        # æ‰“å°å®Œæ•´çš„è¾“å…¥ï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰
        if log_full_io:
            input_ids_text = processor.tokenizer.batch_decode(
                inputs['input_ids'],
                skip_special_tokens=False,
                clean_up_tokenization_spaces=False
            )
            _log.info("=" * 80)
            _log.info("ğŸ”¤ æ¨¡å‹å®Œæ•´è¾“å…¥ï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰ï¼š")
            _log.info(input_ids_text[0])
            _log.info("=" * 80)
        
        # æ£€æŸ¥ä¸­æ–­
        if interrupt_event and interrupt_event.is_set():
            _log.warning("âš ï¸ åœ¨æ‰“å°è¾“å…¥åæ£€æµ‹åˆ°ä¸­æ–­")
            return None, False, True
        
        # é…ç½®åœæ­¢æ¡ä»¶
        stopping_criteria_list = StoppingCriteriaList()
        if interrupt_event:
            stopping_criteria_list.append(InterruptStoppingCriteria(interrupt_event))
        
        _log.info(f"ğŸ¯ å¼€å§‹è‡ªå›å½’ç”Ÿæˆï¼Œmax_new_tokens={max_new_tokens}, temperature={temperature}, do_sample={do_sample}")
        _log.info("å¼€å§‹ç”Ÿæˆå›å¤...")
        
        # ä½¿ç”¨æ¨¡å‹é”ç¡®ä¿ä¸²è¡Œæ¨ç†ï¼Œå¹¶ä½¿ç”¨torch.no_grad()èŠ‚çœæ˜¾å­˜
        if model_lock is None:
            import api.server_state as server_state
            model_lock = server_state.model_lock
        
        with model_lock:
            # åœ¨è·å–é”åå†æ¬¡æ£€æŸ¥ä¸­æ–­
            if interrupt_event and interrupt_event.is_set():
                _log.warning("âš ï¸ åœ¨è·å–æ¨¡å‹é”åæ£€æµ‹åˆ°ä¸­æ–­")
                return None, False, True
            
            with torch.no_grad():  # å…³é”®ï¼šç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œå¤§å¹…èŠ‚çœæ˜¾å­˜ï¼
                try:
                    output_ids = custom_generate(
                        model=model,
                        processor=processor,
                        memory_db=memory_db,
                        recall_token_ids=recall_token_ids,
                        config=config,
                        inputs=inputs,
                        max_new_tokens=max_new_tokens,
                        stopping_criteria=stopping_criteria_list,
                        temperature=temperature,
                        top_k=top_k,
                        top_p=top_p,
                        do_sample=do_sample,
                        pad_token_id=processor.tokenizer.pad_token_id,
                        eos_token_id=processor.tokenizer.eos_token_id,
                        interrupt_event=interrupt_event,
                    )
                except torch.cuda.OutOfMemoryError as e:
                    _log.error(f"âŒ CUDAæ˜¾å­˜ä¸è¶³: {e}")
                    # æ¸…ç†æ˜¾å­˜
                    torch.cuda.empty_cache()
                    raise
        
        # å¤„ç†è¾“å‡º
        if interrupt_event and interrupt_event.is_set():
            _log.warning("âš ï¸ ç”Ÿæˆè¿‡ç¨‹ä¸­æ£€æµ‹åˆ°ä¸­æ–­ï¼Œä¸¢å¼ƒæœªå®Œæˆçš„è¾“å‡º")
            return None, False, True

        if isinstance(output_ids, tuple):
            generated_ids = output_ids[0]
        else:
            generated_ids = output_ids
        
        # æå–ç”Ÿæˆçš„tokenï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
        input_length = inputs['input_ids'].shape[1]
        generated_ids_trimmed = generated_ids[:, input_length:]
        
        _log.info(f"ğŸ“Š ç”Ÿæˆå®Œæˆï¼Œè¾“å…¥é•¿åº¦: {input_length}, è¾“å‡ºé•¿åº¦: {generated_ids_trimmed.shape[1]}")
        
        # è§£ç ç”Ÿæˆç»“æœï¼ˆåŒ…å«ç‰¹æ®Štokenï¼Œç”¨äºæ—¥å¿—ï¼‰
        output_text_with_special = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=False,
            clean_up_tokenization_spaces=False
        )[0]
        
        if log_full_io:
            _log.info("=" * 80)
            _log.info("ğŸ”¤ æ¨¡å‹å®Œæ•´è¾“å‡ºï¼ˆåŒ…æ‹¬ç‰¹æ®Štokenï¼‰ï¼š")
            _log.info(output_text_with_special)
            _log.info("=" * 80)
        
        # è§£ç ç”Ÿæˆç»“æœï¼ˆè·³è¿‡ç‰¹æ®Štokenï¼Œç”¨äºå®é™…å›å¤ï¼‰
        output_text = processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]
        
        # æå–æœ€ç»ˆå›å¤
        final_reply, should_reply, actions = extract_final_reply(output_text)
        
        if log_full_io:
            _log.info("=" * 80)
            _log.info(f"âœ… æœ€ç»ˆå›å¤: {final_reply}")
            _log.info(f"ğŸ“Œ æ˜¯å¦éœ€è¦å›å¤: {should_reply}")
            if actions:
                _log.info(f"ğŸ¬ åŠ¨ä½œ: {actions}")
            _log.info("=" * 80)
        
        return final_reply, should_reply, False
        
    except Exception as e:
        _log.error(f"âŒ ç”Ÿæˆå›å¤å¤±è´¥: {e}", exc_info=True)
        raise

