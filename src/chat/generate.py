# -*- coding: utf-8 -*-
"""
è‡ªå®šä¹‰ç”Ÿæˆæ¨¡å—
å®ç°è‡ªå›å½’ç”Ÿæˆ + è®°å¿†æœºåˆ¶
"""
import logging
import threading
import torch
from typing import Optional
from transformers.generation.stopping_criteria import StoppingCriteriaList
from transformers.generation.logits_process import (
    LogitsProcessorList,
    TemperatureLogitsWarper,
    TopKLogitsWarper,
    TopPLogitsWarper,
)

from training.model_utils import forward_backbone, ensure_last_hidden_state, build_causal_lm_output

_log = logging.getLogger(__name__)


def memory_head(query_vector, memory_db, debug=False):
    """
    Memory Head: ä»è®°å¿†åº“ä¸­æ£€ç´¢è®°å¿†å‘é‡ï¼Œè¾“å‡ºlogitsï¼ˆç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
    
    è®¾è®¡ç†å¿µï¼šå°†è®°å¿†æ£€ç´¢è§†ä¸ºç‰¹æ®Šçš„"head"ï¼Œå®Œå…¨ç±»ä¼¼äºlm_headç”¨äºç”Ÿæˆtokenã€‚
    - lm_head: hidden_state -> logits (vocab_size) - å¯¹æ‰€æœ‰vocabè®¡ç®—logits
    - memory_head: query_vector -> logits (memory_candidates) - å¯¹æ‰€æœ‰è®°å¿†å‘é‡è®¡ç®—ç›¸ä¼¼åº¦
    
    ä¸lm_headå®Œå…¨ä¸€è‡´ï¼š
    - éƒ½è®¡ç®—æ‰€æœ‰å€™é€‰çš„åˆ†æ•°ï¼ˆvocab_size æˆ– æ‰€æœ‰è®°å¿†å‘é‡ï¼‰
    - éƒ½åªè¾“å‡ºlogitsï¼Œä¸è¿›è¡Œsoftmaxå’Œé‡‡æ ·
    - softmaxå’Œé‡‡æ ·å°†åœ¨ç”Ÿæˆæµç¨‹ä¸­ç»Ÿä¸€å¤„ç†
    - top-kæˆªæ–­åœ¨logits_warperä¸­è¿›è¡Œï¼Œä¸tokenç”Ÿæˆå®Œå…¨ä¸€è‡´
    
    Args:
        query_vector: æŸ¥è¯¢å‘é‡ [hidden_dim]ï¼Œæ¥è‡ª<recall>ä½ç½®çš„last_hidden_state
        memory_db: è®°å¿†å‘é‡æ•°æ®åº“
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
    
    Returns:
        memory_logits: è®°å¿†å‘é‡çš„logitsï¼ˆç›¸ä¼¼åº¦åˆ†æ•°ï¼‰[num_candidates]ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        memory_candidates: å€™é€‰è®°å¿†å‘é‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'embedding': tensor, 'score': float, 'index': int}
        å¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å› (None, None)
    """
    if memory_db is None or len(memory_db) == 0:
        _log.info("ğŸ” [Memory Head] è®°å¿†å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒåŒ¹é…")
        return None, None

    _log.info(f"ğŸ” [Memory Head] å¼€å§‹æœç´¢è®°å¿†åº“ï¼ŒæŸ¥è¯¢å‘é‡shape: {query_vector.shape}, è®°å¿†åº“å¤§å°: {len(memory_db)}")
    # æ£€ç´¢æ‰€æœ‰è®°å¿†å‘é‡ï¼ˆä¸lm_headå¯¹æ‰€æœ‰vocabè®¡ç®—logitsä¸€è‡´ï¼‰
    # memory_db.searchå†…éƒ¨ä¼šè®¡ç®—æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦ï¼Œç„¶åè¿”å›æ‰€æœ‰ç»“æœ
    # top-kæˆªæ–­å°†åœ¨logits_warperä¸­è¿›è¡Œ
    search_results = memory_db.search(
        query_vector.detach().clone(),
        top_k=len(memory_db),  # æ£€ç´¢æ‰€æœ‰å‘é‡ï¼Œä¸lm_headå¯¹æ‰€æœ‰vocabè®¡ç®—logitsä¸€è‡´
        debug=debug
    )
    if not search_results:
        _log.info("ğŸ” [Memory Head] æœªæ‰¾åˆ°åŒ¹é…çš„è®°å¿†å‘é‡")
        return None, None

    _log.info(f"ğŸ” [Memory Head] æ‰¾åˆ° {len(search_results)} ä¸ªå€™é€‰è®°å¿†å‘é‡")
    for i, result in enumerate(search_results):
        score = result.get('score', 0.0)
        _log.info(f"  [{i+1}] ç›¸ä¼¼åº¦={score:.4f}")

    # æå–logitsï¼ˆç›¸ä¼¼åº¦åˆ†æ•°ï¼‰ï¼Œä¸lm_headè¾“å‡ºlogitså®Œå…¨ä¸€è‡´
    memory_logits = torch.tensor(
        [item['score'] for item in search_results],
        dtype=torch.float32,
        device=query_vector.device
    )
    
    _log.debug(f"ğŸ” [Memory Head] è¾“å‡ºlogits shape: {memory_logits.shape}, èŒƒå›´: [{memory_logits.min():.4f}, {memory_logits.max():.4f}]")
    return memory_logits, search_results


def memory_embedding(memory_vector, model, device=None, dtype=None):
    """
    Memory Embedding: å‡†å¤‡è®°å¿†å‘é‡ç”¨äºæ³¨å…¥ï¼Œè·³è¿‡embeddingå±‚
    
    è®¾è®¡ç†å¿µï¼šå°†è®°å¿†å‘é‡è§†ä¸ºç‰¹æ®Šçš„embeddingï¼Œç±»ä¼¼äºinput_embeddingsç”¨äºtoken IDï¼Œ
    ä½†è¿™é‡Œç›´æ¥ä½¿ç”¨è®°å¿†å‘é‡ï¼Œä¸ç»è¿‡embeddingå±‚è®¡ç®—ã€‚è®°å¿†å‘é‡æ˜¯å·²ç»è®¡ç®—å¥½çš„hidden stateï¼Œ
    ç›´æ¥ä½œä¸ºä¸‹ä¸€ä¸ªä½ç½®çš„è¾“å…¥ã€‚
    
    Args:
        memory_vector: è®°å¿†å‘é‡ [hidden_dim] æˆ– [1, hidden_dim] æˆ– [1, 1, hidden_dim]
        model: æ¨¡å‹å®ä¾‹ï¼ˆç”¨äºè·å–è®¾å¤‡å’Œæ•°æ®ç±»å‹ï¼‰
        device: ç›®æ ‡è®¾å¤‡ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä»modelè·å–ï¼‰
        dtype: ç›®æ ‡æ•°æ®ç±»å‹ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä»modelè·å–ï¼‰
    
    Returns:
        memory_embedding: å‡†å¤‡å¥½çš„è®°å¿†å‘é‡ [1, 1, hidden_dim]ï¼Œå¯ç›´æ¥ä½œä¸ºinputs_embedsä½¿ç”¨
    """
    if memory_vector is None:
        return None
    
    # è°ƒæ•´å½¢çŠ¶ä¸º [1, 1, hidden_dim]
    if memory_vector.dim() == 1:
        memory_vector = memory_vector.unsqueeze(0)  # [1, hidden_dim]
    if memory_vector.dim() == 2:
        memory_vector = memory_vector.unsqueeze(0)  # [1, 1, hidden_dim]
    
    # è·å–è®¾å¤‡å’Œæ•°æ®ç±»å‹
    if device is None or dtype is None:
        model_device = next(model.parameters()).device
        model_dtype = next(model.parameters()).dtype
        if device is None:
            device = model_device
        if dtype is None:
            dtype = model_dtype
    
    # ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡å’Œæ•°æ®ç±»å‹
    memory_embedding = memory_vector.to(device=device, dtype=dtype)
    
    _log.debug(f"ğŸ”§ [Memory Embedding] å‡†å¤‡è®°å¿†å‘é‡ï¼Œshape: {memory_embedding.shape}, device: {device}, dtype: {dtype}")
    return memory_embedding


def custom_generate(
    model,
    processor,
    memory_db,
    recall_token_ids,
    config,
    inputs,
    max_new_tokens: int = 1000,
    stopping_criteria: StoppingCriteriaList = None,
    logits_processor: LogitsProcessorList = None,
    temperature: float = 1.0,
    top_k: int = None,
    top_p: float = None,
    do_sample: bool = True,
    pad_token_id: int = None,
    eos_token_id: int = None,
    interrupt_event: threading.Event = None,
    early_stop_on_tool_call: bool = False,
):
    """
    è‡ªå®šä¹‰ç”Ÿæˆå‡½æ•°ï¼šä¿æŒå®˜æ–¹generateæµç¨‹ï¼Œä½†å°†è®°å¿†æ£€ç´¢è§†ä¸ºç‰¹æ®Šheadï¼Œå°†è®°å¿†å‘é‡è§†ä¸ºç‰¹æ®ŠåµŒå…¥
    
    Args:
        model: æ¨¡å‹å®ä¾‹
        processor: å¤„ç†å™¨å®ä¾‹
        memory_db: è®°å¿†å‘é‡æ•°æ®åº“
        recall_token_ids: ç‰¹æ®Štoken IDæ˜ å°„
        config: é…ç½®å­—å…¸
        inputs: è¾“å…¥å­—å…¸
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        stopping_criteria: åœæ­¢æ¡ä»¶åˆ—è¡¨
        logits_processor: logitså¤„ç†å™¨åˆ—è¡¨
        temperature: æ¸©åº¦å‚æ•°
        top_k: top-ké‡‡æ ·å‚æ•°
        top_p: top-pé‡‡æ ·å‚æ•°
        do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·
        pad_token_id: padding token ID
        eos_token_id: EOS token ID
        interrupt_event: ä¸­æ–­äº‹ä»¶
        early_stop_on_tool_call: æ˜¯å¦åœ¨å·¥å…·è°ƒç”¨æ—¶æå‰åœæ­¢
    
    Returns:
        ç”Ÿæˆçš„token IDsï¼ˆå¦‚æœæœ‰è®°å¿†æ³¨å…¥ï¼Œè¿˜ä¼šè¿”å›æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼‰
    """
    input_ids = inputs.get('input_ids')
    attention_mask = inputs.get('attention_mask', None)

    batch_size = input_ids.shape[0]
    cur_len = input_ids.shape[-1]
    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)

    if stopping_criteria is None:
        stopping_criteria = StoppingCriteriaList()
    if logits_processor is None:
        logits_processor = LogitsProcessorList()

    # é…ç½®logits warper
    logits_warper = None
    if do_sample:
        logits_warper_list = []
        if temperature is not None and temperature != 1.0:
            logits_warper_list.append(TemperatureLogitsWarper(temperature=temperature))
        if top_k is not None and top_k > 0:
            logits_warper_list.append(TopKLogitsWarper(top_k=top_k))
        if top_p is not None and top_p < 1.0:
            logits_warper_list.append(TopPLogitsWarper(top_p=top_p))
        if logits_warper_list:
            logits_warper = LogitsProcessorList(logits_warper_list)

    # å‡†å¤‡model_kwargs
    model_kwargs = {k: v for k, v in inputs.items() if k not in ['input_ids', 'attention_mask']}
    if attention_mask is not None:
        model_kwargs['attention_mask'] = attention_mask
    if 'use_cache' not in model_kwargs:
        model_kwargs['use_cache'] = True

    # é…ç½®cache_position
    if not model_kwargs.get("use_cache", True):
        model_kwargs["cache_position"] = None
    else:
        past_length = 0
        if "past_key_values" in model_kwargs and model_kwargs["past_key_values"] is not None:
            try:
                from transformers.cache_utils import Cache
                if isinstance(model_kwargs["past_key_values"], Cache):
                    past_length = model_kwargs["past_key_values"].get_seq_length()
                else:
                    past_length = model_kwargs["past_key_values"][0][0].shape[2]
            except (ImportError, AttributeError):
                past_length = model_kwargs["past_key_values"][0][0].shape[2]
        if "inputs_embeds" in model_kwargs:
            input_seq_len = model_kwargs["inputs_embeds"].shape[1]
        else:
            input_seq_len = input_ids.shape[-1]
        model_kwargs["cache_position"] = torch.arange(past_length, input_seq_len, device=input_ids.device)

    # å¤„ç†EOS token
    if eos_token_id is not None:
        if isinstance(eos_token_id, (list, tuple)):
            eos_token_ids = torch.tensor(list(eos_token_id), device=input_ids.device)
        else:
            eos_token_ids = torch.tensor([eos_token_id], device=input_ids.device)
    else:
        eos_token_ids = None
    has_eos_stopping_criteria = eos_token_ids is not None

    # è·å–è®°å¿†ç›¸å…³çš„token ID
    recall_token_id = recall_token_ids.get("<recall>") if recall_token_ids else None
    memory_pad_token_id = recall_token_ids.get("<|memory_pad|>") if recall_token_ids else None

    # è®°å¿†æ³¨å…¥ä½ç½®è®°å½•
    memory_injection_positions = []
    
    # è®°å¿†é…ç½®
    memory_cfg = config.get("memory", {}).get("autoregressive_recall", {})
    autorecall_enabled = bool(memory_cfg.get("enabled", False))
    autorecall_top_k = max(1, int(memory_cfg.get("top_k", 5)))
    autorecall_temperature = float(memory_cfg.get("temperature", 1.0))
    autorecall_top_p = float(memory_cfg.get("top_p", 1.0))
    autorecall_use_sampling = bool(memory_cfg.get("use_sampling", True))
    autorecall_debug = bool(memory_cfg.get("debug", False))

    def _update_model_kwargs_helper(outputs_obj):
        """æ›´æ–°model_kwargs"""
        nonlocal model_kwargs
        try:
            model_kwargs = model._update_model_kwargs_for_generation(
                outputs_obj,
                model_kwargs,
                is_encoder_decoder=False,
                standardize_cache_format=True,
            )
        except TypeError:
            try:
                model_kwargs = model._update_model_kwargs_for_generation(
                    outputs_obj,
                    model_kwargs,
                    is_encoder_decoder=False,
                )
            except TypeError:
                model_kwargs = model._update_model_kwargs_for_generation(
                    outputs_obj,
                    model_kwargs,
                )

    def _forward_with_last_hidden_state(forward_inputs):
        """æ‰§è¡Œå‰å‘ä¼ æ’­å¹¶è·å–æœ€åçš„éšè—çŠ¶æ€"""
        local_inputs = dict(forward_inputs)
        use_cache_flag = local_inputs.pop("use_cache", True)
        output_hidden_flag = local_inputs.pop("output_hidden_states", False)

        backbone_outputs = forward_backbone(
            model,
            use_cache=use_cache_flag,
            output_hidden_states=output_hidden_flag,
            return_dict=True,
            **local_inputs,
        )
        outputs = build_causal_lm_output(model, backbone_outputs)
        last_hidden_state = ensure_last_hidden_state(backbone_outputs)
        outputs.last_hidden_state = last_hidden_state
        return outputs


    # ç”Ÿæˆå¾ªç¯
    override_next_embed = None  # ç”¨äºè¦†ç›–ä¸‹ä¸€æ­¥çš„embeddingï¼ˆè®°å¿†å‘é‡ï¼‰
    forced_next_token_id = None

    while cur_len < max_new_tokens:
        # æ£€æŸ¥ä¸­æ–­
        if interrupt_event and interrupt_event.is_set():
            break

        # å‡†å¤‡è¾“å…¥
        model_inputs = model.prepare_inputs_for_generation(
            input_ids,
            **model_kwargs
        )

        # æ£€æŸ¥æ˜¯å¦è§¦å‘å›å¿†
        current_input_ids = model_inputs.get('input_ids', input_ids)
        recall_triggered = False
        if current_input_ids.shape[-1] > 0:
            last_token_id = current_input_ids[0, -1].item()
            if (
                autorecall_enabled
                and recall_token_id is not None
                and last_token_id == recall_token_id
            ):
                recall_triggered = True

        # å‰å‘ä¼ æ’­
        forward_inputs = dict(model_inputs)
        forward_inputs.setdefault("use_cache", model_kwargs.get("use_cache", True))
        if override_next_embed is not None:
            # å½“æä¾›inputs_embedsæ—¶ï¼Œå¿…é¡»ç§»é™¤input_idsä»¥é¿å…æ¡†æ¶æŠ¥é”™
            forward_inputs.pop("input_ids", None)
            forward_inputs["inputs_embeds"] = override_next_embed
        outputs = _forward_with_last_hidden_state(forward_inputs)
        last_hidden_state = outputs.last_hidden_state
        override_next_embed = None
        forced_next_token_id = None

        # å¤„ç†å›å¿†è§¦å‘
        memory_logits = None
        memory_candidates = None
        if recall_triggered:
            _log.info("ğŸ”„ [å›å¿†è§¦å‘] æ£€æµ‹åˆ°<recall> tokenï¼Œå‡†å¤‡æ£€ç´¢è®°å¿†å‘é‡")
            if last_hidden_state is None:
                _log.warning("âš ï¸ [å›å¿†è§¦å‘] æ— æ³•è·å–<recall>éšè—å‘é‡ï¼Œç»§ç»­æ™®é€šç”Ÿæˆ")
            elif memory_db is None or len(memory_db) == 0:
                _log.info("â„¹ï¸ [å›å¿†è§¦å‘] è®°å¿†å‘é‡åº“ä¸ºç©ºï¼Œ<recall> æŒ‰æ™®é€štokenå¤„ç†")
            else:
                # Memory Head: ä»è®°å¿†åº“ä¸­æ£€ç´¢ï¼Œè¾“å‡ºlogitsï¼ˆä¸lm_headå®Œå…¨ä¸€è‡´ï¼‰
                query_vector = last_hidden_state[0, -1, :]
                # æ£€ç´¢æ‰€æœ‰è®°å¿†å‘é‡ï¼ˆä¸lm_headå¯¹æ‰€æœ‰vocabè®¡ç®—logitsä¸€è‡´ï¼‰
                # top-kæˆªæ–­å°†åœ¨logits_warperä¸­è¿›è¡Œ
                memory_logits, memory_candidates = memory_head(
                    query_vector=query_vector,
                    memory_db=memory_db,
                    debug=autorecall_debug
                )
                
                if memory_logits is None or memory_candidates is None:
                    _log.info("â„¹ï¸ [å›å¿†è§¦å‘] æœªæ‰¾åˆ°å¯ç”¨è®°å¿†ï¼Œ<recall> æŒ‰æ™®é€štokenå¤„ç†")
                else:
                    _log.info(f"ğŸ¯ [å›å¿†è§¦å‘] Memory Headè¾“å‡ºlogitsï¼Œå€™é€‰æ•°: {len(memory_candidates)}")

        # å¤„ç†è®°å¿†æ£€ç´¢ï¼ˆä¸tokenç”Ÿæˆå®Œå…¨ç»Ÿä¸€çš„æµç¨‹ï¼‰
        if memory_logits is not None and memory_candidates is not None:
            # 1. åº”ç”¨logits processorï¼ˆå¦‚æœéœ€è¦ï¼Œå¯ä»¥å¯¹è®°å¿†logitsåº”ç”¨ç›¸åŒçš„å¤„ç†ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œæš‚æ—¶ä¸åº”ç”¨logits_processorï¼Œå› ä¸ºå®ƒæ˜¯ä¸ºtokenè®¾è®¡çš„
            # å¦‚æœéœ€è¦ï¼Œå¯ä»¥åˆ›å»ºä¸“é—¨çš„memory_logits_processor
            memory_scores = memory_logits
            
            # 2. åº”ç”¨logits warperï¼ˆæ¸©åº¦ã€top-kã€top-pï¼‰- ä¸tokenç”Ÿæˆå®Œå…¨ä¸€è‡´
            if autorecall_use_sampling:
                # åˆ›å»ºè®°å¿†ä¸“ç”¨çš„logits warperï¼ˆä½¿ç”¨è®°å¿†é…ç½®çš„æ¸©åº¦ã€top-kã€top-pç­‰ï¼‰
                memory_warper_list = []
                if autorecall_temperature is not None and autorecall_temperature != 1.0:
                    memory_warper_list.append(TemperatureLogitsWarper(temperature=autorecall_temperature))
                if autorecall_top_k is not None and autorecall_top_k > 0:
                    memory_warper_list.append(TopKLogitsWarper(top_k=autorecall_top_k))
                if autorecall_top_p is not None and autorecall_top_p < 1.0:
                    memory_warper_list.append(TopPLogitsWarper(top_p=autorecall_top_p))
                if memory_warper_list:
                    memory_warper = LogitsProcessorList(memory_warper_list)
                    # æ³¨æ„ï¼šlogits_warperéœ€è¦input_idsï¼Œè¿™é‡Œä¼ å…¥dummy input_ids
                    dummy_input_ids = torch.zeros((1, 1), dtype=torch.long, device=memory_scores.device)
                    memory_scores = memory_warper(dummy_input_ids, memory_scores.unsqueeze(0)).squeeze(0)
                    _log.debug(f"ğŸ” [Memory Head] åº”ç”¨logits_warperåï¼Œå€™é€‰æ•°: {memory_scores.shape[0]}")
            
            # 3. é‡‡æ ·æˆ–è´ªå©ªé€‰æ‹©ï¼ˆä¸tokenç”Ÿæˆå®Œå…¨ä¸€è‡´ï¼‰
            if autorecall_use_sampling:
                probs = torch.nn.functional.softmax(memory_scores, dim=-1)
                choice_idx = torch.multinomial(probs, num_samples=1).item()
                _log.info(f"ğŸ” [Memory Head] ä½¿ç”¨é‡‡æ ·æ–¹å¼é€‰æ‹©è®°å¿†ï¼Œé€‰æ‹©ç´¢å¼•: {choice_idx}, æ¦‚ç‡: {probs[choice_idx]:.4f}")
            else:
                choice_idx = torch.argmax(memory_scores).item()
                _log.info(f"ğŸ” [Memory Head] ä½¿ç”¨è´ªå©ªæ–¹å¼é€‰æ‹©è®°å¿†ï¼Œé€‰æ‹©ç´¢å¼•: {choice_idx}, æœ€é«˜ç›¸ä¼¼åº¦: {memory_scores[choice_idx]:.4f}")
            
            # 4. è·å–é€‰ä¸­çš„è®°å¿†å‘é‡
            selected = memory_candidates[choice_idx]
            memory_vector = selected['embedding']
            memory_score = selected.get('score', 0.0)
            _log.info(f"âœ… [Memory Head] å·²é€‰æ‹©è®°å¿†å‘é‡ï¼Œç›¸ä¼¼åº¦={memory_score:.4f}")
            
            # 5. å‡†å¤‡è®°å¿†æ³¨å…¥
            if memory_pad_token_id is not None:
                forced_next_token_id = memory_pad_token_id
                injection_pos = input_ids.shape[-1]
                memory_injection_positions.append((injection_pos, memory_score))
                
                # Memory Embedding: å‡†å¤‡è®°å¿†å‘é‡ï¼Œè·³è¿‡embeddingå±‚
                override_next_embed = memory_embedding(
                    memory_vector=memory_vector,
                    model=model
                )
                
                # æ›´æ–°attention mask
                if 'attention_mask' in model_kwargs and model_kwargs['attention_mask'] is not None:
                    model_kwargs['attention_mask'] = torch.cat(
                        [model_kwargs['attention_mask'], torch.ones((1, 1), device=model_kwargs['attention_mask'].device, dtype=model_kwargs['attention_mask'].dtype)],
                        dim=1
                    )
                _log.info("âœ… [å›å¿†è§¦å‘] Memory Embeddingå·²å‡†å¤‡ï¼Œå°†å¼ºåˆ¶ç”Ÿæˆ<|memory_pad|>å¹¶ç”¨è®°å¿†å‘é‡è¦†ç›–å…¶embedding")
            else:
                _log.warning("âš ï¸ [å›å¿†è§¦å‘] æœªæ‰¾åˆ°<|memory_pad|> tokenï¼Œæ— æ³•æ’å…¥è®°å¿†å‘é‡ï¼Œç»§ç»­æ™®é€šç”Ÿæˆ")
        
        # è·å–ä¸‹ä¸€ä¸ªtokenï¼ˆä¸è®°å¿†æ£€ç´¢å®Œå…¨ç»Ÿä¸€çš„æµç¨‹ï¼‰
        next_token_logits = outputs.logits[:, -1, :]
        next_token_scores = logits_processor(input_ids, next_token_logits)

        if do_sample and logits_warper is not None:
            next_token_scores = logits_warper(input_ids, next_token_scores)

        # ç”Ÿæˆnext token
        if forced_next_token_id is not None:
            next_tokens = torch.full(
                (batch_size,),
                forced_next_token_id,
                device=input_ids.device,
                dtype=input_ids.dtype
            )
        else:
            if do_sample:
                probs = torch.nn.functional.softmax(next_token_scores, dim=-1)
                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
            else:
                next_tokens = torch.argmax(next_token_scores, dim=-1)

        # å¤„ç†EOS
        if has_eos_stopping_criteria:
            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)

        # æ·»åŠ æ–°token
        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)

        # æ›´æ–°model_kwargs
        _update_model_kwargs_helper(outputs)

        # æ£€æŸ¥EOS
        if eos_token_ids is not None:
            eos_in_sentence = (next_tokens.unsqueeze(-1) == eos_token_ids.unsqueeze(0)).any(dim=-1)
            unfinished_sequences = unfinished_sequences & ~eos_in_sentence

        cur_len += 1

        # æ£€æŸ¥åœæ­¢æ¡ä»¶
        should_stop = stopping_criteria(input_ids, next_token_scores)
        if isinstance(should_stop, bool):
            should_stop_tensor = torch.tensor([should_stop], device=unfinished_sequences.device, dtype=torch.bool)
            if batch_size > 1:
                should_stop_tensor = should_stop_tensor.expand(batch_size)
        else:
            should_stop_tensor = should_stop.bool() if should_stop.dtype != torch.bool else should_stop
        unfinished_sequences = unfinished_sequences & ~should_stop_tensor

        if unfinished_sequences.max() == 0:
            if interrupt_event and interrupt_event.is_set():
                _log.info("âš ï¸ ç”Ÿæˆå› ä¸­æ–­è€Œåœæ­¢")
            else:
                _log.debug("ç”Ÿæˆå› StoppingCriteriaè€Œåœæ­¢ï¼ˆæ­£å¸¸åœæ­¢ï¼‰")
            break

        # æå‰åœæ­¢ï¼ˆå·¥å…·è°ƒç”¨ï¼‰
        if early_stop_on_tool_call:
            try:
                decoded_so_far = processor.batch_decode(input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)[0]
                open_idx = decoded_so_far.rfind("<tool_call")
                if open_idx != -1:
                    close_idx = decoded_so_far.rfind("</tool_call>")
                    if close_idx != -1 and close_idx > open_idx:
                        _log.info("ğŸ”§ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨é—­åˆæ ‡ç­¾ï¼Œæå‰ç»“æŸé¦–è½®ç”Ÿæˆ")
                        break
            except Exception:
                pass

    # è¿”å›ç»“æœ
    if memory_injection_positions:
        return input_ids, memory_injection_positions
    return input_ids

