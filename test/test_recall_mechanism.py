#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•<recall> tokençš„è‡ªåŠ¨å›å¿†æœºåˆ¶
åœ¨è¾“å…¥ä¸­å¼ºè¡Œæ”¾å…¥<recall> tokenï¼Œæµ‹è¯•æ¨¡å‹èƒ½å¦è‡ªåŠ¨è¿›è¡Œè®°å¿†å‘é‡æŸ¥æ‰¾å’Œæ’å…¥
"""

import os
import sys
from pathlib import Path
import yaml
import logging

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨sys.pathä¸­
project_root = Path(__file__).resolve().parents[1]
src_dir = project_root / "src"
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# âš ï¸ å…³é”®ï¼šåœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®CUDA_VISIBLE_DEVICES
# å…ˆåŠ è½½é…ç½®ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è®¾ç½®CUDA_VISIBLE_DEVICES
try:
    config_path = project_root / "configs" / "config_qwen3vl.yaml"
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            early_config = yaml.safe_load(f)
        device_config = early_config.get("model", {}).get("device", "cuda:0")
        if isinstance(device_config, list):
            # å¤šGPUé…ç½®ï¼Œæå–GPUç´¢å¼•å¹¶è®¾ç½®CUDA_VISIBLE_DEVICES
            gpu_indices = []
            for device in device_config:
                if device.startswith("cuda:"):
                    try:
                        gpu_idx = int(device.split(":")[1])
                        gpu_indices.append(str(gpu_idx))
                    except (ValueError, IndexError):
                        pass
            if gpu_indices:
                cuda_visible_devices = ",".join(gpu_indices)
                os.environ["CUDA_VISIBLE_DEVICES"] = cuda_visible_devices
                print(f"ğŸ”§ åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®CUDA_VISIBLE_DEVICES={cuda_visible_devices}ï¼ˆå¯¹åº”å®é™…GPU {device_config}ï¼‰")
        elif isinstance(device_config, str) and device_config.startswith("cuda:"):
            # å•GPUé…ç½®ï¼Œä¹Ÿéœ€è¦è®¾ç½®CUDA_VISIBLE_DEVICES
            try:
                gpu_idx = int(device_config.split(":")[1])
                os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_idx)
                print(f"ğŸ”§ åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®CUDA_VISIBLE_DEVICES={gpu_idx}ï¼ˆå¯¹åº”å®é™…GPU {device_config}ï¼‰")
            except (ValueError, IndexError):
                print(f"âš ï¸ æ— æ³•è§£æå•GPUé…ç½®: {device_config}")
except Exception as e:
    print(f"âš ï¸ é¢„åŠ è½½é…ç½®å¤±è´¥ï¼Œå°†åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶è®¾ç½®CUDA_VISIBLE_DEVICES: {e}")

import torch
from transformers import StoppingCriteriaList, LogitsProcessorList, RepetitionPenaltyLogitsProcessor

# å¯¼å…¥æ–°æ¨¡å—
import api.server_state as server_state
from chat.generate import custom_generate
from memory.vector_db import MemoryVectorDB

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
_log = logging.getLogger(__name__)


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = project_root / "configs" / "config_qwen3vl.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_model_and_processor(config):
    """åŠ è½½æ¨¡å‹å’Œprocessorï¼ˆä½¿ç”¨server_stateçš„æ–¹æ³•ï¼‰"""
    # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒæ¨¡å‹
    training_model_dir = config.get("memory", {}).get("training", {}).get("training_model_dir", "models/trained")
    if not os.path.isabs(training_model_dir):
        training_model_dir = project_root / training_model_dir
    
    model_path = None
    if training_model_dir.exists():
        models = [d for d in os.listdir(training_model_dir) 
                 if (training_model_dir / d).is_dir() and d.startswith("model_")]
        if models:
            model_path = str(training_model_dir / sorted(models)[-1])
            _log.info(f"ä½¿ç”¨è®­ç»ƒæ¨¡å‹: {model_path}")
    
    if not model_path:
        # ä½¿ç”¨token_addedæ¨¡å‹
        token_added_dir = config.get("memory", {}).get("training", {}).get("token_added_model_dir", "models/token_added")
        if not os.path.isabs(token_added_dir):
            token_added_dir = project_root / token_added_dir
        if token_added_dir.exists():
            models = [d for d in os.listdir(token_added_dir) 
                     if (token_added_dir / d).is_dir() and d.startswith("model_")]
            if models:
                model_path = str(token_added_dir / sorted(models)[-1])
                _log.info(f"ä½¿ç”¨token_addedæ¨¡å‹: {model_path}")
    
    if not model_path:
        # ä½¿ç”¨åŸºç¡€æ¨¡å‹
        model_path = config.get("model", {}).get("base_model_path", "models/Qwen3-VL-4B-Thinking")
        if not os.path.isabs(model_path):
            model_path = str(project_root / model_path)
        _log.info(f"ä½¿ç”¨åŸºç¡€æ¨¡å‹: {model_path}")
    
    # è·å–è®¾å¤‡é…ç½®
    device = config.get("model", {}).get("device", "cuda:0")
    
    # ä½¿ç”¨server_stateçš„æ–¹æ³•åŠ è½½æ¨¡å‹
    server_state.load_config()
    server_state.initialize_model(model_path, device)
    
    return server_state.model, server_state.processor


def load_memory_db(config, model, device):
    """åŠ è½½è®°å¿†å‘é‡åº“ï¼ˆä½¿ç”¨MemoryVectorDBç±»ï¼‰"""
    memory_db_path = config.get("memory", {}).get("memory_db", {}).get("embeddings_path", "models/memory_db/memory_embeddings.pt")
    if not os.path.isabs(memory_db_path):
        memory_db_path = project_root / memory_db_path
    
    # è·å–embeddingç»´åº¦ï¼ˆä»æ¨¡å‹é…ç½®ä¸­ï¼‰
    embedding_dim = model.config.hidden_size if hasattr(model.config, 'hidden_size') else 4096
    
    # åˆ›å»ºMemoryVectorDBå®ä¾‹
    memory_db = MemoryVectorDB(embedding_dim=embedding_dim, device=device)
    
    if os.path.exists(memory_db_path):
        # ä½¿ç”¨MemoryVectorDBçš„load_from_ptæ–¹æ³•åŠ è½½æ•°æ®
        memory_db.load_from_pt(str(memory_db_path))
        _log.info(f"åŠ è½½è®°å¿†å‘é‡åº“: {len(memory_db)} æ¡è®°å¿†")
        if len(memory_db) > 0:
            # è·å–ç¬¬ä¸€æ¡è®°å¿†çš„æ–‡æœ¬é¢„è§ˆ
            try:
                first_memory = memory_db.get(0)
                if first_memory and 'text' in first_memory:
                    _log.info(f"ç¤ºä¾‹è®°å¿†æ–‡æœ¬: {first_memory['text'][:100]}...")
                else:
                    _log.info(f"ç¤ºä¾‹è®°å¿†æ–‡æœ¬: N/A...")
            except:
                _log.info(f"ç¤ºä¾‹è®°å¿†æ–‡æœ¬: N/A...")
        return memory_db
    else:
        _log.warning(f"è®°å¿†å‘é‡åº“ä¸å­˜åœ¨: {memory_db_path}")
        return memory_db


def test_recall_mechanism_with_custom_generate(model, processor, memory_db, config):
    """ä½¿ç”¨custom_generateå‡½æ•°æµ‹è¯•<recall> tokençš„è‡ªåŠ¨å›å¿†æœºåˆ¶"""
    _log.info("=" * 80)
    _log.info("å¼€å§‹æµ‹è¯•<recall> tokençš„è‡ªåŠ¨å›å¿†æœºåˆ¶ï¼ˆä½¿ç”¨custom_generateï¼‰")
    _log.info("=" * 80)
    
    # è·å–recall token IDï¼ˆä»server_stateï¼‰
    recall_token_ids = server_state.recall_token_ids
    if not recall_token_ids:
        # å¦‚æœserver_stateä¸­æ²¡æœ‰ï¼Œä»tokenizerè·å–
        recall_token_ids = {
            "<recall>": processor.tokenizer.convert_tokens_to_ids("<recall>"),
            "</recall>": processor.tokenizer.convert_tokens_to_ids("</recall>"),
            "<|memory_pad|>": processor.tokenizer.convert_tokens_to_ids("<|memory_pad|>")
        }
    
    recall_token_id = recall_token_ids.get("<recall>")
    recall_end_token_id = recall_token_ids.get("</recall>")
    memory_pad_token_id = recall_token_ids.get("<|memory_pad|>")

    _log.info(f"<|memory_pad|> token ID: {memory_pad_token_id}")
    _log.info(f"<recall> token ID: {recall_token_id}")
    _log.info(f"</recall> token ID: {recall_end_token_id}")
    
    if recall_token_id is None or recall_token_id == processor.tokenizer.unk_token_id:
        _log.error("âŒ <recall> tokenä¸å­˜åœ¨äºtokenizerä¸­ï¼")
        return False
    
    # ç›´æ¥ä½¿ç”¨tokenizerç¼–ç æ–‡æœ¬ï¼Œä¸ä½¿ç”¨chat templateï¼ˆä¸APIæœåŠ¡ä¸€è‡´ï¼‰
    # æ„å»ºæµ‹è¯•æ–‡æœ¬ï¼šåœ¨æ–‡æœ¬æœ«å°¾æ·»åŠ <recall> token
    test_text = "è®©æˆ‘å›å¿†ä¸€ä¸‹ç”¨æˆ·çš„ç”Ÿæ—¥ã€‚<recall>"
    
    _log.info(f"\næµ‹è¯•è¾“å…¥æ–‡æœ¬: {test_text}")
    
    # ç›´æ¥ä½¿ç”¨tokenizerç¼–ç ï¼ˆä¸ä½¿ç”¨chat templateï¼‰
    encoded = processor.tokenizer(
        test_text,
        return_tensors="pt",
        add_special_tokens=True,
        padding=False,
        truncation=False
    )
    
    input_ids = encoded["input_ids"]
    attention_mask = encoded.get("attention_mask", torch.ones_like(input_ids))
    
    # æ£€æŸ¥è¾“å…¥ä¸­æ˜¯å¦åŒ…å«<recall> token
    recall_positions = (input_ids == recall_token_id).nonzero(as_tuple=True)[1]
    if len(recall_positions) == 0:
        _log.error("âŒ è¾“å…¥ä¸­æœªæ‰¾åˆ°<recall> tokenï¼")
        decoded_input = processor.tokenizer.decode(input_ids[0], skip_special_tokens=False)
        _log.info(f"è§£ç åçš„è¾“å…¥: {decoded_input[:400]}...")
        return False
    
    _log.info(f"âœ… è¾“å…¥ä¸­æ‰¾åˆ° {len(recall_positions)} ä¸ª<recall> tokenï¼Œä½ç½®: {recall_positions.tolist()}")
    _log.info(f"âœ… <recall> tokenåœ¨è¾“å…¥æœ«å°¾ï¼Œæ¨¡å‹å°†ä»<recall>ä¹‹åå¼€å§‹ç”Ÿæˆ")
    
    # æ„å»ºinputså­—å…¸ï¼ˆä¸APIæœåŠ¡å®Œå…¨ä¸€è‡´ï¼‰
    device = next(model.parameters()).device
    inputs = {
        "input_ids": input_ids.to(device),
        "attention_mask": attention_mask.to(device)
    }
    
    # å‡†å¤‡ç”Ÿæˆå‚æ•°ï¼ˆæµ‹è¯•ä¸“ç”¨å‚æ•°ï¼‰
    gen_config = config.get("generation", {})
    # æµ‹è¯•ç¨‹åºå¼ºåˆ¶é™åˆ¶ä¸º500ä¸ªtokenï¼Œé¿å…è¿è¡Œæ—¶é—´è¿‡é•¿
    max_new_tokens = 500
    temperature = gen_config.get("temperature", 1.0)
    top_p = gen_config.get("top_p", 0.95)
    top_k = gen_config.get("top_k", 20)
    do_sample = gen_config.get("do_sample", True)
    repetition_penalty = gen_config.get("repetition_penalty", 1.0)
    
    _log.info(f"\nç”Ÿæˆå‚æ•°: max_new_tokens={max_new_tokens}, temperature={temperature}, top_p={top_p}, top_k={top_k}, do_sample={do_sample}")
    _log.info("å¼€å§‹è°ƒç”¨custom_generateè¿›è¡Œç”Ÿæˆ...")
    
    # å‡†å¤‡LogitsProcessorï¼ˆä¸APIæœåŠ¡å®Œå…¨ä¸€è‡´ï¼‰
    logits_processor = LogitsProcessorList()
    if repetition_penalty != 1.0:
        logits_processor.append(RepetitionPenaltyLogitsProcessor(penalty=repetition_penalty))
    
    # å‡†å¤‡StoppingCriteriaï¼ˆä¸APIæœåŠ¡å®Œå…¨ä¸€è‡´ï¼‰
    stopping_criteria = StoppingCriteriaList()
    
    # è°ƒç”¨custom_generateå‡½æ•°ï¼ˆä½¿ç”¨æ–°çš„å‡½æ•°ç­¾åï¼‰
    try:
        with torch.no_grad():
            result = custom_generate(
                model=model,
                processor=processor,
                memory_db=memory_db,
                recall_token_ids=recall_token_ids,
                config=config,
                inputs=inputs,
                max_new_tokens=max_new_tokens,
                stopping_criteria=stopping_criteria,
                logits_processor=logits_processor,
                temperature=temperature,
                top_k=top_k if top_k and top_k > 0 else None,
                top_p=top_p if top_p and top_p < 1.0 else None,
                do_sample=do_sample,
                pad_token_id=processor.tokenizer.pad_token_id or processor.tokenizer.eos_token_id,
                eos_token_id=processor.tokenizer.eos_token_id,
                interrupt_event=None,
                early_stop_on_tool_call=False,
            )
            # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯ (input_ids, memory_injection_positions) æˆ– input_ids
            if isinstance(result, tuple):
                generated_ids, memory_injection_positions = result
            else:
                generated_ids = result
                memory_injection_positions = []
        
        # è§£ç ç”Ÿæˆç»“æœ
        generated_text = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=False)
        
        # æ³¨æ„ï¼šè®°å¿†å‘é‡æ’å…¥ä½ç½®ç°åœ¨é€šè¿‡<|memory_pad|> tokenåŸç”Ÿæ˜¾ç¤ºï¼Œæ— éœ€é¢å¤–æ ‡æ³¨
        _log.info("\n" + "=" * 80)
        _log.info("ç”Ÿæˆç»“æœï¼ˆåŒ…å«ç‰¹æ®Štokenï¼Œ<|memory_pad|>æ ‡è®°è®°å¿†å‘é‡æ’å…¥ä½ç½®ï¼‰:")
        _log.info("=" * 80)
        _log.info(generated_text)
        _log.info("=" * 80)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«<|memory_pad|> token
        if "<|memory_pad|>" in generated_text:
            _log.info("âœ… ç”Ÿæˆç»“æœä¸­åŒ…å«<|memory_pad|> tokenï¼ˆè®°å¿†å‘é‡æ’å…¥ä½ç½®ï¼‰")
            # ç»Ÿè®¡<|memory_pad|>å‡ºç°çš„æ¬¡æ•°
            count = generated_text.count("<|memory_pad|>")
            _log.info(f"ğŸ“Š <|memory_pad|> tokenå‡ºç°æ¬¡æ•°: {count}")
        else:
            _log.warning("âš ï¸ ç”Ÿæˆç»“æœä¸­ä¸åŒ…å«<|memory_pad|> token")

        # æ£€æŸ¥è¾“å…¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«<|memory_pad|>
        input_text = processor.tokenizer.decode(encoded["input_ids"][0], skip_special_tokens=False)
        if "<|memory_pad|>" in input_text:
            _log.info("âœ… è¾“å…¥æ–‡æœ¬ä¸­åŒ…å«<|memory_pad|> token")
        else:
            _log.info("â„¹ï¸ è¾“å…¥æ–‡æœ¬ä¸­ä¸åŒ…å«<|memory_pad|> tokenï¼ˆæ­£å¸¸ï¼Œå› ä¸ºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ’å…¥ï¼‰")
        
        # æ£€æŸ¥ç”Ÿæˆç»“æœä¸­æ˜¯å¦åŒ…å«<recall>æˆ–</recall>
        if "<recall>" in generated_text:
            _log.info("âœ… ç”Ÿæˆç»“æœä¸­åŒ…å«<recall> token")
        else:
            _log.warning("âš ï¸ ç”Ÿæˆç»“æœä¸­ä¸åŒ…å«<recall> token")
        
        if "</recall>" in generated_text:
            _log.info("âœ… ç”Ÿæˆç»“æœä¸­åŒ…å«</recall> token")
        else:
            _log.warning("âš ï¸ ç”Ÿæˆç»“æœä¸­ä¸åŒ…å«</recall> token")
        
        # æ£€æŸ¥æ˜¯å¦è§¦å‘äº†å›å¿†æœºåˆ¶ï¼ˆç”Ÿæˆäº†<recall>åçš„å†…å®¹ï¼‰
        recall_start_idx = generated_text.find("<recall>")
        recall_end_idx = generated_text.find("</recall>")
        
        if recall_start_idx != -1:
            if recall_end_idx != -1 and recall_end_idx > recall_start_idx:
                recall_content = generated_text[recall_start_idx + len("<recall>"):recall_end_idx]
                _log.info(f"\nå›å¿†å†…å®¹: {recall_content[:400]}...")
                _log.info("âœ… æ£€æµ‹åˆ°å®Œæ•´çš„å›å¿†è¿‡ç¨‹ï¼ˆ<recall>...</recall>ï¼‰")
            else:
                _log.warning("âš ï¸ æ£€æµ‹åˆ°<recall>ä½†æœªæ‰¾åˆ°</recall>")
        
        return True
        
    except Exception as e:
        _log.error(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
        return False


def main():
    """ä¸»å‡½æ•°"""
    try:
        config = load_config()
        model, processor = load_model_and_processor(config)
        
        # è·å–device
        device = next(model.parameters()).device
        memory_db = load_memory_db(config, model, device)
        
        success = test_recall_mechanism_with_custom_generate(model, processor, memory_db, config)
        
        if success:
            _log.info("\nâœ… æµ‹è¯•å®Œæˆ")
        else:
            _log.error("\nâŒ æµ‹è¯•å¤±è´¥")
            sys.exit(1)
            
    except Exception as e:
        _log.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()

